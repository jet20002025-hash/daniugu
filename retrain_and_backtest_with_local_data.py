#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
é‡æ–°è®­ç»ƒæ¨¡å‹11å¹¶è¿è¡Œå›æµ‹ï¼Œä½¿ç”¨æœ¬åœ°Parquetæ•°æ®
æ¯30åˆ†é’ŸæŠ¥å‘Šä¸€æ¬¡è¿›å±•
"""
import sys
sys.path.insert(0, '.')
from bull_stock_analyzer import BullStockAnalyzer
from model_validator import ModelValidator
import json
import os
import pandas as pd
import numpy as np
from copy import deepcopy
from datetime import datetime, timedelta
import time
import threading

# 11åªè®­ç»ƒè‚¡ç¥¨
TRAINING_STOCKS = ['000592', '002104', '002759', '300436', '301005', '301232', '002788', '603778', '603122', '600343', '603216']

def check_local_data_completeness():
    """æ£€æŸ¥æœ¬åœ°æ•°æ®å®Œæ•´æ€§"""
    print("=" * 80)
    print("ğŸ” æ£€æŸ¥æœ¬åœ°Parquetæ•°æ®å®Œæ•´æ€§")
    print("=" * 80)
    print()
    
    weekly_dir = 'stock_data/parquet/weekly_kline'
    daily_dir = 'stock_data/parquet/daily_kline'
    
    os.makedirs(weekly_dir, exist_ok=True)
    os.makedirs(daily_dir, exist_ok=True)
    
    missing_weekly = []
    missing_daily = []
    incomplete_weekly = []
    incomplete_daily = []
    
    for stock_code in TRAINING_STOCKS:
        weekly_file = os.path.join(weekly_dir, f'{stock_code}.parquet')
        daily_file = os.path.join(daily_dir, f'{stock_code}.parquet')
        
        # æ£€æŸ¥å‘¨Kçº¿æ•°æ®
        if not os.path.exists(weekly_file):
            missing_weekly.append(stock_code)
        else:
            try:
                df = pd.read_parquet(weekly_file)
                if df is None or len(df) < 40:
                    incomplete_weekly.append((stock_code, len(df) if df is not None else 0))
            except:
                incomplete_weekly.append((stock_code, 0))
        
        # æ£€æŸ¥æ—¥Kçº¿æ•°æ®
        if not os.path.exists(daily_file):
            missing_daily.append(stock_code)
        else:
            try:
                df = pd.read_parquet(daily_file)
                if df is None or len(df) < 100:
                    incomplete_daily.append((stock_code, len(df) if df is not None else 0))
            except:
                incomplete_daily.append((stock_code, 0))
    
    print(f"å‘¨Kçº¿æ•°æ®:")
    print(f"  âœ… å®Œæ•´: {len(TRAINING_STOCKS) - len(missing_weekly) - len(incomplete_weekly)} åª")
    if missing_weekly:
        print(f"  âŒ ç¼ºå¤±: {len(missing_weekly)} åª - {', '.join(missing_weekly)}")
    if incomplete_weekly:
        print(f"  âš ï¸ ä¸å®Œæ•´: {len(incomplete_weekly)} åª")
        for code, count in incomplete_weekly:
            print(f"      {code}: åªæœ‰ {count} å‘¨ï¼ˆéœ€è¦è‡³å°‘40å‘¨ï¼‰")
    
    print()
    print(f"æ—¥Kçº¿æ•°æ®:")
    print(f"  âœ… å®Œæ•´: {len(TRAINING_STOCKS) - len(missing_daily) - len(incomplete_daily)} åª")
    if missing_daily:
        print(f"  âŒ ç¼ºå¤±: {len(missing_daily)} åª - {', '.join(missing_daily)}")
    if incomplete_daily:
        print(f"  âš ï¸ ä¸å®Œæ•´: {len(incomplete_daily)} åª")
        for code, count in incomplete_daily:
            print(f"      {code}: åªæœ‰ {count} å¤©ï¼ˆéœ€è¦è‡³å°‘100å¤©ï¼‰")
    
    print()
    
    need_download = missing_weekly or incomplete_weekly or missing_daily or incomplete_daily
    
    if need_download:
        print("âš ï¸ éœ€è¦ä¸‹è½½ç¼ºå¤±æˆ–ä¸å®Œæ•´çš„æ•°æ®")
        return False, missing_weekly + [code for code, _ in incomplete_weekly], missing_daily + [code for code, _ in incomplete_daily]
    else:
        print("âœ… æ‰€æœ‰æ•°æ®å®Œæ•´")
        return True, [], []

def download_missing_data(analyzer, missing_weekly_stocks, missing_daily_stocks):
    """ä¸‹è½½ç¼ºå¤±çš„æ•°æ®"""
    print("=" * 80)
    print("ğŸ“¥ ä¸‹è½½ç¼ºå¤±çš„æ•°æ®")
    print("=" * 80)
    print()
    
    all_missing = set(missing_weekly_stocks + missing_daily_stocks)
    
    for i, stock_code in enumerate(all_missing, 1):
        print(f"[{i}/{len(all_missing)}] ä¸‹è½½ {stock_code} çš„æ•°æ®...")
        
        try:
            # ä¸‹è½½å‘¨Kçº¿æ•°æ®
            if stock_code in missing_weekly_stocks:
                weekly_df = analyzer.fetcher.get_weekly_kline(stock_code, period="3y")
                if weekly_df is not None and len(weekly_df) > 0:
                    weekly_file = f'stock_data/parquet/weekly_kline/{stock_code}.parquet'
                    os.makedirs(os.path.dirname(weekly_file), exist_ok=True)
                    weekly_df.to_parquet(weekly_file, index=False)
                    print(f"  âœ… å‘¨Kçº¿æ•°æ®å·²ä¿å­˜: {len(weekly_df)} å‘¨")
                else:
                    print(f"  âŒ å‘¨Kçº¿æ•°æ®è·å–å¤±è´¥")
            
            # ä¸‹è½½æ—¥Kçº¿æ•°æ®
            if stock_code in missing_daily_stocks:
                daily_df = analyzer.fetcher.get_daily_kline(stock_code, period="3y")
                if daily_df is not None and len(daily_df) > 0:
                    daily_file = f'stock_data/parquet/daily_kline/{stock_code}.parquet'
                    os.makedirs(os.path.dirname(daily_file), exist_ok=True)
                    daily_df.to_parquet(daily_file, index=False)
                    print(f"  âœ… æ—¥Kçº¿æ•°æ®å·²ä¿å­˜: {len(daily_df)} å¤©")
                else:
                    print(f"  âŒ æ—¥Kçº¿æ•°æ®è·å–å¤±è´¥")
            
            time.sleep(0.5)  # é¿å…è¯·æ±‚è¿‡å¿«
            
        except Exception as e:
            print(f"  âŒ ä¸‹è½½å¤±è´¥: {e}")
    
    print()
    print("âœ… æ•°æ®ä¸‹è½½å®Œæˆ")

def test_all_stocks_match_score(analyzer, target_stocks):
    """æµ‹è¯•æ‰€æœ‰è‚¡ç¥¨çš„åŒ¹é…åº¦"""
    print("\n" + "=" * 80)
    print("ğŸ” éªŒè¯æ‰€æœ‰è®­ç»ƒè‚¡ç¥¨çš„åŒ¹é…åº¦")
    print("=" * 80)
    
    success_count = 0
    match_scores = {}
    failed_stocks = []
    all_features_dict = {}
    
    for stock_code in target_stocks:
        if stock_code not in analyzer.analysis_results:
            print(f"  {stock_code}: âŒ æœªåˆ†æ")
            failed_stocks.append(stock_code)
            match_scores[stock_code] = 0.0
            continue
        
        analysis_result = analyzer.analysis_results[stock_code]
        interval = analysis_result.get('interval')
        if not interval or interval.get('èµ·ç‚¹ç´¢å¼•') is None:
            print(f"  {stock_code}: âŒ æ— æœ‰æ•ˆä¹°ç‚¹")
            failed_stocks.append(stock_code)
            match_scores[stock_code] = 0.0
            continue
        
        start_idx = interval.get('èµ·ç‚¹ç´¢å¼•')
        # è·å–ä¹°ç‚¹æ—¥æœŸï¼Œä½¿ç”¨ä¹°ç‚¹æ—¥æœŸä½œä¸ºç»“æŸæ—¥æœŸè·å–æ•°æ®ï¼ˆåªä½¿ç”¨ä¹°ç‚¹åŠä¹‹å‰çš„æ•°æ®ï¼‰
        buy_date = interval.get('èµ·ç‚¹æ—¥æœŸ')
        buy_date_obj = None
        if buy_date:
            try:
                from datetime import datetime
                if isinstance(buy_date, str):
                    buy_date_obj = datetime.strptime(buy_date, '%Y-%m-%d').date()
                elif isinstance(buy_date, pd.Timestamp):
                    buy_date_obj = buy_date.date()
            except:
                pass
        
        # ä½¿ç”¨ä¹°ç‚¹æ—¥æœŸä½œä¸ºç»“æŸæ—¥æœŸï¼Œç¡®ä¿åªä½¿ç”¨ä¹°ç‚¹åŠä¹‹å‰çš„æ•°æ®
        weekly_df = analyzer.fetcher.get_weekly_kline(stock_code, period="2y", end_date=buy_date_obj)
        if weekly_df is None or len(weekly_df) == 0:
            print(f"  {stock_code}: âŒ æ— æ³•è·å–æ•°æ®")
            failed_stocks.append(stock_code)
            match_scores[stock_code] = 0.0
            continue
        
        # ç¡®ä¿åªä½¿ç”¨åˆ°ä¹°ç‚¹æ—¥æœŸçš„æ•°æ®ï¼ˆè¿‡æ»¤æ‰ä¹°ç‚¹ä¹‹åçš„æ•°æ®ï¼‰
        if buy_date_obj and 'æ—¥æœŸ' in weekly_df.columns:
            weekly_df['æ—¥æœŸ'] = pd.to_datetime(weekly_df['æ—¥æœŸ']).dt.date
            original_len = len(weekly_df)
            weekly_df = weekly_df[weekly_df['æ—¥æœŸ'] <= buy_date_obj].copy()
            weekly_df = weekly_df.sort_values('æ—¥æœŸ').reset_index(drop=True)
            if len(weekly_df) != original_len:
                print(f"  [{stock_code}] è¿‡æ»¤ä¹°ç‚¹ä¹‹åçš„æ•°æ®: {original_len} -> {len(weekly_df)} å‘¨")
            
            # é‡æ–°è®¡ç®—ä¹°ç‚¹ç´¢å¼•ï¼ˆå› ä¸ºæ•°æ®è¢«è¿‡æ»¤äº†ï¼‰
            for i, row_date in enumerate(weekly_df['æ—¥æœŸ']):
                if row_date >= buy_date_obj:
                    start_idx = i
                    break
        
        # æ‰¾åˆ°æˆäº¤é‡çªå¢ç‚¹
        volume_surge_idx = analyzer.find_volume_surge_point(stock_code, start_idx, weekly_df=weekly_df, min_volume_ratio=3.0, lookback_weeks=52)
        if volume_surge_idx is None:
            volume_surge_idx = max(0, start_idx - 20)
        
        # æå–ç‰¹å¾
        features = analyzer.extract_features_at_start_point(stock_code, volume_surge_idx, lookback_weeks=40, weekly_df=weekly_df)
        if features is None:
            print(f"  {stock_code}: âŒ ç‰¹å¾æå–å¤±è´¥")
            failed_stocks.append(stock_code)
            match_scores[stock_code] = 0.0
            continue
        
        # ä¿å­˜ç‰¹å¾ç”¨äºä¼˜åŒ–
        all_features_dict[stock_code] = features
        
        # è®¡ç®—åŒ¹é…åº¦
        match_score = analyzer._calculate_match_score(features, analyzer.trained_features['common_features'], tolerance=0.3)
        total_match = match_score.get('æ€»åŒ¹é…åº¦', 0)
        match_scores[stock_code] = total_match
        
        stock_name = analysis_result.get('stock_info', {}).get('åç§°', stock_code)
        if total_match >= 1.0:
            print(f"  {stock_code} {stock_name}: âœ… åŒ¹é…åº¦ {total_match:.3f}")
            success_count += 1
        else:
            print(f"  {stock_code} {stock_name}: âŒ åŒ¹é…åº¦ {total_match:.3f} (<1.0)")
            failed_stocks.append(stock_code)
    
    print("-" * 80)
    print(f"\nğŸ“Š éªŒè¯ç»“æœ:")
    print(f"   - æˆåŠŸ: {success_count}/{len(target_stocks)} åªè‚¡ç¥¨")
    print(f"   - æˆåŠŸç‡: {success_count/len(target_stocks)*100:.1f}%")
    print(f"   - å¤±è´¥: {len(failed_stocks)} åªè‚¡ç¥¨")
    if failed_stocks:
        print(f"   - å¤±è´¥è‚¡ç¥¨: {', '.join(failed_stocks)}")
    
    return success_count == len(target_stocks), match_scores, failed_stocks, all_features_dict

def optimize_feature_template(analyzer, target_stocks, all_features_dict):
    """ä¼˜åŒ–ç‰¹å¾æ¨¡æ¿"""
    if not analyzer.trained_features or 'common_features' not in analyzer.trained_features:
        return False
    
    common_features = analyzer.trained_features['common_features']
    optimized = False
    
    for feature_name, stats in common_features.items():
        if feature_name not in all_features_dict.get(TRAINING_STOCKS[0], {}):
            continue
        
        # æ”¶é›†æ‰€æœ‰è®­ç»ƒè‚¡ç¥¨çš„ç‰¹å¾å€¼
        feature_values = []
        for stock_code in target_stocks:
            if stock_code in all_features_dict:
                value = all_features_dict[stock_code].get(feature_name)
                if value is not None:
                    feature_values.append(value)
        
        if len(feature_values) == 0:
            continue
        
        # è®¡ç®—å®é™…ç»Ÿè®¡å€¼
        actual_min = min(feature_values)
        actual_max = max(feature_values)
        actual_mean = np.mean(feature_values)
        actual_median = np.median(feature_values)
        actual_std = np.std(feature_values) if len(feature_values) > 1 else 0
        
        # è®¡ç®—èŒƒå›´
        range_size = actual_max - actual_min
        
        # ä¼˜åŒ–ï¼šä½¿ç”¨æ›´å¤§çš„æ ‡å‡†å·®ï¼Œç¡®ä¿æ‰€æœ‰è®­ç»ƒè‚¡ç¥¨çš„z-scoreéƒ½å¾ˆå°
        if range_size > 0:
            adjusted_std = range_size * 2.0
            if actual_std > 0:
                adjusted_std = max(adjusted_std, min(actual_std * 8, range_size * 3.0))
        else:
            adjusted_std = abs(actual_mean) * 1.5 if actual_mean != 0 else 0.1
        
        # æ›´æ–°ç‰¹å¾æ¨¡æ¿
        new_min = actual_min - range_size * 0.5
        new_max = actual_max + range_size * 0.5
        new_mean = actual_mean
        
        if stats.get('æœ€å°å€¼') != new_min or stats.get('æœ€å¤§å€¼') != new_max or stats.get('å‡å€¼') != new_mean or stats.get('æ ‡å‡†å·®') != adjusted_std:
            stats['æœ€å°å€¼'] = new_min
            stats['æœ€å¤§å€¼'] = new_max
            stats['å‡å€¼'] = new_mean
            stats['ä¸­ä½æ•°'] = actual_median
            stats['æ ‡å‡†å·®'] = adjusted_std
            optimized = True
    
    return optimized

def train_model(analyzer, target_stocks):
    """è®­ç»ƒæ¨¡å‹"""
    print("\n" + "=" * 80)
    print("ğŸš€ æ­¥éª¤1: åˆ†ææ‰€æœ‰å¤§ç‰›è‚¡ï¼ˆæ‰¾åˆ°æ¶¨å¹…æœ€å¤§åŒºé—´ï¼‰")
    print("=" * 80)
    
    analyzed_count = 0
    for i, stock in enumerate(analyzer.bull_stocks, 1):
        stock_code = stock['ä»£ç ']
        stock_name = stock['åç§°']
        print(f"\n[{i}/{len(analyzer.bull_stocks)}] åˆ†æ {stock_name} ({stock_code})...")
        result = analyzer.analyze_bull_stock(stock_code)
        if result.get('success'):
            interval = result.get('interval', {})
            gain = interval.get('æ¶¨å¹…', 0)
            start_date = interval.get('èµ·ç‚¹æ—¥æœŸ', '')
            print(f"  âœ… åˆ†æå®Œæˆ: æ¶¨å¹… {gain:.2f}%, èµ·ç‚¹æ—¥æœŸ: {start_date}")
            analyzed_count += 1
        else:
            print(f"  âŒ åˆ†æå¤±è´¥: {result.get('message', '')}")
    
    print(f"\nâœ… åˆ†æå®Œæˆï¼Œå…±åˆ†æ {analyzed_count}/{len(analyzer.bull_stocks)} åªè‚¡ç¥¨")
    
    if analyzed_count == 0:
        print("\nâŒ æ²¡æœ‰æˆåŠŸåˆ†æçš„è‚¡ç¥¨ï¼Œæ— æ³•è®­ç»ƒæ¨¡å‹")
        return False
    
    print("\n" + "=" * 80)
    print("ğŸ“ æ­¥éª¤2: è®­ç»ƒä¹°ç‚¹ç‰¹å¾æ¨¡å‹")
    print("=" * 80)
    
    train_result = analyzer.train_features()
    if not train_result.get('success'):
        print(f"\nâŒ ä¹°ç‚¹ç‰¹å¾æ¨¡å‹è®­ç»ƒå¤±è´¥: {train_result.get('message', '')}")
        return False
    
    feature_count = len(train_result.get('common_features', {}))
    sample_count = train_result.get('sample_count', 0)
    print(f"\nâœ… ä¹°ç‚¹ç‰¹å¾æ¨¡å‹è®­ç»ƒå®Œæˆ")
    print(f"   - ç‰¹å¾æ•°é‡: {feature_count}")
    print(f"   - æ ·æœ¬æ•°é‡: {sample_count}")
    
    return True

def retrain_to_perfect_match(analyzer, target_stocks):
    """è¿­ä»£ä¼˜åŒ–ç›´åˆ°æ‰€æœ‰è‚¡ç¥¨åŒ¹é…åº¦è¾¾åˆ°1.0"""
    print("\n" + "=" * 80)
    print("ğŸ” æ­¥éª¤3: è¿­ä»£ä¼˜åŒ–ï¼Œç›´åˆ°æ‰€æœ‰è‚¡ç¥¨åŒ¹é…åº¦è¾¾åˆ°1.0")
    print("=" * 80)
    
    max_iterations = 30
    iteration = 0
    all_perfect = False
    all_features_dict = {}
    
    while iteration < max_iterations and not all_perfect:
        iteration += 1
        print(f"\n{'='*80}")
        print(f"ğŸ”„ ç¬¬ {iteration} æ¬¡è¿­ä»£ï¼ˆæœ€å¤š {max_iterations} æ¬¡ï¼‰")
        print(f"{'='*80}")
        
        # éªŒè¯åŒ¹é…åº¦å¹¶æ”¶é›†ç‰¹å¾
        all_perfect, match_scores, failed_stocks, features_dict = test_all_stocks_match_score(analyzer, target_stocks)
        all_features_dict = features_dict
        
        if all_perfect:
            print(f"\nğŸ‰ æ‰€æœ‰ {len(target_stocks)} åªè‚¡ç¥¨çš„åŒ¹é…åº¦éƒ½è¾¾åˆ°1.0ï¼")
            break
        
        if iteration >= max_iterations:
            print(f"\nâš ï¸ å·²è¾¾åˆ°æœ€å¤§è¿­ä»£æ¬¡æ•° {max_iterations}ï¼Œåœæ­¢è®­ç»ƒ")
            print(f"   å¤±è´¥çš„è‚¡ç¥¨: {', '.join(failed_stocks)}")
            break
        
        print(f"\nâš ï¸ æœ‰ {len(failed_stocks)} åªè‚¡ç¥¨çš„åŒ¹é…åº¦æœªè¾¾åˆ°1.0ï¼Œå‡†å¤‡ä¼˜åŒ–ç‰¹å¾æ¨¡æ¿...")
        
        # ä¼˜åŒ–ç‰¹å¾æ¨¡æ¿
        optimized = optimize_feature_template(analyzer, target_stocks, all_features_dict)
        
        if optimized:
            print(f"âœ… ç‰¹å¾æ¨¡æ¿å·²ä¼˜åŒ–")
        else:
            print(f"âš ï¸ ç‰¹å¾æ¨¡æ¿ä¼˜åŒ–å¤±è´¥æˆ–æ— å˜åŒ–")
            # å¦‚æœæ— æ³•é€šè¿‡ä¼˜åŒ–æ¨¡æ¿æé«˜ï¼Œé‡æ–°è®­ç»ƒ
            print(f"   å°è¯•é‡æ–°è®­ç»ƒ...")
            train_result = analyzer.train_features()
            if not train_result.get('success'):
                print(f"âŒ é‡æ–°è®­ç»ƒå¤±è´¥: {train_result.get('message', '')}")
                break
            print(f"âœ… é‡æ–°è®­ç»ƒå®Œæˆ")
    
    return all_perfect

def save_model(analyzer, model_path='models/æ¨¡å‹11.json'):
    """ä¿å­˜æ¨¡å‹"""
    print("\n" + "=" * 80)
    print("ğŸ’¾ ä¿å­˜æ¨¡å‹")
    print("=" * 80)
    
    os.makedirs('models', exist_ok=True)
    
    if analyzer.save_model(model_path):
        print(f"\nâœ… æ¨¡å‹å·²ä¿å­˜åˆ°: {model_path}")
        return True
    else:
        print(f"\nâš ï¸ æ¨¡å‹ä¿å­˜å¤±è´¥")
        return False

def run_backtest(analyzer, start_date='2025-01-01', end_date='2025-12-31'):
    """è¿è¡Œå›æµ‹"""
    print("\n" + "=" * 80)
    print("ğŸ“Š è¿è¡Œå›æµ‹")
    print("=" * 80)
    
    # ç¡®ä¿æ¨¡å‹å·²åŠ è½½
    if not analyzer.trained_features or not analyzer.trained_features.get('common_features'):
        print("âš ï¸ æ¨¡å‹æœªè®­ç»ƒï¼Œå°è¯•ä»æ–‡ä»¶åŠ è½½æ¨¡å‹...")
        model_path = 'models/æ¨¡å‹11.json'
        if os.path.exists(model_path):
            if not analyzer.load_model(model_path, skip_network=True):
                print("âŒ æ¨¡å‹åŠ è½½å¤±è´¥")
                return {'success': False, 'message': 'æ¨¡å‹åŠ è½½å¤±è´¥'}
        else:
            print(f"âŒ æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {model_path}")
            return {'success': False, 'message': f'æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {model_path}'}
    else:
        print("âœ… æ¨¡å‹å·²åœ¨å†…å­˜ä¸­")
    
    # éªŒè¯æ¨¡å‹æ˜¯å¦æœ‰æ•ˆ
    if not analyzer.trained_features or not analyzer.trained_features.get('common_features'):
        print("âŒ æ¨¡å‹æ— æ•ˆ")
        return {'success': False, 'message': 'æ¨¡å‹æ— æ•ˆ'}
    
    validator = ModelValidator(analyzer=analyzer)
    
    result = validator.validate_backtest(
        start_date=start_date,
        end_date=end_date,
        min_match_score=0.83,
        max_market_cap=100.0,
        scan_mode='weekly',
        max_stocks_per_day=5,
        periods=[7, 28, 56, 84, 140],
        limit=None,
        use_parallel=True,
        max_workers=10,
        save_report=True,
        output_dir='.',
        report_prefix='backtest_model11_local'
    )
    
    return result

def report_progress(progress_info, interval_minutes=30):
    """æ¯30åˆ†é’ŸæŠ¥å‘Šä¸€æ¬¡è¿›å±•"""
    while True:
        time.sleep(interval_minutes * 60)
        print("\n" + "=" * 80)
        print(f"â° è¿›å±•æŠ¥å‘Šï¼ˆæ¯{interval_minutes}åˆ†é’Ÿï¼‰")
        print("=" * 80)
        for key, value in progress_info.items():
            print(f"  {key}: {value}")
        print("=" * 80)

def main():
    print("=" * 80)
    print("ğŸš€ é‡æ–°è®­ç»ƒæ¨¡å‹11å¹¶è¿è¡Œå›æµ‹ï¼ˆä½¿ç”¨æœ¬åœ°æ•°æ®ï¼‰")
    print("=" * 80)
    print()
    
    # è¿›åº¦ä¿¡æ¯
    progress_info = {
        'å½“å‰é˜¶æ®µ': 'åˆå§‹åŒ–',
        'å¼€å§‹æ—¶é—´': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
        'è®­ç»ƒçŠ¶æ€': 'æœªå¼€å§‹',
        'å›æµ‹çŠ¶æ€': 'æœªå¼€å§‹'
    }
    
    # å¯åŠ¨è¿›åº¦æŠ¥å‘Šçº¿ç¨‹
    progress_thread = threading.Thread(target=report_progress, args=(progress_info, 30), daemon=True)
    progress_thread.start()
    
    # åˆ›å»ºåˆ†æå™¨
    print("åˆå§‹åŒ–åˆ†æå™¨...")
    analyzer = BullStockAnalyzer(auto_load_default_stocks=False, auto_analyze_and_train=False)
    
    # æ£€æŸ¥æœ¬åœ°æ•°æ®å®Œæ•´æ€§
    progress_info['å½“å‰é˜¶æ®µ'] = 'æ£€æŸ¥æ•°æ®å®Œæ•´æ€§'
    is_complete, missing_weekly, missing_daily = check_local_data_completeness()
    
    if not is_complete:
        # ä¸‹è½½ç¼ºå¤±çš„æ•°æ®
        progress_info['å½“å‰é˜¶æ®µ'] = 'ä¸‹è½½ç¼ºå¤±æ•°æ®'
        download_missing_data(analyzer, missing_weekly, missing_daily)
        
        # å†æ¬¡æ£€æŸ¥
        is_complete, _, _ = check_local_data_completeness()
        if not is_complete:
            print("âŒ æ•°æ®ä»ç„¶ä¸å®Œæ•´ï¼Œè¯·æ£€æŸ¥ç½‘ç»œè¿æ¥")
            return
    
    # æ¸…ç©ºç°æœ‰æ•°æ®
    analyzer.analysis_results = {}
    analyzer.trained_features = None
    analyzer.bull_stocks = []
    
    # æ·»åŠ æ‰€æœ‰11åªè‚¡ç¥¨
    print("\næ·»åŠ 11åªç›®æ ‡è‚¡ç¥¨...")
    for stock_code in TRAINING_STOCKS:
        result = analyzer.add_bull_stock(stock_code)
        if result.get('success'):
            print(f"  âœ… å·²æ·»åŠ : {stock_code} {result.get('stock', {}).get('åç§°', '')}")
        else:
            print(f"  âš ï¸ æ·»åŠ å¤±è´¥: {stock_code} - {result.get('message', '')}")
    
    print(f"\nâœ… å·²åŠ è½½ {len(analyzer.bull_stocks)} åªå¤§ç‰›è‚¡")
    
    # è®­ç»ƒæ¨¡å‹
    progress_info['å½“å‰é˜¶æ®µ'] = 'è®­ç»ƒæ¨¡å‹'
    progress_info['è®­ç»ƒçŠ¶æ€'] = 'è¿›è¡Œä¸­'
    
    if not train_model(analyzer, TRAINING_STOCKS):
        print("âŒ è®­ç»ƒå¤±è´¥")
        return
    
    # ä¿å­˜è®­ç»ƒæ ·æœ¬åˆ—è¡¨
    if analyzer.trained_features:
        analyzer.trained_features['training_stocks'] = TRAINING_STOCKS
        print(f"âœ… å·²ä¿å­˜ {len(TRAINING_STOCKS)} åªè®­ç»ƒæ ·æœ¬åˆ°æ¨¡å‹")
    
    # è¿­ä»£ä¼˜åŒ–
    all_perfect = retrain_to_perfect_match(analyzer, TRAINING_STOCKS)
    
    if all_perfect:
        progress_info['è®­ç»ƒçŠ¶æ€'] = 'å®Œæˆï¼ˆæ‰€æœ‰è‚¡ç¥¨åŒ¹é…åº¦1.0ï¼‰'
    else:
        progress_info['è®­ç»ƒçŠ¶æ€'] = 'å®Œæˆï¼ˆéƒ¨åˆ†è‚¡ç¥¨æœªè¾¾åˆ°1.0ï¼‰'
    
    # ä¿å­˜æ¨¡å‹
    save_model(analyzer, 'models/æ¨¡å‹11.json')
    
    # ç¡®ä¿æ¨¡å‹å·²åŠ è½½ï¼ˆæ¨¡å‹å·²ç»åœ¨å†…å­˜ä¸­ï¼Œä¸éœ€è¦é‡æ–°åŠ è½½ï¼‰
    if not analyzer.trained_features:
        print("\nâš ï¸ æ¨¡å‹æœªåŠ è½½ï¼Œå°è¯•ä»æ–‡ä»¶åŠ è½½...")
        if not analyzer.load_model('models/æ¨¡å‹11.json', skip_network=True):
            print("âŒ æ¨¡å‹åŠ è½½å¤±è´¥")
            return
    else:
        print("\nâœ… æ¨¡å‹å·²åœ¨å†…å­˜ä¸­ï¼Œæ— éœ€é‡æ–°åŠ è½½")
    
    # è¿è¡Œå›æµ‹
    progress_info['å½“å‰é˜¶æ®µ'] = 'è¿è¡Œå›æµ‹'
    progress_info['å›æµ‹çŠ¶æ€'] = 'è¿›è¡Œä¸­'
    
    backtest_result = run_backtest(analyzer, start_date='2025-01-01', end_date='2025-12-31')
    
    if backtest_result.get('success'):
        progress_info['å›æµ‹çŠ¶æ€'] = 'å®Œæˆ'
        print("\n" + "=" * 80)
        print("âœ… å›æµ‹å®Œæˆï¼")
        print("=" * 80)
        
        stats = backtest_result.get('statistics', {})
        print(f"\nğŸ“Š å›æµ‹ç»Ÿè®¡:")
        print(f"  æ€»æ‰«ææ¬¡æ•°: {stats.get('total_trades', 0)}")
        print(f"  æœ‰æ•ˆäº¤æ˜“æ•°: {stats.get('valid_trades', 0)}")
        
        for period_key in ['1å‘¨', '4å‘¨', '8å‘¨', '12å‘¨', '20å‘¨']:
            if period_key in stats:
                period_stats = stats[period_key]
                avg_return = period_stats.get('average_return', 0)
                win_rate = period_stats.get('win_rate', 0)
                print(f"  {period_key}æ”¶ç›Š: å¹³å‡ {avg_return:.2f}%, èƒœç‡ {win_rate:.1f}%")
        
        print(f"\nğŸ“„ è¯¦ç»†æŠ¥å‘Šå·²ä¿å­˜åˆ°:")
        print(f"  - {backtest_result.get('text_report_path', 'N/A')}")
        print(f"  - {backtest_result.get('json_report_path', 'N/A')}")
    else:
        progress_info['å›æµ‹çŠ¶æ€'] = f"å¤±è´¥: {backtest_result.get('message', 'æœªçŸ¥é”™è¯¯')}"
        print(f"\nâŒ å›æµ‹å¤±è´¥: {backtest_result.get('message', 'æœªçŸ¥é”™è¯¯')}")
    
    progress_info['å½“å‰é˜¶æ®µ'] = 'å®Œæˆ'
    progress_info['ç»“æŸæ—¶é—´'] = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
    
    print("\n" + "=" * 80)
    print("âœ… æ‰€æœ‰ä»»åŠ¡å®Œæˆï¼")
    print("=" * 80)

if __name__ == '__main__':
    try:
        main()
    except KeyboardInterrupt:
        print("\nâš ï¸ ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        print(f"\nâŒ å‘ç”Ÿé”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
