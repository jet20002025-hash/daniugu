#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
é€šè¾¾ä¿¡æ•°æ®å¯¼å…¥å·¥å…·
å°†é€šè¾¾ä¿¡çš„ .day æ–‡ä»¶è½¬æ¢ä¸ºç³»ç»Ÿå¯ç”¨çš„ CSV æ ¼å¼
"""

import os
import struct
import pandas as pd
from datetime import datetime
import json

# é…ç½®
CACHE_DIR = 'cache'
DAILY_DIR = os.path.join(CACHE_DIR, 'daily_kline')
WEEKLY_DIR = os.path.join(CACHE_DIR, 'weekly_kline')

def read_tdx_day_file(file_path):
    """
    è¯»å–é€šè¾¾ä¿¡ .day æ–‡ä»¶
    :param file_path: .day æ–‡ä»¶è·¯å¾„
    :return: DataFrameï¼ŒåŒ…å«æ—¥æœŸã€å¼€ç›˜ã€æ”¶ç›˜ã€æœ€é«˜ã€æœ€ä½ã€æˆäº¤é‡ã€æˆäº¤é¢
    """
    try:
        with open(file_path, 'rb') as f:
            data = f.read()
        
        if len(data) % 32 != 0:
            print(f"âš ï¸ è­¦å‘Šï¼š{file_path} æ–‡ä»¶å¤§å°ä¸æ˜¯32çš„å€æ•°ï¼Œå¯èƒ½æŸå")
            return None
        
        records = []
        for i in range(0, len(data), 32):
            # æ¯32å­—èŠ‚ä¸€æ¡è®°å½•
            record = data[i:i+32]
            if len(record) < 32:
                break
            
            # è§£ææ•°æ®
            # 00~03: å¹´æœˆæ—¥ (YYYYMMDD)
            date_int = struct.unpack('I', record[0:4])[0]
            year = date_int // 10000
            month = (date_int % 10000) // 100
            day = date_int % 100
            date_str = f"{year:04d}-{month:02d}-{day:02d}"
            
            # 04~07: å¼€ç›˜ä»· (éœ€é™¤ä»¥100)
            open_price = struct.unpack('I', record[4:8])[0] / 100.0
            
            # 08~11: æœ€é«˜ä»· (éœ€é™¤ä»¥100)
            high_price = struct.unpack('I', record[8:12])[0] / 100.0
            
            # 12~15: æœ€ä½ä»· (éœ€é™¤ä»¥100)
            low_price = struct.unpack('I', record[12:16])[0] / 100.0
            
            # 16~19: æ”¶ç›˜ä»· (éœ€é™¤ä»¥100)
            close_price = struct.unpack('I', record[16:20])[0] / 100.0
            
            # 20~23: æˆäº¤é¢ (æµ®ç‚¹æ•°ï¼Œå•ä½ï¼šå…ƒ)
            amount = struct.unpack('f', record[20:24])[0]
            
            # 24~27: æˆäº¤é‡ (éœ€é™¤ä»¥100ï¼Œå•ä½ï¼šæ‰‹)
            volume = struct.unpack('I', record[24:28])[0] / 100.0
            
            records.append({
                'æ—¥æœŸ': date_str,
                'å¼€ç›˜': open_price,
                'æ”¶ç›˜': close_price,
                'æœ€é«˜': high_price,
                'æœ€ä½': low_price,
                'æˆäº¤é‡': int(volume * 100),  # è½¬æ¢ä¸ºè‚¡æ•°ï¼ˆæ‰‹ * 100ï¼‰
                'æˆäº¤é¢': amount
            })
        
        if not records:
            return None
        
        df = pd.DataFrame(records)
        df = df.sort_values('æ—¥æœŸ').reset_index(drop=True)
        return df
    
    except Exception as e:
        print(f"âŒ è¯»å– {file_path} å¤±è´¥: {e}")
        return None

def convert_tdx_to_csv(tdx_dir, output_dir=None):
    """
    æ‰¹é‡è½¬æ¢é€šè¾¾ä¿¡æ•°æ®ç›®å½•ä¸­çš„æ‰€æœ‰ .day æ–‡ä»¶
    :param tdx_dir: é€šè¾¾ä¿¡æ•°æ®ç›®å½•ï¼ˆå¦‚ vipdoc/sh/lday æˆ– vipdoc/sz/ldayï¼‰
    :param output_dir: è¾“å‡ºç›®å½•ï¼ˆé»˜è®¤ä½¿ç”¨ cache/daily_klineï¼‰
    :return: è½¬æ¢ç»Ÿè®¡ä¿¡æ¯
    """
    if output_dir is None:
        output_dir = DAILY_DIR
    
    os.makedirs(output_dir, exist_ok=True)
    
    if not os.path.exists(tdx_dir):
        print(f"âŒ ç›®å½•ä¸å­˜åœ¨: {tdx_dir}")
        return {'success': 0, 'failed': 0, 'skipped': 0}
    
    # è·å–æ‰€æœ‰ .day æ–‡ä»¶
    day_files = [f for f in os.listdir(tdx_dir) if f.endswith('.day')]
    
    if not day_files:
        print(f"âš ï¸ ç›®å½•ä¸­æ²¡æœ‰æ‰¾åˆ° .day æ–‡ä»¶: {tdx_dir}")
        return {'success': 0, 'failed': 0, 'skipped': 0}
    
    print(f"ğŸ“ æ‰¾åˆ° {len(day_files)} ä¸ª .day æ–‡ä»¶")
    print(f"ğŸ“‚ è¾“å‡ºç›®å½•: {output_dir}")
    print()
    
    success_count = 0
    failed_count = 0
    skipped_count = 0
    
    # åŠ è½½æ•°æ®æ ‡è®°æ–‡ä»¶
    markers = load_data_markers()
    
    total_files = len(day_files)
    print(f"ğŸ“Š æ€»å…± {total_files} ä¸ªæ–‡ä»¶éœ€è¦è½¬æ¢")
    print()
    
    for idx, day_file in enumerate(sorted(day_files), 1):
        # æ¯100ä¸ªæ–‡ä»¶æ˜¾ç¤ºä¸€æ¬¡è¿›åº¦
        if idx % 100 == 0 or idx == total_files:
            print(f"ğŸ“ˆ è¿›åº¦: {idx}/{total_files} ({idx*100//total_files}%)")
        # æå–è‚¡ç¥¨ä»£ç 
        # æ ¼å¼ï¼šsh600000.day æˆ– sz000001.day
        base_name = day_file.replace('.day', '')
        if base_name.startswith('sh') or base_name.startswith('sz'):
            stock_code = base_name[2:]  # å»æ‰å¸‚åœºå‰ç¼€
        else:
            print(f"âš ï¸ è·³è¿‡æ— æ³•è¯†åˆ«çš„æ–‡ä»¶: {day_file}")
            skipped_count += 1
            continue
        
        tdx_file_path = os.path.join(tdx_dir, day_file)
        csv_file_path = os.path.join(output_dir, f'{stock_code}.csv')
        
        try:
            # è¯»å–é€šè¾¾ä¿¡æ•°æ®
            df = read_tdx_day_file(tdx_file_path)
            
            if df is None or len(df) == 0:
                print(f"âš ï¸ {stock_code}: æ•°æ®ä¸ºç©ºï¼Œè·³è¿‡")
                skipped_count += 1
                continue
            
            # å¦‚æœCSVæ–‡ä»¶å·²å­˜åœ¨ï¼Œåˆå¹¶æ•°æ®ï¼ˆè¿½åŠ æ–°æ•°æ®ï¼‰
            if os.path.exists(csv_file_path):
                try:
                    existing_df = pd.read_csv(csv_file_path)
                    # åˆå¹¶æ•°æ®ï¼ˆå»é‡ï¼‰
                    combined_df = pd.concat([existing_df, df], ignore_index=True)
                    combined_df = combined_df.drop_duplicates(subset=['æ—¥æœŸ'], keep='last')
                    combined_df = combined_df.sort_values('æ—¥æœŸ').reset_index(drop=True)
                    df = combined_df
                except Exception as e:
                    print(f"âš ï¸ {stock_code}: åˆå¹¶ç°æœ‰æ•°æ®å¤±è´¥ï¼Œè¦†ç›–ä¿å­˜: {e}")
            
            # ä¿å­˜ä¸ºCSV
            df.to_csv(csv_file_path, index=False, encoding='utf-8')
            
            # æ›´æ–°æ•°æ®æ ‡è®°
            latest_date = str(df['æ—¥æœŸ'].max())[:10]
            update_marker(stock_code, daily_latest_date=latest_date, markers=markers)
            
            print(f"âœ… {stock_code}: è½¬æ¢æˆåŠŸï¼Œå…± {len(df)} æ¡æ•°æ®ï¼Œæœ€æ–°æ—¥æœŸ: {latest_date}")
            success_count += 1
            
        except Exception as e:
            print(f"âŒ {stock_code}: è½¬æ¢å¤±è´¥: {e}")
            failed_count += 1
    
    # ä¿å­˜æ ‡è®°æ–‡ä»¶
    save_data_markers(markers)
    
    print()
    print("=" * 60)
    print(f"è½¬æ¢å®Œæˆï¼æˆåŠŸ: {success_count}, å¤±è´¥: {failed_count}, è·³è¿‡: {skipped_count}")
    print("=" * 60)
    
    return {
        'success': success_count,
        'failed': failed_count,
        'skipped': skipped_count
    }

def generate_weekly_kline_from_daily():
    """
    ä»æ—¥Kçº¿æ•°æ®ç”Ÿæˆå‘¨Kçº¿æ•°æ®
    """
    os.makedirs(WEEKLY_DIR, exist_ok=True)
    
    daily_files = [f for f in os.listdir(DAILY_DIR) if f.endswith('.csv')]
    
    print(f"ğŸ“Š ä» {len(daily_files)} ä¸ªæ—¥Kçº¿æ–‡ä»¶ç”Ÿæˆå‘¨Kçº¿æ•°æ®...")
    
    markers = load_data_markers()
    success_count = 0
    
    for daily_file in sorted(daily_files):
        stock_code = daily_file.replace('.csv', '')
        daily_path = os.path.join(DAILY_DIR, daily_file)
        weekly_path = os.path.join(WEEKLY_DIR, daily_file)
        
        try:
            daily_df = pd.read_csv(daily_path)
            if len(daily_df) == 0:
                continue
            
            # è½¬æ¢æ—¥æœŸæ ¼å¼
            daily_df['æ—¥æœŸ'] = pd.to_datetime(daily_df['æ—¥æœŸ'])
            
            # æŒ‰å‘¨åˆ†ç»„
            daily_df['å¹´å‘¨'] = daily_df['æ—¥æœŸ'].dt.to_period('W')
            
            # è®¡ç®—å‘¨Kçº¿æ•°æ®
            weekly_data = []
            for week, group in daily_df.groupby('å¹´å‘¨'):
                weekly_data.append({
                    'æ—¥æœŸ': group['æ—¥æœŸ'].max().strftime('%Y-%m-%d'),  # ä½¿ç”¨è¯¥å‘¨æœ€åä¸€ä¸ªäº¤æ˜“æ—¥
                    'å¼€ç›˜': group.iloc[0]['å¼€ç›˜'],
                    'æ”¶ç›˜': group.iloc[-1]['æ”¶ç›˜'],
                    'æœ€é«˜': group['æœ€é«˜'].max(),
                    'æœ€ä½': group['æœ€ä½'].min(),
                    'å‘¨æˆäº¤é‡': int(group['æˆäº¤é‡'].sum())
                })
            
            if weekly_data:
                weekly_df = pd.DataFrame(weekly_data)
                weekly_df = weekly_df.sort_values('æ—¥æœŸ').reset_index(drop=True)
                weekly_df.to_csv(weekly_path, index=False, encoding='utf-8')
                
                # æ›´æ–°æ ‡è®°
                latest_date = str(weekly_df['æ—¥æœŸ'].max())[:10]
                update_marker(stock_code, weekly_latest_date=latest_date, markers=markers)
                
                success_count += 1
                if success_count % 100 == 0:
                    print(f"  å·²å¤„ç† {success_count} åªè‚¡ç¥¨...")
        
        except Exception as e:
            print(f"âš ï¸ {stock_code}: ç”Ÿæˆå‘¨Kçº¿å¤±è´¥: {e}")
    
    save_data_markers(markers)
    print(f"âœ… å‘¨Kçº¿ç”Ÿæˆå®Œæˆï¼Œå…± {success_count} åªè‚¡ç¥¨")

def load_data_markers():
    """åŠ è½½æ•°æ®æ ‡è®°æ–‡ä»¶"""
    marker_file = os.path.join(CACHE_DIR, 'data_markers.json')
    if os.path.exists(marker_file):
        try:
            with open(marker_file, 'r', encoding='utf-8') as f:
                return json.load(f)
        except Exception:
            return {}
    return {}

def save_data_markers(markers):
    """ä¿å­˜æ•°æ®æ ‡è®°æ–‡ä»¶"""
    marker_file = os.path.join(CACHE_DIR, 'data_markers.json')
    with open(marker_file, 'w', encoding='utf-8') as f:
        json.dump(markers, f, ensure_ascii=False, indent=2)

def update_marker(code, daily_latest_date=None, weekly_latest_date=None, markers=None):
    """æ›´æ–°è‚¡ç¥¨çš„æ•°æ®æ ‡è®°"""
    if markers is None:
        markers = load_data_markers()
    if code not in markers:
        markers[code] = {}
    if daily_latest_date:
        markers[code]['daily_latest_date'] = daily_latest_date
    if weekly_latest_date:
        markers[code]['weekly_latest_date'] = weekly_latest_date

def main():
    print("=" * 60)
    print("ğŸ“Š é€šè¾¾ä¿¡æ•°æ®å¯¼å…¥å·¥å…·")
    print("=" * 60)
    print()
    
    # æç¤ºç”¨æˆ·è¾“å…¥é€šè¾¾ä¿¡æ•°æ®ç›®å½•
    print("è¯·æä¾›é€šè¾¾ä¿¡æ•°æ®ç›®å½•è·¯å¾„ï¼š")
    print("  ç¤ºä¾‹ï¼ˆWindowsï¼‰: C:\\new_tdx\\vipdoc\\sh\\lday")
    print("  ç¤ºä¾‹ï¼ˆWindowsï¼‰: C:\\new_tdx\\vipdoc\\sz\\lday")
    print("  ç¤ºä¾‹ï¼ˆMacï¼‰: /Users/ç”¨æˆ·å/é€šè¾¾ä¿¡/vipdoc/sh/lday")
    print()
    
    # å¯ä»¥æ‰‹åŠ¨æŒ‡å®šç›®å½•ï¼Œæˆ–è€…è®©ç”¨æˆ·è¾“å…¥
    import sys
    if len(sys.argv) > 1:
        tdx_dir = sys.argv[1]
    else:
        tdx_dir = input("è¯·è¾“å…¥é€šè¾¾ä¿¡æ•°æ®ç›®å½•è·¯å¾„ï¼ˆç›´æ¥å›è½¦è·³è¿‡ï¼‰: ").strip()
        if not tdx_dir:
            print("âš ï¸ æœªæä¾›ç›®å½•ï¼Œé€€å‡º")
            return
    
    if not os.path.exists(tdx_dir):
        print(f"âŒ ç›®å½•ä¸å­˜åœ¨: {tdx_dir}")
        return
    
    # è½¬æ¢æ—¥Kçº¿æ•°æ®
    print()
    print("=" * 60)
    print("æ­¥éª¤1: è½¬æ¢æ—¥Kçº¿æ•°æ®")
    print("=" * 60)
    result = convert_tdx_to_csv(tdx_dir)
    
    # ç”Ÿæˆå‘¨Kçº¿æ•°æ®
    print()
    print("=" * 60)
    print("æ­¥éª¤2: ç”Ÿæˆå‘¨Kçº¿æ•°æ®")
    print("=" * 60)
    generate_weekly_kline_from_daily()
    
    print()
    print("=" * 60)
    print("âœ… å¯¼å…¥å®Œæˆï¼")
    print("=" * 60)
    print()
    print("æ•°æ®å·²ä¿å­˜åˆ°:")
    print(f"  - æ—¥Kçº¿: {DAILY_DIR}")
    print(f"  - å‘¨Kçº¿: {WEEKLY_DIR}")
    print()
    print("ç°åœ¨å¯ä»¥åœ¨ç³»ç»Ÿä¸­ä½¿ç”¨è¿™äº›æ•°æ®äº†ï¼")

if __name__ == '__main__':
    main()
