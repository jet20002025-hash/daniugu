#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç¥ç»ç½‘ç»œç‰ˆæœ¬çš„è‚¡ç¥¨ç‰¹å¾åŒ¹é…æ¨¡å‹
ä½¿ç”¨å¤šå±‚æ„ŸçŸ¥å™¨ï¼ˆMLPï¼‰è¿›è¡Œç‰¹å¾åŒ¹é…åº¦é¢„æµ‹
"""
import numpy as np
import json
import os
from datetime import datetime
from typing import Dict, List, Optional, Tuple
import pickle

try:
    from sklearn.neural_network import MLPRegressor
    from sklearn.preprocessing import StandardScaler
    from sklearn.model_selection import train_test_split
    SKLEARN_AVAILABLE = True
except ImportError:
    SKLEARN_AVAILABLE = False
    print("âš ï¸ scikit-learnæœªå®‰è£…ï¼Œæ— æ³•ä½¿ç”¨ç¥ç»ç½‘ç»œæ¨¡å‹")
    print("   è¯·è¿è¡Œ: pip install scikit-learn")

from bull_stock_analyzer import BullStockAnalyzer


class BullStockNeuralNetwork:
    """åŸºäºç¥ç»ç½‘ç»œçš„å¤§ç‰›è‚¡ç‰¹å¾åŒ¹é…æ¨¡å‹"""
    
    def __init__(self, hidden_layer_sizes: Tuple = (128, 64, 32), random_state: int = 42):
        """
        åˆå§‹åŒ–ç¥ç»ç½‘ç»œæ¨¡å‹
        :param hidden_layer_sizes: éšè—å±‚ç»“æ„ï¼Œé»˜è®¤(128, 64, 32)
        :param random_state: éšæœºç§å­
        """
        if not SKLEARN_AVAILABLE:
            raise ImportError("scikit-learnæœªå®‰è£…ï¼Œè¯·è¿è¡Œ: pip install scikit-learn")
        
        self.hidden_layer_sizes = hidden_layer_sizes
        self.random_state = random_state
        self.model = None
        self.scaler = StandardScaler()
        self.feature_names = []
        self.is_trained = False
        self.trained_at = None
        
    def prepare_training_data(self, analyzer: BullStockAnalyzer, target_stocks: List[str]) -> Tuple[np.ndarray, np.ndarray]:
        """
        å‡†å¤‡è®­ç»ƒæ•°æ®
        :param analyzer: BullStockAnalyzerå®ä¾‹
        :param target_stocks: è®­ç»ƒè‚¡ç¥¨åˆ—è¡¨
        :return: (X, y) ç‰¹å¾çŸ©é˜µå’Œæ ‡ç­¾ï¼ˆåŒ¹é…åº¦ï¼‰
        """
        print("=" * 80)
        print("ğŸ“Š å‡†å¤‡ç¥ç»ç½‘ç»œè®­ç»ƒæ•°æ®")
        print("=" * 80)
        
        # å…ˆæ·»åŠ è‚¡ç¥¨
        print("\nğŸ“Š æ­¥éª¤0: æ·»åŠ è®­ç»ƒè‚¡ç¥¨...")
        for stock_code in target_stocks:
            analyzer.add_bull_stock(stock_code)
        
        # åˆ†ææ‰€æœ‰è‚¡ç¥¨
        print("\nğŸ“Š æ­¥éª¤1: åˆ†ææ‰€æœ‰è®­ç»ƒè‚¡ç¥¨...")
        for stock_code in target_stocks:
            print(f"  åˆ†æ {stock_code}...", end=" ", flush=True)
            result = analyzer.analyze_bull_stock(stock_code)
            if result.get('success'):
                print("âœ…")
            else:
                print(f"âŒ {result.get('message', '')}")
        
        # è®­ç»ƒç‰¹å¾æ¨¡æ¿ï¼ˆè·å–ç‰¹å¾åˆ—è¡¨ï¼‰
        print("\nğŸ“Š æ­¥éª¤2: è®­ç»ƒç‰¹å¾æ¨¡æ¿...")
        train_result = analyzer.train_features()
        if not train_result.get('success'):
            raise ValueError(f"ç‰¹å¾è®­ç»ƒå¤±è´¥: {train_result.get('message', '')}")
        
        # è·å–ç‰¹å¾åç§°
        common_features = analyzer.trained_features.get('common_features', {})
        self.feature_names = sorted(common_features.keys())
        print(f"  ç‰¹å¾æ•°é‡: {len(self.feature_names)}")
        
        # æå–æ‰€æœ‰è‚¡ç¥¨çš„ç‰¹å¾
        print("\nğŸ“Š æ­¥éª¤3: æå–æ‰€æœ‰è‚¡ç¥¨çš„ç‰¹å¾...")
        X_list = []
        y_list = []
        
        for stock_code in target_stocks:
            if stock_code not in analyzer.analysis_results:
                continue
            
            analysis_result = analyzer.analysis_results[stock_code]
            interval = analysis_result.get('interval')
            if not interval or interval.get('èµ·ç‚¹ç´¢å¼•') is None:
                continue
            
            start_idx = interval.get('èµ·ç‚¹ç´¢å¼•')
            weekly_df = analyzer.fetcher.get_weekly_kline(stock_code, period="2y")
            if weekly_df is None or len(weekly_df) == 0:
                continue
            
            # æ‰¾åˆ°æˆäº¤é‡çªå¢ç‚¹
            volume_surge_idx = analyzer.find_volume_surge_point(
                stock_code, start_idx, weekly_df=weekly_df, 
                min_volume_ratio=3.0, lookback_weeks=52
            )
            if volume_surge_idx is None:
                volume_surge_idx = max(0, start_idx - 20)
            
            # æå–ç‰¹å¾
            features = analyzer.extract_features_at_start_point(
                stock_code, volume_surge_idx, lookback_weeks=40, weekly_df=weekly_df
            )
            if features is None:
                continue
            
            # æ„å»ºç‰¹å¾å‘é‡ï¼ˆæŒ‰ç…§ç‰¹å¾åç§°é¡ºåºï¼‰
            feature_vector = []
            for feature_name in self.feature_names:
                if feature_name in features:
                    feature_vector.append(float(features[feature_name]) if features[feature_name] is not None else 0.0)
                else:
                    feature_vector.append(0.0)
            
            X_list.append(feature_vector)
            
            # æ ‡ç­¾ï¼šå¯¹äºè®­ç»ƒæ ·æœ¬ï¼ŒæœŸæœ›åŒ¹é…åº¦ä¸º1.0
            # è¿™é‡Œæˆ‘ä»¬ä½¿ç”¨å½“å‰ç»Ÿè®¡æ¨¡å‹çš„åŒ¹é…åº¦ä½œä¸ºæ ‡ç­¾
            match_score = analyzer._calculate_match_score(
                features, common_features, tolerance=0.3
            )
            total_match = match_score.get('æ€»åŒ¹é…åº¦', 0.95)  # è®­ç»ƒæ ·æœ¬æœŸæœ›é«˜åŒ¹é…åº¦
            y_list.append(total_match)
            
            stock_name = analysis_result.get('stock_info', {}).get('åç§°', stock_code)
            print(f"  âœ… {stock_code} {stock_name}: ç‰¹å¾æ•°={len(feature_vector)}, æ ‡ç­¾={total_match:.3f}")
        
        X = np.array(X_list)
        y = np.array(y_list)
        
        print(f"\nğŸ“Š è®­ç»ƒæ•°æ®å‡†å¤‡å®Œæˆ:")
        print(f"  æ ·æœ¬æ•°: {len(X)}")
        print(f"  ç‰¹å¾æ•°: {X.shape[1]}")
        print(f"  æ ‡ç­¾èŒƒå›´: [{y.min():.3f}, {y.max():.3f}]")
        print(f"  æ ‡ç­¾å‡å€¼: {y.mean():.3f}")
        
        return X, y
    
    def train(self, X: np.ndarray, y: np.ndarray, test_size: float = 0.2):
        """
        è®­ç»ƒç¥ç»ç½‘ç»œæ¨¡å‹
        :param X: ç‰¹å¾çŸ©é˜µ
        :param y: æ ‡ç­¾ï¼ˆåŒ¹é…åº¦ï¼‰
        :param test_size: æµ‹è¯•é›†æ¯”ä¾‹
        """
        if not SKLEARN_AVAILABLE:
            raise ImportError("scikit-learnæœªå®‰è£…ï¼Œè¯·è¿è¡Œ: pip install scikit-learn")
        
        print("\n" + "=" * 80)
        print("ğŸ§  è®­ç»ƒç¥ç»ç½‘ç»œæ¨¡å‹")
        print("=" * 80)
        
        # æ•°æ®æ ‡å‡†åŒ–
        print("\nğŸ“Š æ­¥éª¤1: æ ‡å‡†åŒ–ç‰¹å¾æ•°æ®...")
        X_scaled = self.scaler.fit_transform(X)
        
        # åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†
        if len(X) > 5:  # å¦‚æœæ ·æœ¬æ•°è¶³å¤Ÿï¼Œåˆ’åˆ†æµ‹è¯•é›†
            X_train, X_test, y_train, y_test = train_test_split(
                X_scaled, y, test_size=test_size, random_state=self.random_state
            )
            print(f"  è®­ç»ƒé›†: {len(X_train)} æ ·æœ¬")
            print(f"  æµ‹è¯•é›†: {len(X_test)} æ ·æœ¬")
        else:
            X_train, y_train = X_scaled, y
            X_test, y_test = None, None
            print(f"  æ ·æœ¬æ•°è¾ƒå°‘ï¼Œä½¿ç”¨å…¨éƒ¨æ•°æ®è®­ç»ƒ: {len(X_train)} æ ·æœ¬")
        
        # åˆ›å»ºæ¨¡å‹
        print(f"\nğŸ“Š æ­¥éª¤2: åˆ›å»ºç¥ç»ç½‘ç»œæ¨¡å‹...")
        print(f"  éšè—å±‚ç»“æ„: {self.hidden_layer_sizes}")
        self.model = MLPRegressor(
            hidden_layer_sizes=self.hidden_layer_sizes,
            activation='relu',
            solver='adam',
            alpha=0.0001,  # L2æ­£åˆ™åŒ–ç³»æ•°
            batch_size='auto',
            learning_rate='adaptive',
            learning_rate_init=0.001,
            max_iter=1000,
            shuffle=True,
            random_state=self.random_state,
            tol=1e-4,
            verbose=False,
            warm_start=False,
            momentum=0.9,
            nesterovs_momentum=True,
            early_stopping=False,
            validation_fraction=0.1,
            beta_1=0.9,
            beta_2=0.999,
            epsilon=1e-8
        )
        
        # è®­ç»ƒæ¨¡å‹
        print(f"\nğŸ“Š æ­¥éª¤3: è®­ç»ƒæ¨¡å‹...")
        self.model.fit(X_train, y_train)
        self.is_trained = True
        self.trained_at = datetime.now()
        
        # è¯„ä¼°æ¨¡å‹
        print(f"\nğŸ“Š æ­¥éª¤4: è¯„ä¼°æ¨¡å‹...")
        train_score = self.model.score(X_train, y_train)
        print(f"  è®­ç»ƒé›†RÂ²åˆ†æ•°: {train_score:.4f}")
        
        if X_test is not None:
            test_score = self.model.score(X_test, y_test)
            print(f"  æµ‹è¯•é›†RÂ²åˆ†æ•°: {test_score:.4f}")
            
            # é¢„æµ‹æµ‹è¯•é›†
            y_pred = self.model.predict(X_test)
            mae = np.mean(np.abs(y_pred - y_test))
            rmse = np.sqrt(np.mean((y_pred - y_test) ** 2))
            print(f"  æµ‹è¯•é›†MAE: {mae:.4f}")
            print(f"  æµ‹è¯•é›†RMSE: {rmse:.4f}")
        
        print("\nâœ… ç¥ç»ç½‘ç»œæ¨¡å‹è®­ç»ƒå®Œæˆï¼")
        print("=" * 80)
    
    def predict(self, features: Dict) -> float:
        """
        é¢„æµ‹åŒ¹é…åº¦
        :param features: ç‰¹å¾å­—å…¸
        :return: åŒ¹é…åº¦ï¼ˆ0-1ï¼‰
        """
        if not self.is_trained:
            raise ValueError("æ¨¡å‹å°šæœªè®­ç»ƒï¼Œè¯·å…ˆè°ƒç”¨train()æ–¹æ³•")
        
        # æ„å»ºç‰¹å¾å‘é‡
        feature_vector = []
        for feature_name in self.feature_names:
            if feature_name in features:
                feature_vector.append(float(features[feature_name]) if features[feature_name] is not None else 0.0)
            else:
                feature_vector.append(0.0)
        
        X = np.array([feature_vector])
        X_scaled = self.scaler.transform(X)
        
        # é¢„æµ‹
        y_pred = self.model.predict(X_scaled)[0]
        
        # é™åˆ¶åœ¨[0, 1]èŒƒå›´å†…
        return max(0.0, min(1.0, float(y_pred)))
    
    def save_model(self, filepath: str):
        """
        ä¿å­˜æ¨¡å‹
        :param filepath: ä¿å­˜è·¯å¾„
        """
        if not self.is_trained:
            raise ValueError("æ¨¡å‹å°šæœªè®­ç»ƒï¼Œæ— æ³•ä¿å­˜")
        
        model_data = {
            'model': self.model,
            'scaler': self.scaler,
            'feature_names': self.feature_names,
            'hidden_layer_sizes': self.hidden_layer_sizes,
            'random_state': self.random_state,
            'trained_at': self.trained_at.isoformat() if self.trained_at else None,
            'is_trained': self.is_trained
        }
        
        os.makedirs(os.path.dirname(filepath) if os.path.dirname(filepath) else '.', exist_ok=True)
        
        with open(filepath, 'wb') as f:
            pickle.dump(model_data, f)
        
        print(f"âœ… æ¨¡å‹å·²ä¿å­˜åˆ°: {filepath}")
    
    def load_model(self, filepath: str):
        """
        åŠ è½½æ¨¡å‹
        :param filepath: æ¨¡å‹è·¯å¾„
        """
        with open(filepath, 'rb') as f:
            model_data = pickle.load(f)
        
        self.model = model_data['model']
        self.scaler = model_data['scaler']
        self.feature_names = model_data['feature_names']
        self.hidden_layer_sizes = model_data['hidden_layer_sizes']
        self.random_state = model_data['random_state']
        self.is_trained = model_data['is_trained']
        self.trained_at = datetime.fromisoformat(model_data['trained_at']) if model_data['trained_at'] else None
        
        print(f"âœ… æ¨¡å‹å·²åŠ è½½: {filepath}")
        print(f"  è®­ç»ƒæ—¶é—´: {self.trained_at}")
        print(f"  éšè—å±‚ç»“æ„: {self.hidden_layer_sizes}")
        print(f"  ç‰¹å¾æ•°: {len(self.feature_names)}")


def test_neural_network_model():
    """æµ‹è¯•ç¥ç»ç½‘ç»œæ¨¡å‹"""
    print("=" * 80)
    print("ğŸ§  æµ‹è¯•ç¥ç»ç½‘ç»œæ¨¡å‹")
    print("=" * 80)
    
    if not SKLEARN_AVAILABLE:
        print("\nâŒ scikit-learnæœªå®‰è£…ï¼Œæ— æ³•æµ‹è¯•ç¥ç»ç½‘ç»œæ¨¡å‹")
        print("   è¯·è¿è¡Œ: pip install scikit-learn")
        return
    
    # 11åªè®­ç»ƒè‚¡ç¥¨
    target_stocks = ['000592', '002104', '002759', '300436', '301005', '301232', 
                     '002788', '603778', '603122', '600343', '603216']
    
    # åˆ›å»ºåˆ†æå™¨
    print("\nåˆå§‹åŒ–åˆ†æå™¨...")
    analyzer = BullStockAnalyzer(auto_load_default_stocks=False, auto_analyze_and_train=False)
    
    # åˆ›å»ºç¥ç»ç½‘ç»œæ¨¡å‹
    print("\nåˆ›å»ºç¥ç»ç½‘ç»œæ¨¡å‹...")
    nn_model = BullStockNeuralNetwork(
        hidden_layer_sizes=(128, 64, 32),  # 3å±‚éšè—å±‚
        random_state=42
    )
    
    # å‡†å¤‡è®­ç»ƒæ•°æ®
    X, y = nn_model.prepare_training_data(analyzer, target_stocks)
    
    # è®­ç»ƒæ¨¡å‹
    nn_model.train(X, y, test_size=0.2)
    
    # æµ‹è¯•æ‰€æœ‰è®­ç»ƒè‚¡ç¥¨
    print("\n" + "=" * 80)
    print("ğŸ” æµ‹è¯•ç¥ç»ç½‘ç»œæ¨¡å‹é¢„æµ‹ç»“æœ")
    print("=" * 80)
    
    match_scores = {}
    for stock_code in target_stocks:
        if stock_code not in analyzer.analysis_results:
            continue
        
        analysis_result = analyzer.analysis_results[stock_code]
        interval = analysis_result.get('interval')
        if not interval or interval.get('èµ·ç‚¹ç´¢å¼•') is None:
            continue
        
        start_idx = interval.get('èµ·ç‚¹ç´¢å¼•')
        weekly_df = analyzer.fetcher.get_weekly_kline(stock_code, period="2y")
        if weekly_df is None or len(weekly_df) == 0:
            continue
        
        volume_surge_idx = analyzer.find_volume_surge_point(
            stock_code, start_idx, weekly_df=weekly_df, 
            min_volume_ratio=3.0, lookback_weeks=52
        )
        if volume_surge_idx is None:
            volume_surge_idx = max(0, start_idx - 20)
        
        features = analyzer.extract_features_at_start_point(
            stock_code, volume_surge_idx, lookback_weeks=40, weekly_df=weekly_df
        )
        if features is None:
            continue
        
        # ä½¿ç”¨ç¥ç»ç½‘ç»œé¢„æµ‹
        nn_score = nn_model.predict(features)
        
        # ä½¿ç”¨åŸå§‹ç»Ÿè®¡æ¨¡å‹é¢„æµ‹ï¼ˆå¯¹æ¯”ï¼‰
        common_features = analyzer.trained_features.get('common_features', {})
        stat_score = analyzer._calculate_match_score(features, common_features, tolerance=0.3)
        stat_total = stat_score.get('æ€»åŒ¹é…åº¦', 0)
        
        match_scores[stock_code] = {
            'nn_score': nn_score,
            'stat_score': stat_total
        }
        
        stock_name = analysis_result.get('stock_info', {}).get('åç§°', stock_code)
        print(f"  {stock_code} {stock_name}:")
        print(f"    ç¥ç»ç½‘ç»œ: {nn_score:.3f}")
        print(f"    ç»Ÿè®¡æ¨¡å‹: {stat_total:.3f}")
        print(f"    å·®å¼‚: {abs(nn_score - stat_total):.3f}")
    
    # ç»Ÿè®¡
    if match_scores:
        nn_scores = [s['nn_score'] for s in match_scores.values()]
        stat_scores = [s['stat_score'] for s in match_scores.values()]
        
        print(f"\nğŸ“Š ç»Ÿè®¡:")
        print(f"  ç¥ç»ç½‘ç»œå¹³å‡åŒ¹é…åº¦: {np.mean(nn_scores):.3f}")
        print(f"  ç»Ÿè®¡æ¨¡å‹å¹³å‡åŒ¹é…åº¦: {np.mean(stat_scores):.3f}")
        print(f"  ç¥ç»ç½‘ç»œæœ€é«˜åŒ¹é…åº¦: {np.max(nn_scores):.3f}")
        print(f"  ç»Ÿè®¡æ¨¡å‹æœ€é«˜åŒ¹é…åº¦: {np.max(stat_scores):.3f}")
        print(f"  ç¥ç»ç½‘ç»œæœ€ä½åŒ¹é…åº¦: {np.min(nn_scores):.3f}")
        print(f"  ç»Ÿè®¡æ¨¡å‹æœ€ä½åŒ¹é…åº¦: {np.min(stat_scores):.3f}")
        print(f"  å¹³å‡å·®å¼‚: {np.mean([abs(s['nn_score'] - s['stat_score']) for s in match_scores.values()]):.3f}")
    
    # ä¿å­˜æ¨¡å‹
    print("\n" + "=" * 80)
    print("ğŸ’¾ ä¿å­˜ç¥ç»ç½‘ç»œæ¨¡å‹")
    print("=" * 80)
    
    os.makedirs('models', exist_ok=True)
    model_path = 'models/ç¥ç»ç½‘ç»œæ¨¡å‹.pkl'
    nn_model.save_model(model_path)
    
    print("\n" + "=" * 80)
    print("âœ… ç¥ç»ç½‘ç»œæ¨¡å‹æµ‹è¯•å®Œæˆï¼")
    print("=" * 80)


if __name__ == '__main__':
    test_neural_network_model()
