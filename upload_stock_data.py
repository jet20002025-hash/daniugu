#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ä¸Šä¼ æœ¬åœ°è‚¡ç¥¨æ•°æ®åˆ°å¤–éƒ¨å­˜å‚¨
æ”¯æŒå¤šç§å­˜å‚¨æ–¹æ¡ˆï¼šGitHub Releasesã€AWS S3ã€Google Cloud Storageç­‰
"""
import os
import sys
import json
import tarfile
import gzip
import pandas as pd
import shutil
import tempfile
from pathlib import Path
from datetime import datetime

# é…ç½®
CACHE_DIR = 'cache'
STOCK_DATA_DIR = 'stock_data'
UPLOAD_CONFIG_FILE = 'upload_config.json'

def get_data_size_mb(directory):
    """è·å–ç›®å½•å¤§å°ï¼ˆMBï¼‰"""
    total_size = 0
    file_count = 0
    for dirpath, dirnames, filenames in os.walk(directory):
        for filename in filenames:
            filepath = os.path.join(dirpath, filename)
            if os.path.exists(filepath):
                total_size += os.path.getsize(filepath)
                file_count += 1
    return total_size / (1024 * 1024), file_count

def filter_csv_by_date_range(csv_path, start_date, end_date):
    """è¿‡æ»¤ CSV æ–‡ä»¶ï¼Œåªä¿ç•™æŒ‡å®šæ—¥æœŸèŒƒå›´å†…çš„è¡Œ"""
    try:
        df = pd.read_csv(csv_path, encoding='utf-8-sig')
        if df.empty:
            return None
        # æŸ¥æ‰¾æ—¥æœŸåˆ—
        date_col = None
        for col in df.columns:
            if 'æ—¥æœŸ' in str(col) or 'date' in str(col).lower() or col == 'æ—¥æœŸ':
                date_col = col
                break
        if date_col is None:
            # å¦‚æœæ²¡æœ‰æ‰¾åˆ°æ—¥æœŸåˆ—ï¼Œè¿”å›åŸæ–‡ä»¶
            return df
        # è½¬æ¢æ—¥æœŸæ ¼å¼å¹¶è¿‡æ»¤
        df[date_col] = pd.to_datetime(df[date_col], errors='coerce')
        df = df[(df[date_col] >= pd.Timestamp(start_date)) & (df[date_col] <= pd.Timestamp(end_date))]
        return df if not df.empty else None
    except Exception as e:
        print(f"   âš ï¸  è¿‡æ»¤ {csv_path} å¤±è´¥: {e}ï¼Œè·³è¿‡")
        return None

def create_upload_package(years_only=False, start_date='2024-01-01', end_date='2025-12-31'):
    """åˆ›å»ºä¸Šä¼ åŒ…
    
    :param years_only: æ˜¯å¦åªæ‰“åŒ…æŒ‡å®šå¹´ä»½èŒƒå›´çš„æ•°æ®ï¼ˆé»˜è®¤ Falseï¼Œæ‰“åŒ…å…¨éƒ¨ï¼‰
    :param start_date: å¼€å§‹æ—¥æœŸï¼ˆYYYY-MM-DDï¼‰ï¼Œä»…åœ¨ years_only=True æ—¶ç”Ÿæ•ˆ
    :param end_date: ç»“æŸæ—¥æœŸï¼ˆYYYY-MM-DDï¼‰ï¼Œä»…åœ¨ years_only=True æ—¶ç”Ÿæ•ˆ
    """
    print("=" * 60)
    if years_only:
        print(f"ğŸ“¦ åˆ›å»ºæ•°æ®ä¸Šä¼ åŒ…ï¼ˆä»… {start_date} è‡³ {end_date}ï¼‰")
    else:
        print("ğŸ“¦ åˆ›å»ºæ•°æ®ä¸Šä¼ åŒ…")
    print("=" * 60)
    
    # æ£€æŸ¥ç›®å½•æ˜¯å¦å­˜åœ¨
    if not os.path.exists(CACHE_DIR):
        print(f"âŒ ç›®å½•ä¸å­˜åœ¨: {CACHE_DIR}")
        return None
    
    if not os.path.exists(STOCK_DATA_DIR):
        print(f"âš ï¸  ç›®å½•ä¸å­˜åœ¨: {STOCK_DATA_DIR}ï¼Œè·³è¿‡")
        stock_data_exists = False
    else:
        stock_data_exists = True
    
    # è·å–æ•°æ®å¤§å°
    cache_size_mb, cache_files = get_data_size_mb(CACHE_DIR)
    print(f"ğŸ“Š cache ç›®å½•: {cache_size_mb:.2f} MB, {cache_files} ä¸ªæ–‡ä»¶")
    
    if stock_data_exists:
        stock_size_mb, stock_files = get_data_size_mb(STOCK_DATA_DIR)
        print(f"ğŸ“Š stock_data ç›®å½•: {stock_size_mb:.2f} MB, {stock_files} ä¸ªæ–‡ä»¶")
        total_size_mb = cache_size_mb + stock_size_mb
        total_files = cache_files + stock_files
    else:
        total_size_mb = cache_size_mb
        total_files = cache_files
    
    print(f"ğŸ“Š æ€»è®¡: {total_size_mb:.2f} MB, {total_files} ä¸ªæ–‡ä»¶")
    
    # åˆ›å»ºå‹ç¼©åŒ…
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    suffix = '_2024_2025' if years_only else ''
    package_name = f'stock_data{suffix}_{timestamp}.tar.gz'
    
    print(f"\nğŸ“¦ æ­£åœ¨åˆ›å»ºå‹ç¼©åŒ…: {package_name}")
    if years_only:
        print(f"   ğŸ“… æ—¥æœŸèŒƒå›´: {start_date} è‡³ {end_date}")
    print("   è¿™å¯èƒ½éœ€è¦å‡ åˆ†é’Ÿæ—¶é—´...")
    
    try:
        if years_only:
            # åªæ‰“åŒ…æŒ‡å®šå¹´ä»½èŒƒå›´ï¼šåˆ›å»ºä¸´æ—¶ç›®å½•ï¼Œè¿‡æ»¤ CSV åå¤åˆ¶
            with tempfile.TemporaryDirectory() as tmpdir:
                tmp_cache = os.path.join(tmpdir, 'cache')
                tmp_stock_data = os.path.join(tmpdir, 'stock_data') if stock_data_exists else None
                os.makedirs(tmp_cache, exist_ok=True)
                if tmp_stock_data:
                    os.makedirs(tmp_stock_data, exist_ok=True)
                
                # å¤åˆ¶é CSV æ–‡ä»¶ï¼ˆå¦‚ stock_list_all.json, .meta.json ç­‰ï¼‰
                print("   å¤åˆ¶é CSV æ–‡ä»¶...")
                for root, dirs, files in os.walk(CACHE_DIR):
                    rel_root = os.path.relpath(root, CACHE_DIR)
                    target_root = os.path.join(tmp_cache, rel_root)
                    os.makedirs(target_root, exist_ok=True)
                    for f in files:
                        if not f.endswith('.csv'):
                            src = os.path.join(root, f)
                            dst = os.path.join(target_root, f)
                            shutil.copy2(src, dst)
                
                # è¿‡æ»¤å¹¶å¤åˆ¶ CSV æ–‡ä»¶ï¼ˆåªä¿ç•™æŒ‡å®šæ—¥æœŸèŒƒå›´ï¼‰
                print("   è¿‡æ»¤ CSV æ–‡ä»¶ï¼ˆåªä¿ç•™æŒ‡å®šæ—¥æœŸèŒƒå›´ï¼‰...")
                csv_count = 0
                for kline_type in ['daily_kline', 'weekly_kline']:
                    src_dir = os.path.join(CACHE_DIR, kline_type)
                    if not os.path.exists(src_dir):
                        continue
                    dst_dir = os.path.join(tmp_cache, kline_type)
                    os.makedirs(dst_dir, exist_ok=True)
                    for f in os.listdir(src_dir):
                        if f.endswith('.csv'):
                            csv_path = os.path.join(src_dir, f)
                            df_filtered = filter_csv_by_date_range(csv_path, start_date, end_date)
                            if df_filtered is not None:
                                dst_path = os.path.join(dst_dir, f)
                                df_filtered.to_csv(dst_path, index=False, encoding='utf-8-sig')
                                csv_count += 1
                                # å¤åˆ¶å¯¹åº”çš„ .meta.jsonï¼ˆå¦‚æœå­˜åœ¨ï¼‰
                                meta_src = csv_path.replace('.csv', '.meta.json')
                                if os.path.exists(meta_src):
                                    meta_dst = dst_path.replace('.csv', '.meta.json')
                                    shutil.copy2(meta_src, meta_dst)
                print(f"   âœ… å·²è¿‡æ»¤ {csv_count} ä¸ª CSV æ–‡ä»¶")
                
                # å¤åˆ¶ stock_dataï¼ˆå¦‚æœå­˜åœ¨ä¸”ä¸éœ€è¦è¿‡æ»¤ï¼‰
                if tmp_stock_data:
                    print("   å¤åˆ¶ stock_data ç›®å½•...")
                    shutil.copytree(STOCK_DATA_DIR, tmp_stock_data, dirs_exist_ok=True)
                
                # æ‰“åŒ…ä¸´æ—¶ç›®å½•
                print("   æ‰“åŒ…ä¸­...")
                with tarfile.open(package_name, 'w:gz') as tar:
                    tar.add(tmp_cache, arcname='cache', filter=lambda info: info if info.size < 100 * 1024 * 1024 else None)
                    if tmp_stock_data and os.path.exists(tmp_stock_data):
                        tar.add(tmp_stock_data, arcname='stock_data', filter=lambda info: info if info.size < 100 * 1024 * 1024 else None)
        else:
            # æ‰“åŒ…å…¨éƒ¨æ•°æ®ï¼ˆåŸæœ‰é€»è¾‘ï¼‰
            with tarfile.open(package_name, 'w:gz') as tar:
                # æ·»åŠ  cache ç›®å½•
                print("   æ·»åŠ  cache ç›®å½•...")
                tar.add(CACHE_DIR, arcname='cache', filter=lambda info: info if info.size < 100 * 1024 * 1024 else None)  # è·³è¿‡å¤§äº100MBçš„æ–‡ä»¶
                
                # æ·»åŠ  stock_data ç›®å½•ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
                if stock_data_exists:
                    print("   æ·»åŠ  stock_data ç›®å½•...")
                    tar.add(STOCK_DATA_DIR, arcname='stock_data', filter=lambda info: info if info.size < 100 * 1024 * 1024 else None)
        
        package_size_mb = os.path.getsize(package_name) / (1024 * 1024)
        print(f"âœ… å‹ç¼©åŒ…åˆ›å»ºæˆåŠŸ: {package_name}")
        print(f"   å‹ç¼©åå¤§å°: {package_size_mb:.2f} MB")
        print(f"   å‹ç¼©ç‡: {(1 - package_size_mb / total_size_mb) * 100:.1f}%")
        
        return package_name
    except Exception as e:
        print(f"âŒ åˆ›å»ºå‹ç¼©åŒ…å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return None

def upload_to_github_release(package_name, github_token=None, repo=None):
    """ä¸Šä¼ åˆ° GitHub Releasesï¼ˆéœ€è¦å®‰è£… PyGithubï¼‰"""
    try:
        from github import Github
    except ImportError:
        print("âŒ éœ€è¦å®‰è£… PyGithub: pip install PyGithub")
        return False
    
    if not github_token:
        github_token = os.environ.get('GITHUB_TOKEN')
    
    if not github_token:
        print("âŒ éœ€è¦è®¾ç½® GITHUB_TOKEN ç¯å¢ƒå˜é‡")
        return False
    
    if not repo:
        repo = os.environ.get('GITHUB_REPO', 'jet20002025-hash/daniugu')
    
    try:
        g = Github(github_token)
        repo_obj = g.get_repo(repo)
        
        # åˆ›å»ºæˆ–è·å– release
        tag_name = f"data-{datetime.now().strftime('%Y%m%d')}"
        try:
            release = repo_obj.get_release(tag_name)
            print(f"ğŸ“Œ ä½¿ç”¨ç°æœ‰ Release: {tag_name}")
        except:
            release = repo_obj.create_git_release(
                tag=tag_name,
                name=f"è‚¡ç¥¨æ•°æ®åŒ… - {datetime.now().strftime('%Y-%m-%d')}",
                message=f"è‡ªåŠ¨ä¸Šä¼ çš„è‚¡ç¥¨æ•°æ®åŒ…\nä¸Šä¼ æ—¶é—´: {datetime.now().isoformat()}\næ–‡ä»¶: {package_name}",
                draft=False,
                prerelease=False
            )
            print(f"âœ… åˆ›å»ºæ–° Release: {tag_name}")
        
        # ä¸Šä¼ æ–‡ä»¶
        print(f"ğŸ“¤ æ­£åœ¨ä¸Šä¼  {package_name} åˆ° GitHub Releases...")
        with open(package_name, 'rb') as f:
            release.upload_asset(
                path=package_name,
                label=package_name,
                content_type='application/gzip'
            )
        
        print(f"âœ… ä¸Šä¼ æˆåŠŸï¼")
        print(f"   Release URL: {release.html_url}")
        return True
    except Exception as e:
        print(f"âŒ ä¸Šä¼ åˆ° GitHub Releases å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

def save_upload_info(package_name, upload_method='manual'):
    """ä¿å­˜ä¸Šä¼ ä¿¡æ¯"""
    info = {
        'package_name': package_name,
        'upload_time': datetime.now().isoformat(),
        'upload_method': upload_method,
        'package_size_mb': os.path.getsize(package_name) / (1024 * 1024) if os.path.exists(package_name) else 0
    }
    
    with open(UPLOAD_CONFIG_FILE, 'w', encoding='utf-8') as f:
        json.dump(info, f, ensure_ascii=False, indent=2)
    
    print(f"âœ… ä¸Šä¼ ä¿¡æ¯å·²ä¿å­˜åˆ°: {UPLOAD_CONFIG_FILE}")

def main():
    """ä¸»å‡½æ•°"""
    import argparse
    parser = argparse.ArgumentParser(description='åˆ›å»ºè‚¡ç¥¨æ•°æ®ä¸Šä¼ åŒ…')
    parser.add_argument('--years-only', action='store_true', help='åªæ‰“åŒ… 2024-2025 ä¸¤å¹´çš„æ•°æ®ï¼ˆå¤§å¹…å‡å°åŒ…ä½“ç§¯ï¼‰')
    parser.add_argument('--start-date', default='2024-01-01', help='å¼€å§‹æ—¥æœŸï¼ˆYYYY-MM-DDï¼‰ï¼Œä»…åœ¨ --years-only æ—¶ç”Ÿæ•ˆ')
    parser.add_argument('--end-date', default='2025-12-31', help='ç»“æŸæ—¥æœŸï¼ˆYYYY-MM-DDï¼‰ï¼Œä»…åœ¨ --years-only æ—¶ç”Ÿæ•ˆ')
    args = parser.parse_args()
    
    print("\n" + "=" * 60)
    print("ğŸš€ è‚¡ç¥¨æ•°æ®ä¸Šä¼ å·¥å…·")
    if args.years_only:
        print(f"   ğŸ“… æ¨¡å¼ï¼šä»…æ‰“åŒ… {args.start_date} è‡³ {args.end_date} çš„æ•°æ®")
    print("=" * 60 + "\n")
    
    # åˆ›å»ºå‹ç¼©åŒ…
    package_name = create_upload_package(
        years_only=args.years_only,
        start_date=args.start_date,
        end_date=args.end_date
    )
    if not package_name:
        print("\nâŒ åˆ›å»ºå‹ç¼©åŒ…å¤±è´¥ï¼Œé€€å‡º")
        sys.exit(1)
    
    # ä¿å­˜ä¸Šä¼ ä¿¡æ¯
    save_upload_info(package_name)
    
    # è¯¢é—®ä¸Šä¼ æ–¹å¼
    print("\n" + "=" * 60)
    print("ğŸ“¤ ä¸Šä¼ é€‰é¡¹")
    print("=" * 60)
    print("1. GitHub Releasesï¼ˆéœ€è¦ GITHUB_TOKENï¼‰")
    print("2. æ‰‹åŠ¨ä¸Šä¼ ï¼ˆç¨åæ‰‹åŠ¨ä¸Šä¼ å‹ç¼©åŒ…ï¼‰")
    print("3. è·³è¿‡ä¸Šä¼ ï¼ˆä»…åˆ›å»ºå‹ç¼©åŒ…ï¼‰")
    
    choice = input("\nè¯·é€‰æ‹©ä¸Šä¼ æ–¹å¼ (1/2/3ï¼Œé»˜è®¤3): ").strip() or '3'
    
    if choice == '1':
        # ä¸Šä¼ åˆ° GitHub Releases
        github_token = os.environ.get('GITHUB_TOKEN') or input("è¯·è¾“å…¥ GitHub Token: ").strip()
        if github_token:
            upload_to_github_release(package_name, github_token)
        else:
            print("âŒ æœªæä¾› GitHub Tokenï¼Œè·³è¿‡ä¸Šä¼ ")
    elif choice == '2':
        print(f"\nâœ… å‹ç¼©åŒ…å·²åˆ›å»º: {package_name}")
        print("   è¯·æ‰‹åŠ¨ä¸Šä¼ åˆ°äº‘å­˜å‚¨æœåŠ¡ï¼ˆå¦‚ï¼š")
        print("   - GitHub Releases")
        print("   - AWS S3")
        print("   - Google Cloud Storage")
        print("   - é˜¿é‡Œäº‘ OSS")
        print("   - å…¶ä»–äº‘å­˜å‚¨æœåŠ¡")
    else:
        print(f"\nâœ… å‹ç¼©åŒ…å·²åˆ›å»º: {package_name}")
        print("   å¯ä»¥ç¨åæ‰‹åŠ¨ä¸Šä¼ ")
    
    print("\n" + "=" * 60)
    print("âœ… å®Œæˆï¼")
    print("=" * 60)
    print(f"ğŸ“¦ å‹ç¼©åŒ…: {package_name}")
    print(f"ğŸ“Š å¤§å°: {os.path.getsize(package_name) / (1024 * 1024):.2f} MB")
    print(f"ğŸ“ ä¸Šä¼ ä¿¡æ¯: {UPLOAD_CONFIG_FILE}")

if __name__ == '__main__':
    main()
