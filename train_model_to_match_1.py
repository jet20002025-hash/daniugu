#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
è®­ç»ƒæ¨¡å‹ï¼Œç¡®ä¿æ‰€æœ‰å¤§ç‰›è‚¡åŒ¹é…åº¦è¾¾åˆ°1.0
å¼ºè°ƒå‡çº¿ç²˜åˆåº¦ä½œä¸ºé‡è¦æƒé‡
"""
import os
import sys
import json
from datetime import datetime
import pandas as pd
import numpy as np

sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))
from bull_stock_analyzer import BullStockAnalyzer

# æ‰€æœ‰å¤§ç‰›è‚¡çš„ä¹°ç‚¹ä¿¡æ¯ï¼ˆå·²æ›´æ–°ï¼‰
ALL_BULL_STOCKS = {
    '301005': {'name': 'è¶…æ·è‚¡ä»½', 'buy_date': '2025-11-14', 'buy_price': 48.76, 'ma_diff_percent': 1.25},
    '000592': {'name': 'å¹³æ½­å‘å±•', 'buy_date': '2025-10-16', 'buy_price': 3.37, 'ma_diff_percent': 3.03},
    '600343': {'name': 'èˆªå¤©åŠ¨åŠ›', 'buy_date': '2025-10-31', 'buy_price': 15.30, 'ma_diff_percent': 3.59},
    '603122': {'name': 'åˆå¯Œä¸­å›½', 'buy_date': '2025-10-17', 'buy_price': 6.28, 'ma_diff_percent': 4.31},
    '002104': {'name': 'æ’å®è‚¡ä»½', 'buy_date': '2025-05-23', 'buy_price': 7.13, 'ma_diff_percent': 6.81},
    '603778': {'name': 'å›½æ™Ÿç§‘æŠ€', 'buy_date': '2025-10-10', 'buy_price': 3.48, 'ma_diff_percent': 7.59},
    '603216': {'name': 'æ¢¦å¤©å®¶å±…', 'buy_date': '2025-11-05', 'buy_price': 15.70, 'ma_diff_percent': 8.17},
    '002788': {'name': 'é¹­ç‡•åŒ»è¯', 'buy_date': '2025-12-11', 'buy_price': 10.52, 'ma_diff_percent': 9.30},
    '301232': {'name': 'é£æ²ƒç§‘æŠ€', 'buy_date': '2025-12-02', 'buy_price': 58.29, 'ma_diff_percent': 12.85},
    '300436': {'name': 'å¹¿ç”Ÿå ‚', 'buy_date': '2025-07-02', 'buy_price': 35.65, 'ma_diff_percent': 4.46},
    '002759': {'name': 'å¤©é™…è‚¡ä»½', 'buy_date': '2025-08-28', 'buy_price': 9.91, 'ma_diff_percent': 11.20},
}

# ç›®æ ‡åŒ¹é…åº¦ï¼šæ‰€æœ‰è‚¡ç¥¨éƒ½è¾¾åˆ°1.0
TARGET_SCORE = 1.0

# æœ€å¤§è¿­ä»£æ¬¡æ•°ï¼ˆæ‰©å±•å‚æ•°åç»„åˆæ›´å¤šï¼‰
MAX_ITERATIONS = 200
# å¿«é€Ÿæ¨¡å¼ï¼šåªè¯•å°‘é‡ç»„åˆï¼Œä¾¿äºæœ¬åœ°éªŒè¯ï¼›è®¾ä¸º 0 ç¦ç”¨
QUICK_MODE_ITERATIONS = 0

# é€‰æ‹©æ€§æ¨¡å¼ï¼šç´§å‚æ•°ã€ä¸æ¿€è¿›æ‰©åŒºé—´ï¼Œæ‰«ææ—¶å‘½ä¸­å°‘ã€åŒºåˆ†åº¦é«˜ã€‚ä¸â€œåŒ¹é…åº¦1â€ç›¸åã€‚
SELECTIVE_MODE = os.environ.get('TRAIN_SELECTIVE', '0') == '1'


def extract_features_for_stock(analyzer, code, buy_date):
    """æå–è‚¡ç¥¨åœ¨ä¹°ç‚¹æ—¥æœŸçš„ç‰¹å¾ï¼ˆä»…ç”¨æœ¬åœ°æ•°æ®ï¼‰"""
    try:
        weekly_df = analyzer.fetcher.get_weekly_kline(code, period='2y', use_cache=True, local_only=True)
        if weekly_df is None or len(weekly_df) == 0:
            return None, None
        
        weekly_df['æ—¥æœŸ'] = pd.to_datetime(weekly_df['æ—¥æœŸ'])
        buy_dt = pd.to_datetime(buy_date)
        weekly_df = weekly_df[weekly_df['æ—¥æœŸ'] <= buy_dt].copy()
        weekly_df = weekly_df.sort_values('æ—¥æœŸ').reset_index(drop=True)
        
        if len(weekly_df) < 40:
            return None, None
        
        buy_point_idx = len(weekly_df) - 1
        
        volume_surge_idx = analyzer.find_volume_surge_point(
            code, buy_point_idx, weekly_df=weekly_df, min_volume_ratio=3.0, lookback_weeks=52
        )
        
        if volume_surge_idx is not None and volume_surge_idx >= 40:
            feature_idx = volume_surge_idx
        else:
            feature_idx = max(0, buy_point_idx - 20)
        
        features = analyzer.extract_features_at_start_point(
            code, feature_idx, lookback_weeks=40, weekly_df=weekly_df
        )
        
        return features, feature_idx
    except Exception as e:
        print(f"  âš ï¸ æå– {code} ç‰¹å¾å¤±è´¥: {e}")
        return None, None


def verify_all_stocks(analyzer, precomputed_features=None, verbose=True):
    """éªŒè¯æ‰€æœ‰è‚¡ç¥¨çš„åŒ¹é…åº¦
    
    :param precomputed_features: é¢„æå–çš„ç‰¹å¾å­—å…¸ {code: {'features': ..., ...}}
    :param verbose: æ˜¯å¦æ‰“å°è¯¦ç»†ä¿¡æ¯
    """
    results = {}
    
    if verbose:
        print(f"\n{'='*80}")
        print(f"éªŒè¯åŒ¹é…åº¦ï¼ˆç›®æ ‡: {TARGET_SCORE:.3f}ï¼‰")
        print(f"{'='*80}\n")
    
    for idx, (code, info) in enumerate(ALL_BULL_STOCKS.items(), 1):
        stock_name = info['name']
        buy_date = info['buy_date']
        ma_diff = info['ma_diff_percent']
        
        # âœ… ä½¿ç”¨é¢„æå–çš„ç‰¹å¾ï¼Œé¿å…é‡å¤è®¡ç®—
        if precomputed_features and code in precomputed_features:
            features = precomputed_features[code]['features']
        else:
            features, _ = extract_features_for_stock(analyzer, code, buy_date)
        
        if features is None:
            results[code] = {'score': 0, 'target': TARGET_SCORE, 'error': 'æ— æ³•æå–ç‰¹å¾'}
            if verbose:
                print(f"  âŒ {code} {stock_name}: æ— æ³•æå–ç‰¹å¾")
            continue
        
        common_features = analyzer._get_common_features()
        if not common_features:
            results[code] = {'score': 0, 'target': TARGET_SCORE, 'error': 'ç‰¹å¾æ¨¡æ¿ä¸ºç©º'}
            if verbose:
                print(f"  âŒ {code} {stock_name}: ç‰¹å¾æ¨¡æ¿ä¸ºç©º")
            continue
        
        match_result = analyzer._calculate_match_score(features, common_features, tolerance=0.3)
        match_score = match_result.get('æ€»åŒ¹é…åº¦', 0)
        
        passed = match_score >= TARGET_SCORE
        status = "âœ…" if passed else "âŒ"
        results[code] = {
            'score': match_score,
            'target': TARGET_SCORE,
            'passed': passed,
            'name': stock_name,
            'ma_diff_percent': ma_diff
        }
        
        # åªåœ¨verboseæ¨¡å¼ä¸‹æ‰“å°è¯¦ç»†ä¿¡æ¯
        if verbose:
            ma_tag = "ï¼ˆå‡çº¿ç²˜åˆï¼‰" if ma_diff < 10 else "ï¼ˆå‡çº¿åˆ†æ•£ï¼‰"
            print(f"  {status} {code} {stock_name}: {match_score:.3f} (ç›®æ ‡: {TARGET_SCORE:.3f}, å‡çº¿å·®å€¼: {ma_diff:.2f}%) {ma_tag} {'âœ“' if passed else 'âœ—'}")
    
    passed_count = sum(1 for r in results.values() if r.get('passed', False))
    total_count = len(results)
    
    if verbose:
        print(f"\n{'='*80}")
        print(f"ç»“æœç»Ÿè®¡: {passed_count}/{total_count} åªè‚¡ç¥¨è¾¾åˆ°ç›®æ ‡ {TARGET_SCORE:.3f}")
        print(f"{'='*80}\n")
    
    all_passed = passed_count == total_count
    return results, all_passed


def create_model_from_all_stocks(analyzer, std_multiplier=3.0, range_buffer=0.5, ma_weight_factor=0.3, precomputed_features=None, excluded_big_bear_codes=None, selective=False):
    """
    åŸºäºæ‰€æœ‰å¤§ç‰›è‚¡çš„ç‰¹å¾åˆ›å»ºæ–°æ¨¡å‹ã€‚
    é€‚åº¦åŠ æƒå‡çº¿ç²˜åˆåº¦ï¼ˆma_weight_factor=0.3ï¼Œè¾ƒä½æƒé‡ï¼Œé¿å…è¿‡åº¦åå‘ï¼‰ã€‚
    ä¹°ç‚¹å½“æ—¥ä¸ºå¤§é˜´çº¿çš„è‚¡ç¥¨æ’é™¤ï¼Œä¸å‚ä¸è®­ç»ƒã€‚
    
    :param precomputed_features: é¢„æå–çš„ç‰¹å¾å­—å…¸ {code: {'features': ..., 'feature_idx': ..., ...}}
    :param excluded_big_bear_codes: é¢„å…ˆæ’é™¤çš„å¤§é˜´çº¿è‚¡ç¥¨ä»£ç é›†åˆ
    :param selective: é€‰æ‹©æ€§æ¨¡å¼ã€‚True=ç´§å‚æ•°ã€ä¸æ¿€è¿›æ‰©åŒºé—´ï¼Œæ‰«æå‘½ä¸­å°‘ï¼›False=åŒ¹é…åº¦1 çš„å®½æ¾é€»è¾‘ã€‚
    """
    all_features = []
    ma_convergence_features = []  # å‡çº¿ç²˜åˆåº¦ä½çš„è‚¡ç¥¨ç‰¹å¾ï¼ˆç”¨äºåŠ æƒï¼‰
    
    for code, info in ALL_BULL_STOCKS.items():
        stock_name = info['name']
        buy_date = info['buy_date']
        ma_diff = info['ma_diff_percent']
        
        # âœ… å¦‚æœè‚¡ç¥¨è¢«é¢„å…ˆæ’é™¤ï¼ˆå¤§é˜´çº¿ï¼‰ï¼Œè·³è¿‡
        if excluded_big_bear_codes and code in excluded_big_bear_codes:
            continue
        
        # âœ… å¦‚æœæä¾›äº†é¢„æå–ç‰¹å¾ï¼Œç›´æ¥ä½¿ç”¨ï¼Œè·³è¿‡æ‰€æœ‰æ£€æŸ¥
        if precomputed_features and code in precomputed_features:
            features = precomputed_features[code]['features']
        else:
            # å›é€€ï¼šå¦‚æœæ²¡æœ‰é¢„æå–ï¼Œæ‰è¿›è¡Œæ£€æŸ¥å’Œæå–ï¼ˆè¿™ç§æƒ…å†µä¸åº”è¯¥å‘ç”Ÿï¼‰
            if hasattr(analyzer, '_is_big_bearish_candle_on_date') and analyzer._is_big_bearish_candle_on_date(code, buy_date):
                continue
            features, _ = extract_features_for_stock(analyzer, code, buy_date)
        
        if features:
            features['è‚¡ç¥¨ä»£ç '] = code
            features['è‚¡ç¥¨åç§°'] = stock_name
            features['å‡çº¿å·®å€¼ç™¾åˆ†æ¯”'] = ma_diff
            all_features.append(features)
            
            # å‡çº¿ç²˜åˆåº¦ä½äº10%çš„è‚¡ç¥¨ç‰¹å¾ï¼ˆç”¨äºé€‚åº¦åŠ æƒï¼‰
            if ma_diff < 10:
                ma_convergence_features.append(features)
    
    if len(all_features) == 0:
        return None
    
    # è®¡ç®—å…±åŒç‰¹å¾ç»Ÿè®¡å€¼
    common_features = {}
    
    # è·å–æ‰€æœ‰ç‰¹å¾å
    all_feature_names = set()
    for f in all_features:
        all_feature_names.update(f.keys())
    
    # ç§»é™¤éç‰¹å¾å­—æ®µ
    all_feature_names.discard('è‚¡ç¥¨ä»£ç ')
    all_feature_names.discard('è‚¡ç¥¨åç§°')
    all_feature_names.discard('å‡çº¿å·®å€¼ç™¾åˆ†æ¯”')
    
    # âœ… ä¼˜åŒ–ï¼šæ‰¹é‡è®¡ç®—ï¼Œå‡å°‘å¾ªç¯å¼€é”€
    feature_count = len(all_feature_names)
    processed = 0
    
    for feature_name in all_feature_names:
        processed += 1
        if processed % 10 == 0:
            print(f"  è®¡ç®—ç‰¹å¾ç»Ÿè®¡å€¼: {processed}/{feature_count}...", end='\r', flush=True)
        
        # æ‰€æœ‰è‚¡ç¥¨çš„å€¼
        all_values = []
        for f in all_features:
            val = f.get(feature_name)
            if val is not None and isinstance(val, (int, float)) and not np.isnan(val):
                all_values.append(float(val))
        
        if len(all_values) == 0:
            continue
        
        # å‡çº¿ç²˜åˆåº¦ä½çš„è‚¡ç¥¨çš„å€¼ï¼ˆç”¨äºé€‚åº¦åŠ æƒï¼‰
        ma_values = []
        for f in ma_convergence_features:
            val = f.get(feature_name)
            if val is not None and isinstance(val, (int, float)) and not np.isnan(val):
                ma_values.append(float(val))
        
        # è®¡ç®—ç»Ÿè®¡å€¼ï¼ˆé€‚åº¦åŠ æƒå‡çº¿ç²˜åˆåº¦ï¼Œä½†ä¸è¿‡åº¦ï¼‰
        mean_val = np.mean(all_values)
        median_val = np.median(all_values)
        std_val = np.std(all_values) if len(all_values) > 1 else abs(mean_val) * 0.2 if mean_val != 0 else 0.1
        min_val = np.min(all_values)
        max_val = np.max(all_values)
        
        # å¦‚æœå‡çº¿ç²˜åˆåº¦ä½çš„è‚¡ç¥¨æœ‰æ•°æ®ï¼Œä½¿ç”¨é€‚åº¦åŠ æƒï¼ˆæƒé‡è¾ƒä½ï¼Œé¿å…è¿‡åº¦åå‘ï¼‰
        if len(ma_values) > 0 and ma_weight_factor > 0:
            ma_mean = np.mean(ma_values)
            # é€‚åº¦åŠ æƒï¼šå‡çº¿ç²˜åˆåº¦ä½çš„è‚¡ç¥¨æƒé‡è¾ƒä½ï¼ˆ0.7 vs 0.3 * ma_weight_factorï¼‰
            # ma_weight_factor=0.3 æ—¶ï¼ŒåŠ æƒåå‡å€¼ = 0.7 * å…¨æ ·æœ¬å‡å€¼ + 0.3 * 0.3 * å‡çº¿ç²˜åˆå‡å€¼ = 0.7 * å…¨æ ·æœ¬ + 0.09 * å‡çº¿ç²˜åˆ
            weighted_mean = (mean_val * 0.7 + ma_mean * 0.3 * ma_weight_factor) / (0.7 + 0.3 * ma_weight_factor)
            mean_val = weighted_mean
        
        # æ ‡å‡†å·®ä¸ [min,max] èŒƒå›´
        expanded_std = std_val * std_multiplier
        range_val = max_val - min_val
        
        if selective:
            # é€‰æ‹©æ€§æ¨¡å¼ï¼ˆä¸¥æ ¼ç‰ˆï¼‰ï¼šä½¿ç”¨ç™¾åˆ†ä½æ•°è€Œé min/maxï¼Œæ’é™¤æç«¯å€¼
            if range_val > 0 and len(all_values) >= 4:
                # ä½¿ç”¨ 25% å’Œ 75% åˆ†ä½æ•°ï¼Œæ’é™¤æç«¯å€¼çš„å½±å“
                q25 = np.percentile(all_values, 25)
                q75 = np.percentile(all_values, 75)
                iqr = q75 - q25  # å››åˆ†ä½è·
                # ä½¿ç”¨å¾ˆå°çš„ bufferï¼ˆ0.1-0.25 å€ IQRï¼‰
                buffer = iqr * range_buffer
                expanded_min = q25 - buffer
                expanded_max = q75 + buffer
                # ç¡®ä¿è‡³å°‘è¦†ç›– min/maxï¼ˆä½†ä¸æ‰©å±•å¤ªå¤šï¼‰
                expanded_min = min(expanded_min, min_val)
                expanded_max = max(expanded_max, max_val)
            elif range_val > 0:
                # æ ·æœ¬æ•°å¤ªå°‘ï¼Œä½¿ç”¨ min/max ä½† buffer å¾ˆå°
                buffer = range_val * range_buffer * 0.5  # å‡åŠ
                expanded_min = min_val - buffer
                expanded_max = max_val + buffer
            else:
                buf = abs(mean_val) * max(0.05, range_buffer * 0.3) if mean_val != 0 else 0.05
                expanded_min = mean_val - buf
                expanded_max = mean_val + buf
            # ä¸åšâ€œç¡®ä¿æ‰€æœ‰å€¼éƒ½åœ¨èŒƒå›´å†…â€çš„äºŒæ¬¡æ‰©å±•
        else:
            # åŒ¹é…åº¦1 æ¨¡å¼ï¼šæ¿€è¿›æ‰©åŒºé—´ï¼Œç¡®ä¿æ‰€æœ‰å¤§ç‰›è‚¡éƒ½åœ¨èŒƒå›´å†…
            if range_val > 0:
                buffer = max(range_val * range_buffer, abs(mean_val) * 0.5)
                expanded_min = min_val - buffer
                expanded_max = max_val + buffer
            else:
                buffer = abs(mean_val) * max(0.5, range_buffer) if mean_val != 0 else 0.1
                expanded_min = mean_val - buffer
                expanded_max = mean_val + buffer
            for f in all_features:
                val = f.get(feature_name)
                if val is not None and isinstance(val, (int, float)) and not np.isnan(val):
                    if val < expanded_min:
                        expanded_min = val - max(abs(val) * 0.2, range_val * 0.3)
                    if val > expanded_max:
                        expanded_max = val + max(abs(val) * 0.2, range_val * 0.3)
        
        common_features[feature_name] = {
            'å‡å€¼': round(float(mean_val), 4),
            'ä¸­ä½æ•°': round(float(median_val), 4),
            'æ ‡å‡†å·®': round(float(expanded_std), 4),
            'æœ€å°å€¼': round(float(expanded_min), 4),
            'æœ€å¤§å€¼': round(float(expanded_max), 4),
            'æ ·æœ¬æ•°': len(all_values),
            'å‡çº¿ç²˜åˆæ ·æœ¬æ•°': len(ma_values)
        }
    
    if processed > 0:
        print(f"  è®¡ç®—ç‰¹å¾ç»Ÿè®¡å€¼: {processed}/{feature_count} âœ…" + " " * 20)  # æ¸…é™¤è¿›åº¦è¡Œ
    
    return {
        'common_features': common_features,
        'sample_count': len(all_features),
        'ma_convergence_count': len(ma_convergence_features),
        'trained_at': datetime.now().isoformat(),
        'model_type': 'selective_ma_weight_moderate' if selective else 'tuned_match_1_ma_weight_moderate',
        'sample_stocks': [f.get('è‚¡ç¥¨ä»£ç ') for f in all_features]
    }


def main():
    print("="*80)
    if SELECTIVE_MODE:
        print("è®­ç»ƒæ¨¡å‹ï¼šé€‰æ‹©æ€§æ¨¡å¼ï¼ˆä¸¥æ ¼ç‰ˆ - æç´§å‚æ•°ã€æå°‘å‘½ä¸­ï¼Œæ‰«ææ—¶åŒºåˆ†åº¦æé«˜ï¼‰")
    else:
        print("è®­ç»ƒæ¨¡å‹ï¼šç¡®ä¿æ‰€æœ‰å¤§ç‰›è‚¡åŒ¹é…åº¦è¾¾åˆ°1.0")
    print("å‡çº¿ç²˜åˆé€‚åº¦åŠ æƒï¼ˆåœ¨å…¶ä»–æ¡ä»¶ç›¸åŒæ—¶ï¼Œå‡çº¿ç²˜åˆåº¦é«˜çš„åŒ¹é…åº¦æ›´é«˜ï¼‰")
    print("ä¹°ç‚¹å½“æ—¥å¤§é˜´çº¿æ’é™¤")
    print("æ•°æ®ï¼šä»…æœ¬åœ°ç¼“å­˜ï¼Œç¼ºå¤±æ—¶å…ˆä¸‹è½½å®Œæ•´æ•°æ®")
    print("="*80)
    
    from download_training_data import ensure_training_data_local
    print("\nğŸ“¥ æ£€æŸ¥è®­ç»ƒæ•°æ®ï¼šæœ¬åœ°ç¼ºå¤±åˆ™ä»ç½‘ç»œä¸‹è½½å®Œæ•´æ•°æ®...")
    ensure_training_data_local()
    print()
    
    # ä¸‹è½½å®Œæˆåï¼Œè®­ç»ƒé˜¶æ®µä»…ä»æœ¬åœ°è¯»å–ï¼Œä¸å†è®¿é—®ç½‘ç»œ
    os.environ["TRAIN_LOCAL_ONLY"] = "1"
    
    # åŠ è½½å½“å‰æ¨¡å‹
    analyzer = BullStockAnalyzer(auto_load_default_stocks=False, auto_analyze_and_train=False)
    if not analyzer.load_model('trained_model.json', skip_network=True):
        print("âŒ æ— æ³•åŠ è½½åŸºç¡€æ¨¡å‹")
        return
    
    print(f"\nâœ… å·²åŠ è½½åŸºç¡€æ¨¡å‹")
    
    # âœ… ä¼˜åŒ–ï¼šé¢„å…ˆæå–æ‰€æœ‰è‚¡ç¥¨çš„ç‰¹å¾ï¼Œé¿å…æ¯æ¬¡è¿­ä»£é‡å¤æå–
    # åŒæ—¶é¢„å…ˆæ£€æŸ¥å¤§é˜´çº¿ï¼Œé¿å…åœ¨è¿­ä»£ä¸­é‡å¤æ£€æŸ¥
    print("\nâš¡ é¢„å…ˆæå–æ‰€æœ‰è‚¡ç¥¨ç‰¹å¾ï¼ˆé¿å…é‡å¤è®¡ç®—ï¼‰...")
    precomputed_features = {}
    excluded_big_bear_codes = set()  # è®°å½•è¢«æ’é™¤çš„è‚¡ç¥¨ä»£ç 
    
    for code, info in ALL_BULL_STOCKS.items():
        stock_name = info['name']
        buy_date = info['buy_date']
        print(f"  æå– {code} {stock_name} çš„ç‰¹å¾...", end='', flush=True)
        
        # é¢„å…ˆæ£€æŸ¥å¤§é˜´çº¿
        if hasattr(analyzer, '_is_big_bearish_candle_on_date') and analyzer._is_big_bearish_candle_on_date(code, buy_date):
            excluded_big_bear_codes.add(code)
            print(" â­ï¸ å¤§é˜´çº¿æ’é™¤")
            continue
        
        features, feature_idx = extract_features_for_stock(analyzer, code, buy_date)
        if features:
            precomputed_features[code] = {'features': features, 'feature_idx': feature_idx, 'name': stock_name, 'buy_date': buy_date}
            print(" âœ…")
        else:
            print(" âŒ å¤±è´¥")
    
    print(f"âœ… æˆåŠŸé¢„æå– {len(precomputed_features)}/{len(ALL_BULL_STOCKS)} åªè‚¡ç¥¨çš„ç‰¹å¾")
    if excluded_big_bear_codes:
        print(f"âœ… å·²æ’é™¤ä¹°ç‚¹å½“æ—¥å¤§é˜´çº¿: {len(excluded_big_bear_codes)} åª")
    
    # éªŒè¯åˆå§‹çŠ¶æ€ï¼ˆä½¿ç”¨é¢„æå–ç‰¹å¾ï¼Œæ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯ï¼‰
    print("\nğŸ“Š éªŒè¯åˆå§‹åŒ¹é…åº¦...")
    initial_results, _ = verify_all_stocks(analyzer, precomputed_features=precomputed_features, verbose=True)
    
    best_model = None
    best_score = -1.0
    best_avg = -1.0
    best_min = -1.0
    best_iteration = 0
    
    # å°è¯•ä¸åŒçš„å‚æ•°ç»„åˆ
    if SELECTIVE_MODE:
        # é€‰æ‹©æ€§æ¨¡å¼ï¼ˆä¸¥æ ¼ç‰ˆï¼‰ï¼šæ›´ç´§çš„ stdã€æ›´ç´§çš„ rangeï¼Œæ‰«æå‘½ä¸­æå°‘
        std_multipliers = [1.0, 1.2, 1.5, 2.0]  # ä» 1.5-3.0 æ”¶ç´§åˆ° 1.0-2.0
        range_buffers = [0.1, 0.15, 0.2, 0.25]  # ä» 0.2-0.5 æ”¶ç´§åˆ° 0.1-0.25
        ma_weight_factors = [0.25, 0.3]
    elif QUICK_MODE_ITERATIONS and QUICK_MODE_ITERATIONS > 0:
        std_multipliers = [12.0, 18.0]
        range_buffers = [1.5, 2.0]
        ma_weight_factors = [0.3]
    else:
        # åŒ¹é…åº¦1 æ¨¡å¼ï¼šå®½æ¾ç½‘æ ¼
        std_multipliers = [10.0, 14.0, 18.0, 22.0]
        range_buffers = [1.2, 1.5, 2.0, 2.5]
        ma_weight_factors = [0.25, 0.35]
    
    iteration = 0
    all_passed = False
    total_iterations = len(std_multipliers) * len(range_buffers) * len(ma_weight_factors)
    
    for std_mult in std_multipliers:
        for range_buf in range_buffers:
            for ma_weight in ma_weight_factors:
                iteration += 1
                if iteration > MAX_ITERATIONS:
                    break
                
                print(f"\n[ {iteration}/{total_iterations} ] std={std_mult} range={range_buf} ma={ma_weight}")
                
                # åˆ›å»ºæ–°æ¨¡å‹ï¼ˆé€‚åº¦åŠ æƒå‡çº¿ç²˜åˆåº¦ï¼Œä½¿ç”¨é¢„æå–ç‰¹å¾ï¼‰
                new_model = create_model_from_all_stocks(
                    analyzer,
                    std_multiplier=std_mult,
                    range_buffer=range_buf,
                    ma_weight_factor=ma_weight,
                    precomputed_features=precomputed_features,
                    excluded_big_bear_codes=excluded_big_bear_codes,
                    selective=SELECTIVE_MODE
                )
                
                if new_model is None:
                    print("âŒ åˆ›å»ºæ¨¡å‹å¤±è´¥ï¼Œè·³è¿‡æœ¬æ¬¡è¿­ä»£")
                    continue
                
                analyzer.trained_features = {
                    'common_features': new_model['common_features'],
                    'sample_count': new_model['sample_count'],
                    'trained_at': new_model['trained_at'],
                    'model_type': new_model['model_type'],
                    'sample_stocks': new_model['sample_stocks']
                }
                
                # éªŒè¯åŒ¹é…åº¦ï¼ˆä½¿ç”¨é¢„æå–ç‰¹å¾ï¼Œè¿­ä»£ä¸­ä¸æ‰“å°è¯¦ç»†ä¿¡æ¯ï¼‰
                results, all_passed = verify_all_stocks(analyzer, precomputed_features=precomputed_features, verbose=False)
                avg_score = sum(r.get('score', 0) for r in results.values()) / len(results) if results else 0
                min_score = min(r.get('score', 0) for r in results.values()) if results else 0
                passed_count = sum(1 for r in results.values() if r.get('passed', False))
                
                # ç»¼åˆå¾—åˆ†ï¼šå…¼é¡¾å¹³å‡ä¸æœ€ä½åˆ†ï¼Œä¼˜å…ˆæå‡çŸ­æ¿è‚¡
                composite = 0.65 * avg_score + 0.35 * min_score
                print(f"  â†’ å¹³å‡={avg_score:.3f} æœ€ä½={min_score:.3f} ç»¼åˆ={composite:.3f} è¾¾æ ‡={passed_count}/{len(results)}", end='')
                
                # ç”¨ç»¼åˆå¾—åˆ†é€‰æœ€ä½³ï¼ˆå…¼é¡¾å¹³å‡ä¸æœ€ä½ï¼Œä¼˜å…ˆæå‡çŸ­æ¿è‚¡ï¼‰
                if composite > best_score:
                    best_score = composite
                    best_avg = avg_score
                    best_min = min_score
                    best_model = new_model
                    best_iteration = iteration
                    print(f" âœ… æœ€ä½³")
                else:
                    print()
                
                if all_passed:
                    best_model = new_model
                    best_iteration = iteration
                    best_score = composite
                    best_avg = avg_score
                    best_min = min_score
                    print(f"\nğŸ‰ æ‰€æœ‰è‚¡ç¥¨éƒ½è¾¾åˆ°ç›®æ ‡åŒ¹é…åº¦ {TARGET_SCORE:.3f}ï¼")
                    break
            if all_passed:
                break
        if all_passed:
            break
        if iteration > MAX_ITERATIONS:
            break
    
    # ä¿å­˜æœ€ä½³æ¨¡å‹
    if best_model:
        print(f"\n{'='*80}")
        print(f"å¾®è°ƒå®Œæˆ")
        print(f"{'='*80}")
        
        # ä½¿ç”¨æœ€ä½³æ¨¡å‹è¿›è¡Œæœ€ç»ˆéªŒè¯
        analyzer.trained_features = {
            'common_features': best_model['common_features'],
            'sample_count': best_model['sample_count'],
            'trained_at': best_model['trained_at'],
            'model_type': best_model['model_type'],
            'sample_stocks': best_model['sample_stocks']
        }
        
        # åŠ è½½å¤§ç‰›è‚¡åˆ—è¡¨ï¼ˆå¦‚æœéœ€è¦ï¼‰
        if not hasattr(analyzer, 'bull_stocks') or not analyzer.bull_stocks:
            analyzer.bull_stocks = []
            for code, info in ALL_BULL_STOCKS.items():
                analyzer.bull_stocks.append({
                    'ä»£ç ': code,
                    'åç§°': info['name'],
                    'ä¹°ç‚¹æ—¥æœŸ': info['buy_date'],
                    'èµ·ç‚¹ä»·æ ¼': info['buy_price']
                })
        
        # ç¡®ä¿ bull_stocks ä¸­çš„æ—¥æœŸæ˜¯å­—ç¬¦ä¸²æ ¼å¼ï¼ˆä¸æ˜¯ datetime å¯¹è±¡ï¼‰
        bull_stocks_for_save = []
        for stock in analyzer.bull_stocks if hasattr(analyzer, 'bull_stocks') else []:
            stock_copy = {}
            for key, value in stock.items():
                if isinstance(value, (pd.Timestamp, datetime)):
                    stock_copy[key] = value.strftime('%Y-%m-%d') if hasattr(value, 'strftime') else str(value)[:10]
                else:
                    stock_copy[key] = value
            bull_stocks_for_save.append(stock_copy)
        
        # ä¿å­˜æ¨¡å‹
        if SELECTIVE_MODE:
            model_filename = f'trained_model_é€‰æ‹©æ€§ä¸¥æ ¼_å‡çº¿é€‚åº¦åŠ æƒ_å¤§é˜´çº¿æ’é™¤_{datetime.now().strftime("%Y%m%d_%H%M%S")}.json'
        else:
            model_filename = f'trained_model_åŒ¹é…åº¦1_å‡çº¿é€‚åº¦åŠ æƒ_å¤§é˜´çº¿æ’é™¤_{datetime.now().strftime("%Y%m%d_%H%M%S")}.json'
        
        model_data = {
            'buy_features': {
                'common_features': best_model['common_features'],
                'sample_count': best_model['sample_count'],
                'trained_at': best_model['trained_at'],
                'model_type': best_model['model_type'],
                'sample_stocks': best_model['sample_stocks']
            },
            'bull_stocks': bull_stocks_for_save  # ä½¿ç”¨è½¬æ¢åçš„åˆ—è¡¨
        }
        
        with open(model_filename, 'w', encoding='utf-8') as f:
            json.dump(model_data, f, ensure_ascii=False, indent=2)
        
        print(f"ğŸ’¾ ä¿å­˜æ¨¡å‹: {model_filename}")
        print(f"ğŸ“Š æœ€ä½³è¿­ä»£: ç¬¬ {best_iteration} æ¬¡")
        print(f"ğŸ“ˆ æœ€ä½³ç»¼åˆ: {best_score:.3f}  å¹³å‡: {best_avg:.3f}  æœ€ä½: {best_min:.3f}")
        
        # æœ€ç»ˆéªŒè¯ï¼ˆä½¿ç”¨é¢„æå–ç‰¹å¾ï¼Œæ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯ï¼‰
        print(f"\n{'='*80}")
        print("æœ€ç»ˆåŒ¹é…åº¦ç»“æœï¼š")
        print(f"{'='*80}")
        final_results, final_all_passed = verify_all_stocks(analyzer, precomputed_features=precomputed_features, verbose=True)
        
        if final_all_passed:
            print("\nğŸ‰ æ‰€æœ‰è‚¡ç¥¨éƒ½è¾¾åˆ°ç›®æ ‡åŒ¹é…åº¦ 1.000ï¼")
        else:
            print(f"\nâš ï¸ éƒ¨åˆ†è‚¡ç¥¨æœªè¾¾åˆ°ç›®æ ‡åŒ¹é…åº¦ {TARGET_SCORE:.3f}")
        
        print(f"\nâœ… è®­ç»ƒè„šæœ¬æ‰§è¡Œå®Œæˆ")
    else:
        print("\nâŒ æœªèƒ½åˆ›å»ºæœ‰æ•ˆæ¨¡å‹")


if __name__ == '__main__':
    main()
