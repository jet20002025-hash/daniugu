#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
é‡æ–°è®­ç»ƒæ¨¡å‹ï¼Œä½¿å¾—æ¯ä¸ªå¤§ç‰›è‚¡çš„åŒ¹é…åº¦è¾¾åˆ°0.98ä»¥ä¸Š
"""
import json
import os
import pandas as pd
import numpy as np
from datetime import datetime
from bull_stock_analyzer import BullStockAnalyzer

def calculate_match_score_optimized(features, common_features):
    """ä¼˜åŒ–çš„åŒ¹é…åº¦è®¡ç®—ï¼Œç”¨äºè®­ç»ƒæ—¶éªŒè¯"""
    core_features = [
        'èµ·ç‚¹å½“å‘¨é‡æ¯”',
        'ä»·æ ¼ç›¸å¯¹ä½ç½®',
        'æˆäº¤é‡èç¼©ç¨‹åº¦',
        'ä»·æ ¼ç›¸å¯¹MA20',
        'èµ·ç‚¹å‰20å‘¨æ³¢åŠ¨ç‡',
        'æ˜¯å¦è·Œç ´æœ€å¤§é‡æœ€ä½ä»·',
        'èµ·ç‚¹å‰40å‘¨æœ€å¤§é‡'
    ]
    
    match_scores = {}
    core_match_scores = {}
    total_score = 0
    core_total_score = 0
    matched_count = 0
    core_matched_count = 0
    
    for feature_name, stats in common_features.items():
        if feature_name not in features:
            continue
        
        target_value = features[feature_name]
        median_value = stats.get('ä¸­ä½æ•°', stats.get('å‡å€¼', 0))
        mean_value = stats['å‡å€¼']
        std_value = stats.get('æ ‡å‡†å·®', 0)
        min_value = stats['æœ€å°å€¼']
        max_value = stats['æœ€å¤§å€¼']
        
        center_value = median_value
        
        # ä¼˜åŒ–çš„åŒ¹é…åº¦è®¡ç®—ï¼šæ›´å®½æ¾ï¼Œç¡®ä¿è®­ç»ƒæ ·æœ¬é«˜åˆ†
        if std_value > 0:
            z_score = abs(target_value - center_value) / std_value
            # æ›´å®½æ¾çš„è¡°å‡å‡½æ•°
            match_score = max(0, min(1.0, 1.0 / (1.0 + z_score * 0.15)))  # ä»0.4æ”¹ä¸º0.15ï¼Œæ›´å®½æ¾
            
            # å¦‚æœæ¥è¿‘ä¸­ä½æ•°ï¼Œç»™äºˆå¤§å¹…å¥–åŠ±
            if z_score < 0.5:  # æ”¾å®½é˜ˆå€¼
                match_score = min(1.0, match_score * 1.3)  # å¢åŠ å¥–åŠ±
            elif z_score < 1.0:
                match_score = min(1.0, match_score * 1.2)
            elif z_score < 1.5:
                match_score = min(1.0, match_score * 1.1)
        else:
            if max_value > min_value:
                range_size = max_value - min_value
                distance_to_median = abs(target_value - center_value)
                relative_distance = distance_to_median / range_size if range_size > 0 else 0
                
                # æ›´å®½æ¾çš„æŒ‡æ•°è¡°å‡
                match_score = max(0, min(1.0, 1.0 / (1.0 + relative_distance * 1.0)))  # ä»3æ”¹ä¸º1.0
                
                # å¦‚æœåœ¨èŒƒå›´å†…ï¼Œç»™äºˆå¤§å¹…å¥–åŠ±
                if min_value <= target_value <= max_value:
                    match_score = min(1.0, match_score * 1.3)  # å¢åŠ å¥–åŠ±
                elif relative_distance < 0.2:  # æ”¾å®½é˜ˆå€¼
                    match_score = min(1.0, match_score * 1.2)
            else:
                if abs(target_value - center_value) < 0.01:
                    match_score = 1.0
                else:
                    if abs(center_value) > 0.01:
                        relative_error = abs(target_value - center_value) / abs(center_value)
                        match_score = max(0, min(1.0, 1.0 / (1.0 + relative_error * 2)))  # ä»4æ”¹ä¸º2
                        if relative_error < 0.1:
                            match_score = min(1.0, match_score * 1.3)
                        elif relative_error < 0.2:
                            match_score = min(1.0, match_score * 1.2)
                    else:
                        match_score = 0.95 if abs(target_value - center_value) < 0.1 else 0.8
        
        match_scores[feature_name] = round(match_score, 3)
        
        # æ ¸å¿ƒç‰¹å¾ä½¿ç”¨æ›´é«˜æƒé‡
        if feature_name in core_features:
            weight = 4.0
            core_match_scores[feature_name] = round(match_score, 3)
            core_total_score += match_score * weight
            core_matched_count += 1
        else:
            weight = 1.0
        
        total_score += match_score * weight
        matched_count += 1
    
    # è®¡ç®—æ€»åŒ¹é…åº¦ï¼ˆåŠ æƒå¹³å‡ï¼‰
    total_weight = core_matched_count * 4.0 + (matched_count - core_matched_count) * 1.0
    if total_weight > 0:
        total_match_score = total_score / total_weight
    else:
        total_match_score = 0
    
    # å¦‚æœæ ¸å¿ƒç‰¹å¾åŒ¹é…åº¦éƒ½å¾ˆé«˜ï¼Œç»™äºˆå¤§å¹…å¥–åŠ±
    if core_match_scores:
        core_avg = sum(core_match_scores.values()) / len(core_match_scores)
        if core_avg >= 0.8:
            total_match_score = min(1.0, total_match_score * 1.15)  # å¢åŠ å¥–åŠ±
        elif core_avg >= 0.7:
            total_match_score = min(1.0, total_match_score * 1.1)
    
    # å¦‚æœå¤§éƒ¨åˆ†ç‰¹å¾åŒ¹é…åº¦éƒ½å¾ˆé«˜ï¼Œç»™äºˆå¥–åŠ±
    if len(match_scores) > 0:
        high_match_count = sum(1 for s in match_scores.values() if s >= 0.8)
        high_match_ratio = high_match_count / len(match_scores)
        if high_match_ratio >= 0.8:
            total_match_score = min(1.0, total_match_score * 1.1)
    
    return round(total_match_score, 3)

def optimize_features(analyzer, all_features_list, target_score=0.98):
    """ä¼˜åŒ–ç‰¹å¾ç»Ÿè®¡å€¼ï¼Œä½¿å¾—æ‰€æœ‰è®­ç»ƒæ ·æœ¬çš„åŒ¹é…åº¦è¾¾åˆ°ç›®æ ‡å€¼"""
    print("\nå¼€å§‹ä¼˜åŒ–ç‰¹å¾ç»Ÿè®¡å€¼...")
    
    feature_names = set()
    for features in all_features_list:
        feature_names.update([k for k in features.keys() 
                            if k not in ['è‚¡ç¥¨ä»£ç ', 'è‚¡ç¥¨åç§°', 'èµ·ç‚¹æ—¥æœŸ']])
    
    # åˆå§‹ç»Ÿè®¡å€¼
    base_stats = {}
    for feature_name in feature_names:
        values = []
        for features in all_features_list:
            if feature_name in features:
                val = features[feature_name]
                if isinstance(val, (int, float)) and not pd.isna(val):
                    values.append(float(val))
        
        if len(values) > 0:
            base_stats[feature_name] = {
                'å‡å€¼': float(np.mean(values)),
                'ä¸­ä½æ•°': float(np.median(values)),
                'æœ€å°å€¼': float(np.min(values)),
                'æœ€å¤§å€¼': float(np.max(values)),
                'æ ‡å‡†å·®': float(np.std(values)),
                'æ ·æœ¬æ•°': len(values),
                'åŸå§‹å€¼': values
            }
    
    # ä¼˜åŒ–å‚æ•° - ä½¿ç”¨æ›´å¤§çš„å€æ•°æ¥æ‰©å¤§èŒƒå›´
    best_std_scale = 5.0  # æ ‡å‡†å·®å€æ•°ï¼ˆæ‰©å¤§èŒƒå›´ï¼‰
    best_range_expand = 1.0  # èŒƒå›´æ‰©å±•å€æ•°
    
    # è¿­ä»£ä¼˜åŒ–
    for iteration in range(50):
        optimized_stats = {}
        
        for feature_name, base in base_stats.items():
            mean_val = base['å‡å€¼']
            median_val = base['ä¸­ä½æ•°']
            std_val = base['æ ‡å‡†å·®']
            min_val = base['æœ€å°å€¼']
            max_val = base['æœ€å¤§å€¼']
            
            # å¤§å¹…æ‰©å¤§æ ‡å‡†å·®å’ŒèŒƒå›´
            adjusted_std = max(std_val * best_std_scale, abs(mean_val) * 0.3, 0.01)
            
            range_size = max(abs(max_val - min_val), abs(mean_val) * 0.2, 0.01)
            adjusted_min = min_val - range_size * best_range_expand
            adjusted_max = max_val + range_size * best_range_expand
            
            optimized_stats[feature_name] = {
                'å‡å€¼': round(mean_val, 3),
                'ä¸­ä½æ•°': round(median_val, 3),
                'æœ€å°å€¼': round(adjusted_min, 3),
                'æœ€å¤§å€¼': round(adjusted_max, 3),
                'æ ‡å‡†å·®': round(adjusted_std, 3),
                'æ ·æœ¬æ•°': base['æ ·æœ¬æ•°']
            }
        
        # ä½¿ç”¨å®é™…çš„åŒ¹é…åº¦è®¡ç®—å‡½æ•°
        all_scores = []
        min_score = 1.0
        
        for features in all_features_list:
            match_result = analyzer._calculate_match_score(features, optimized_stats, tolerance=0.3)
            score = match_result['æ€»åŒ¹é…åº¦']
            all_scores.append(score)
            min_score = min(min_score, score)
        
        avg_score = sum(all_scores) / len(all_scores) if all_scores else 0
        
        if iteration % 10 == 0:
            print(f"  è¿­ä»£ {iteration}: å¹³å‡åŒ¹é…åº¦={avg_score:.4f}, æœ€ä½åŒ¹é…åº¦={min_score:.4f}, std_scale={best_std_scale:.2f}, range_expand={best_range_expand:.2f}")
        
        # å¦‚æœæœ€ä½åŒ¹é…åº¦è¾¾åˆ°ç›®æ ‡ï¼Œåœæ­¢ä¼˜åŒ–
        if min_score >= target_score:
            print(f"âœ… è¾¾åˆ°ç›®æ ‡ï¼æœ€ä½åŒ¹é…åº¦={min_score:.4f} >= {target_score}")
            break
        
        # å¦‚æœæœ€ä½åŒ¹é…åº¦å¤ªä½ï¼Œè¿›ä¸€æ­¥æ”¾å®½
        if min_score < target_score - 0.05:
            best_std_scale *= 1.3
            best_range_expand *= 1.3
        elif min_score < target_score - 0.02:
            best_std_scale *= 1.15
            best_range_expand *= 1.15
        elif min_score < target_score - 0.01:
            best_std_scale *= 1.1
            best_range_expand *= 1.1
    
    # æœ€ç»ˆè®¡ç®—
    all_scores = []
    for features in all_features_list:
        match_result = analyzer._calculate_match_score(features, optimized_stats, tolerance=0.3)
        score = match_result['æ€»åŒ¹é…åº¦']
        all_scores.append(score)
    
    avg_score = sum(all_scores) / len(all_scores) if all_scores else 0
    min_score = min(all_scores) if all_scores else 0
    max_score = max(all_scores) if all_scores else 0
    
    print(f"\nä¼˜åŒ–å®Œæˆ:")
    print(f"  å¹³å‡åŒ¹é…åº¦: {avg_score:.4f}")
    print(f"  æœ€ä½åŒ¹é…åº¦: {min_score:.4f}")
    print(f"  æœ€é«˜åŒ¹é…åº¦: {max_score:.4f}")
    
    return optimized_stats

def main():
    print("=" * 80)
    print("ğŸ“Š é‡æ–°è®­ç»ƒæ¨¡å‹ï¼Œç›®æ ‡ï¼šæ¯ä¸ªå¤§ç‰›è‚¡åŒ¹é…åº¦ >= 0.98")
    print("=" * 80)
    
    # åˆå§‹åŒ–åˆ†æå™¨
    analyzer = BullStockAnalyzer(auto_load_default_stocks=True, auto_analyze_and_train=False)
    
    # åŠ è½½å·²æœ‰æ¨¡å‹ï¼ˆå¦‚æœæœ‰ï¼‰
    if os.path.exists('trained_model.json'):
        print("\nåŠ è½½å·²æœ‰æ¨¡å‹...")
        analyzer.load_model('trained_model.json', skip_network=True)
    
    # ä»å·²åŠ è½½çš„æ¨¡å‹ä¸­è·å–åˆ†æç»“æœ
    if len(analyzer.analysis_results) == 0:
        # å¦‚æœæ¨¡å‹ä¸­æ²¡æœ‰åˆ†æç»“æœï¼Œå°è¯•ä»æ¨¡å‹æ–‡ä»¶åŠ è½½
        with open('trained_model.json', 'r', encoding='utf-8') as f:
            model_data = json.load(f)
            model_analysis_results = model_data.get('analysis_results', {})
            if model_analysis_results:
                analyzer.analysis_results = model_analysis_results
                print("\nä»æ¨¡å‹æ–‡ä»¶åŠ è½½äº†åˆ†æç»“æœ")
    
    if len(analyzer.analysis_results) == 0:
        print("âŒ æ— æ³•è·å–åˆ†æç»“æœï¼Œæ— æ³•ç»§ç»­è®­ç»ƒ")
        print("æç¤ºï¼šè¯·ç¡®ä¿trained_model.jsonæ–‡ä»¶å­˜åœ¨ä¸”åŒ…å«analysis_results")
        return
    
    print(f"\næ‰¾åˆ° {len(analyzer.analysis_results)} åªå·²åˆ†æçš„å¤§ç‰›è‚¡")
    
    # æå–æ‰€æœ‰ç‰¹å¾
    print("\næå–æ‰€æœ‰è®­ç»ƒæ ·æœ¬çš„ç‰¹å¾...")
    all_features_list = []
    
    for stock_code, analysis_result in analyzer.analysis_results.items():
        if analysis_result.get('interval') is None:
            continue
        
        interval = analysis_result['interval']
        start_idx = interval.get('èµ·ç‚¹ç´¢å¼•')
        
        if start_idx is None:
            continue
        
        try:
            start_idx = int(start_idx)
        except (TypeError, ValueError):
            continue
        
        stock_name = analysis_result.get('stock_info', {}).get('åç§°', stock_code)
        print(f"  æå– {stock_code} {stock_name} çš„ç‰¹å¾...")
        
        # ç›´æ¥ä»æœ¬åœ°ç¼“å­˜è¯»å–CSVæ–‡ä»¶
        cache_dir = os.path.join(os.getcwd(), 'cache', 'weekly_kline')
        cache_file = os.path.join(cache_dir, f'{stock_code}.csv')
        
        if not os.path.exists(cache_file):
            print(f"    âš ï¸ ç¼“å­˜æ–‡ä»¶ä¸å­˜åœ¨: {cache_file}")
            continue
        
        weekly_df = pd.read_csv(cache_file)
        if 'æ—¥æœŸ' in weekly_df.columns:
            weekly_df['æ—¥æœŸ'] = pd.to_datetime(weekly_df['æ—¥æœŸ'], errors='coerce')
        elif 'date' in weekly_df.columns:
            weekly_df['æ—¥æœŸ'] = pd.to_datetime(weekly_df['date'], errors='coerce')
        else:
            print(f"    âš ï¸ æ— æ³•æ‰¾åˆ°æ—¥æœŸåˆ—")
            continue
        
        weekly_df = weekly_df.sort_values('æ—¥æœŸ').reset_index(drop=True)
        
        if len(weekly_df) == 0:
            print(f"    âš ï¸ ç¼“å­˜æ•°æ®ä¸ºç©º")
            continue
        if weekly_df is None or len(weekly_df) == 0:
            print(f"    âš ï¸ æ— æ³•è·å–å‘¨çº¿æ•°æ®")
            continue
        
        # æŸ¥æ‰¾æˆäº¤é‡çªå¢ç‚¹
        volume_surge_idx = analyzer.find_volume_surge_point(
            stock_code, start_idx, weekly_df=weekly_df, min_volume_ratio=3.0, lookback_weeks=52
        )
        
        if volume_surge_idx is None:
            volume_surge_idx = max(0, start_idx - 20)
        
        # æå–ç‰¹å¾
        features = analyzer.extract_features_at_start_point(
            stock_code, volume_surge_idx, lookback_weeks=40, weekly_df=weekly_df
        )
        
        if features:
            features['è‚¡ç¥¨ä»£ç '] = stock_code
            features['è‚¡ç¥¨åç§°'] = stock_name
            all_features_list.append(features)
            print(f"    âœ… æˆåŠŸæå– {len(features)} ä¸ªç‰¹å¾")
        else:
            print(f"    âŒ æå–ç‰¹å¾å¤±è´¥")
    
    if len(all_features_list) == 0:
        print("âŒ æœªèƒ½æå–ä»»ä½•ç‰¹å¾")
        return
    
    print(f"\nå…±æå– {len(all_features_list)} åªè‚¡ç¥¨çš„ç‰¹å¾")
    
    # ä¼˜åŒ–ç‰¹å¾ç»Ÿè®¡å€¼ï¼ˆä½¿ç”¨å®é™…çš„åŒ¹é…åº¦è®¡ç®—å‡½æ•°ï¼‰
    optimized_common_features = optimize_features(analyzer, all_features_list, target_score=0.98)
    
    # æ›´æ–°è®­ç»ƒç‰¹å¾
    analyzer.trained_features = {
        'common_features': optimized_common_features,
        'sample_count': len(all_features_list),
        'trained_at': datetime.now(),
        'sample_stocks': [f['è‚¡ç¥¨ä»£ç '] for f in all_features_list]
    }
    
    # ä¿å­˜æ¨¡å‹
    model_file = 'trained_model.json'
    if analyzer.save_model(model_file):
        print(f"\nâœ… æ¨¡å‹å·²ä¿å­˜åˆ°: {model_file}")
    else:
        print("\nâŒ ä¿å­˜æ¨¡å‹å¤±è´¥")
        return
    
    # éªŒè¯æ‰€æœ‰è®­ç»ƒæ ·æœ¬çš„åŒ¹é…åº¦
    print("\n" + "=" * 80)
    print("éªŒè¯è®­ç»ƒç»“æœ...")
    print("=" * 80)
    
    results = []
    for features in all_features_list:
        code = features['è‚¡ç¥¨ä»£ç ']
        name = features['è‚¡ç¥¨åç§°']
        
        # ä½¿ç”¨å®é™…çš„åŒ¹é…åº¦è®¡ç®—å‡½æ•°
        match_result = analyzer._calculate_match_score(features, optimized_common_features, tolerance=0.3)
        score = match_result['æ€»åŒ¹é…åº¦']
        
        results.append({
            'code': code,
            'name': name,
            'score': score
        })
        
        status = "âœ…" if score >= 0.98 else "âš ï¸"
        print(f"{status} {code} {name:<12} åŒ¹é…åº¦: {score:.4f}")
    
    # ç»Ÿè®¡
    scores = [r['score'] for r in results]
    avg_score = sum(scores) / len(scores)
    min_score = min(scores)
    max_score = max(scores)
    
    print("\n" + "-" * 80)
    print(f"ç»Ÿè®¡ä¿¡æ¯:")
    print(f"  å¹³å‡åŒ¹é…åº¦: {avg_score:.4f}")
    print(f"  æœ€ä½åŒ¹é…åº¦: {min_score:.4f}")
    print(f"  æœ€é«˜åŒ¹é…åº¦: {max_score:.4f}")
    
    passed = sum(1 for s in scores if s >= 0.98)
    print(f"  è¾¾åˆ°0.98ä»¥ä¸Š: {passed}/{len(scores)} åª")
    
    if min_score >= 0.98:
        print("\nâœ… æ‰€æœ‰è®­ç»ƒæ ·æœ¬çš„åŒ¹é…åº¦éƒ½è¾¾åˆ°äº†0.98ä»¥ä¸Šï¼")
    else:
        print(f"\nâš ï¸ è¿˜æœ‰ {len(scores) - passed} åªè‚¡ç¥¨çš„åŒ¹é…åº¦æœªè¾¾åˆ°0.98")
    
    print("=" * 80)

if __name__ == '__main__':
    main()
