# 数据更新方案：午盘/尾盘后新浪 + 盘中腾讯

## 一、需求总结

| 场景 | 数据源 | 执行时间 |
|------|--------|----------|
| **扫描用数据（绝大部分时候）** | GitHub 数据包 | 随时（从缓存读取） |
| **午盘/尾盘后更新** | 新浪财经 API | 11:35（午盘后）、15:05（尾盘后） |
| **盘中扫描当天数据** | 腾讯实时行情 | 交易时间段内（9:30-11:30, 13:00-15:00） |

---

## 二、实现方案

### 1. 扫描用数据：GitHub 数据包（保持不变）

- **做法**：继续使用 GitHub 数据包，扫描时 100% 从缓存读取
- **优点**：速度快、不超时、数据一致
- **结论**：**无需修改**

---

### 2. 午盘/尾盘后从新浪更新

#### 2.1 实现方式

**API 端点**：`GET /api/update_data_from_sina?batch=0&batch_size=50`

**特点**：
- 支持**分批处理**（适配 Vercel 10 秒限制）
- 每批处理 50 只股票（可调整）
- 只更新**今天这一根日K**（today-only 模式）
- 使用新浪财经 API（`update_data_sina.py`）

**执行时间**：
- 午盘后：11:35（11:30 收盘后 5 分钟）
- 尾盘后：15:05（15:00 收盘后 5 分钟）

#### 2.2 分批处理流程

由于 Vercel 10 秒限制，全市场 5000+ 只股票无法一次完成，需要分批：

```
批次 0: 处理 0-50 只股票（10秒内完成）
批次 1: 处理 50-100 只股票（10秒内完成）
批次 2: 处理 100-150 只股票（10秒内完成）
...
批次 N: 处理最后一批（10秒内完成）
```

**自动分批调用**：
- 每批完成后，返回 `next_batch_url`
- 使用外部服务（GitHub Actions、UptimeRobot）自动调用下一批
- 或使用本地/国内服务器设置 cron，循环调用直到完成

#### 2.3 Vercel Cron 配置

**注意**：Vercel Hobby 账户**每天只能运行 1 次 cron job**。

**解决方案**：

**方案 A（推荐）**：使用外部服务调用 API
- 使用 **GitHub Actions** 或 **UptimeRobot** 在 11:35 和 15:05 调用 API
- 自动处理所有批次（循环调用直到 `completed: true`）

**方案 B**：本地/国内服务器设置 cron
- 在本地或国内服务器（如阿里云、腾讯云）设置 cron
- 在 11:35 和 15:05 调用 `https://www.daniugu.online/api/update_data_from_sina?batch=0&batch_size=50`
- 自动处理所有批次

**方案 C**：升级到 Vercel Pro
- 解锁每天多次 cron job
- 可以直接在 `vercel.json` 配置两个 cron

#### 2.4 代码实现

**已创建文件**：
- `api/update_data_from_sina.py` - 分批处理逻辑
- `bull_stock_web.py` - 添加了 `/api/update_data_from_sina` 路由
- `vercel.json` - 已添加 cron 配置（但受 Hobby 限制，建议用外部服务）

---

### 3. 盘中扫描当天数据：从腾讯拉取

#### 3.1 实现逻辑

**触发条件**：
- 扫描日期是**今天**
- 当前时间在**交易时间段**（9:30-11:30, 13:00-15:00）
- 扫描时调用 `get_daily_kline_range` 获取当天数据

**数据源**：**腾讯实时行情**（`qt.gtimg.cn`）

**实现位置**：
- `data_fetcher.py` 的 `get_daily_kline_range()` 方法
- 已修改：如果满足条件，从腾讯获取实时 K 线数据

#### 3.2 腾讯批量接口

**已创建文件**：`tencent_realtime.py`

**功能**：
- `get_tencent_realtime_batch(codes)` - 批量获取多只股票的实时行情
- `get_tencent_today_kline(code)` - 获取单只股票今天的 K 线数据

**接口格式**：
```
GET http://qt.gtimg.cn/q=sz000001,sh600519,sz000858
```

**返回数据**：
- 现价、涨跌幅、成交量、最高、最低、开盘等
- 自动转换为与本地缓存一致的 DataFrame 格式

#### 3.3 代码修改

**已修改**：
- `data_fetcher.py` - `get_daily_kline_range()` 方法
  - 检测是否是盘中扫描今天的数据
  - 如果是，从腾讯获取实时数据
  - 合并到缓存的历史数据中

---

## 三、完整工作流程

### 场景 1：日常扫描（非今天）

```
用户点击扫描
  ↓
检查扫描日期（不是今天）
  ↓
从 GitHub 数据包读取 K 线（缓存）
  ↓
执行扫描
  ↓
返回结果
```

**数据源**：GitHub 数据包（缓存）

---

### 场景 2：盘中扫描今天的数据

```
用户点击扫描（扫描日期 = 今天）
  ↓
检查当前时间（交易时间段内）
  ↓
对每只股票：
  - 历史 K 线：从 GitHub 数据包读取
  - 今天 K 线：从腾讯实时接口获取
  ↓
合并数据
  ↓
执行扫描
  ↓
返回结果
```

**数据源**：
- 历史 K 线：GitHub 数据包
- 今天 K 线：腾讯实时接口

---

### 场景 3：午盘/尾盘后更新数据

```
11:35 或 15:05（Cron 触发或外部服务调用）
  ↓
调用 /api/update_data_from_sina?batch=0&batch_size=50
  ↓
处理批次 0（50 只股票，从新浪更新今天的数据）
  ↓
返回 next_batch_url
  ↓
外部服务自动调用下一批
  ↓
重复直到所有批次完成
  ↓
数据更新完成（本地缓存已更新）
```

**数据源**：新浪财经 API

**注意**：更新后的数据保存在 Vercel 的临时文件系统中，**不会持久化**。如果需要持久化，需要：
1. 更新后重新打包上传到 GitHub Releases
2. 或使用外部存储（如 Upstash Redis、Vercel KV）

---

## 四、Vercel Hobby 限制与解决方案

### 限制

- **每天只能运行 1 次 cron job**
- 无法同时配置 11:35 和 15:05 两个 cron

### 解决方案

#### 方案 A：GitHub Actions（推荐，免费）

创建 `.github/workflows/update_data.yml`：

```yaml
name: 更新股票数据

on:
  schedule:
    - cron: '35 11 * * 1-5'  # 午盘后
    - cron: '5 15 * * 1-5'    # 尾盘后
  workflow_dispatch:  # 允许手动触发

jobs:
  update:
    runs-on: ubuntu-latest
    steps:
      - name: 更新数据（分批处理）
        run: |
          BATCH=0
          while true; do
            RESPONSE=$(curl -s "https://www.daniugu.online/api/update_data_from_sina?batch=$BATCH&batch_size=50")
            COMPLETED=$(echo $RESPONSE | jq -r '.completed')
            if [ "$COMPLETED" = "true" ]; then
              echo "所有批次已完成"
              break
            fi
            BATCH=$((BATCH + 1))
            sleep 2
          done
```

**优点**：
- 免费
- 支持每天多次执行
- 自动处理所有批次

#### 方案 B：UptimeRobot（推荐，免费）

1. 注册 UptimeRobot：https://uptimerobot.com
2. 创建两个 HTTP(S) Monitor：
   - Monitor 1：`https://www.daniugu.online/api/update_data_from_sina?batch=0&batch_size=50`
     - 间隔：每天 11:35
   - Monitor 2：`https://www.daniugu.online/api/update_data_from_sina?batch=0&batch_size=50`
     - 间隔：每天 15:05
3. 配置自动重试（处理分批）

**优点**：
- 免费（50 个 monitor）
- 支持自定义间隔
- 自动监控和重试

#### 方案 C：本地/国内服务器 Cron

在本地或国内服务器设置 cron：

```bash
# 午盘后更新（11:35）
35 11 * * 1-5 curl "https://www.daniugu.online/api/update_data_from_sina?batch=0&batch_size=50"

# 尾盘后更新（15:05）
5 15 * * 1-5 curl "https://www.daniugu.online/api/update_data_from_sina?batch=0&batch_size=50"
```

**优点**：
- 完全控制
- 无限制
- 可以添加分批处理逻辑

---

## 五、数据持久化问题

### 问题

Vercel 的文件系统是**临时的**，更新后的数据在函数执行结束后可能丢失。

### 解决方案

#### 方案 A：更新后重新打包上传 GitHub（推荐）

1. 午盘/尾盘后更新完成后
2. 在本地或外部服务器：
   - 从 Vercel 下载更新后的数据（或直接在本地上传）
   - 打包成 tar.gz
   - 上传到 GitHub Releases
3. 下次 Vercel 冷启动时，自动下载最新数据包

#### 方案 B：使用外部存储（Upstash Redis / Vercel KV）

- 将更新后的 K 线数据保存到 Redis/KV
- 扫描时从 Redis/KV 读取
- **注意**：Redis/KV 存储成本较高，不适合存储大量 K 线数据

#### 方案 C：混合方案（推荐）

- **扫描用数据**：GitHub 数据包（历史数据，定期更新）
- **盘中实时数据**：腾讯实时接口（不存储，按需获取）
- **午盘/尾盘更新**：更新本地缓存（临时），定期打包上传 GitHub

---

## 六、实施检查清单

### 已完成

- [x] 创建 `tencent_realtime.py` - 腾讯批量实时接口
- [x] 创建 `api/update_data_from_sina.py` - 午盘/尾盘后更新 API
- [x] 修改 `data_fetcher.py` - 盘中扫描今天数据时从腾讯获取
- [x] 修改 `bull_stock_web.py` - 添加 `/api/update_data_from_sina` 路由
- [x] 修改 `vercel.json` - 添加 cron 配置（受 Hobby 限制）

### 待完成

- [ ] 配置外部服务（GitHub Actions 或 UptimeRobot）自动调用更新 API
- [ ] 测试午盘/尾盘后更新功能
- [ ] 测试盘中扫描今天数据功能（从腾讯获取）
- [ ] （可选）实现数据持久化方案（更新后重新打包上传 GitHub）

---

## 七、使用说明

### 手动触发更新

```bash
# 午盘后更新（从批次 0 开始）
curl "https://www.daniugu.online/api/update_data_from_sina?batch=0&batch_size=50"

# 尾盘后更新（从批次 0 开始）
curl "https://www.daniugu.online/api/update_data_from_sina?batch=0&batch_size=50"

# 强制执行（不在允许时间也可以执行）
curl "https://www.daniugu.online/api/update_data_from_sina?force=true&batch=0&batch_size=50"
```

### 分批处理

API 返回示例：

```json
{
  "success": true,
  "message": "批次 0 处理完成（50 只股票，耗时 8.5秒）",
  "total_stocks": 5000,
  "batch_num": 0,
  "processed": 50,
  "has_next": true,
  "next_batch": 1,
  "next_batch_url": "/api/update_data_from_sina?batch=1&batch_size=50",
  "completed": false
}
```

**自动处理下一批**：
```bash
# 调用返回的 next_batch_url
curl "https://www.daniugu.online/api/update_data_from_sina?batch=1&batch_size=50"
```

重复直到 `completed: true`。

---

## 八、总结

| 场景 | 数据源 | 执行方式 | 数据持久化 |
|------|--------|----------|------------|
| **扫描（非今天）** | GitHub 数据包 | 从缓存读取 | ✅ 已持久化（GitHub） |
| **扫描（今天，盘中）** | 腾讯实时接口 | 按需获取 | ❌ 不存储（实时） |
| **午盘/尾盘后更新** | 新浪财经 API | 分批处理（外部服务调用） | ⚠️ 临时（需定期打包上传 GitHub） |

**关键点**：
1. **扫描主要用 GitHub**，速度快、不超时
2. **盘中扫描今天**，用腾讯实时，轻量快速
3. **午盘/尾盘后更新**，用新浪，分批处理，适配 10 秒限制
4. **需要外部服务**（GitHub Actions / UptimeRobot）自动调用更新 API，因为 Vercel Hobby 每天只能运行 1 次 cron

---

*文档版本：2026-01-27；适用场景：Vercel 部署、中国大陆用户、10 秒限制。*
