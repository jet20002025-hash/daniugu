#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
2025å¹´12æœˆå›æµ‹ï¼šæ¯å‘¨é€‰å‡ºåŒ¹é…åº¦å‰5çš„ä¸ªè‚¡
åŸºäºæœ¬åœ°ç¼“å­˜æ•°æ®ï¼Œä½¿ç”¨æœ€æ–°æ¨¡å‹ trained_model.json
"""
import sys
import os
import json
import pandas as pd
import numpy as np
from datetime import datetime, timedelta

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

from bull_stock_analyzer import BullStockAnalyzer


def get_dec_2025_weeks():
    """
    è·å–2025å¹´12æœˆçš„æ¯å‘¨æ—¥æœŸ
    è¿”å›æ¯å‘¨äº”çš„æ—¥æœŸï¼ˆä½œä¸ºè¯¥å‘¨çš„æ‰«ææˆªæ­¢æ—¥æœŸï¼‰
    """
    weeks = [
        '2025-12-05',  # ç¬¬1å‘¨ï¼ˆ12æœˆç¬¬ä¸€ä¸ªå®Œæ•´äº¤æ˜“å‘¨ï¼‰
        '2025-12-12',  # ç¬¬2å‘¨
        '2025-12-19',  # ç¬¬3å‘¨
        '2025-12-26',  # ç¬¬4å‘¨
        '2025-12-31',  # ç¬¬5å‘¨ï¼ˆæœˆæœ«ï¼‰
    ]
    return weeks


def backtest_single_week(analyzer, scan_date, common_features, top_n=5, min_match_score=0.95, max_market_cap=100.0):
    """
    å¯¹å•å‘¨è¿›è¡Œå›æµ‹ï¼Œæ‰¾å‡ºåŒ¹é…åº¦æœ€é«˜çš„Nåªè‚¡ç¥¨
    
    :param analyzer: BullStockAnalyzerå®ä¾‹
    :param scan_date: æ‰«ææ—¥æœŸï¼ˆè¯¥å‘¨æœ«çš„æ—¥æœŸï¼‰
    :param common_features: è®­ç»ƒå¥½çš„ç‰¹å¾æ¨¡æ¿
    :param top_n: è¿”å›å‰NåªåŒ¹é…åº¦æœ€é«˜çš„è‚¡ç¥¨
    :param min_match_score: æœ€ä½åŒ¹é…åº¦é˜ˆå€¼ï¼ˆç”¨äºåˆæ­¥ç­›é€‰ï¼‰
    :param max_market_cap: æœ€å¤§æµé€šå¸‚å€¼ï¼ˆäº¿å…ƒï¼‰
    :return: åŒ¹é…åº¦å‰Nçš„è‚¡ç¥¨åˆ—è¡¨
    """
    print(f"\n{'='*60}")
    print(f"ğŸ“… æ‰«ææ—¥æœŸ: {scan_date}")
    print(f"   åŒ¹é…åº¦é˜ˆå€¼: {min_match_score} | æµé€šå¸‚å€¼ä¸Šé™: {max_market_cap}äº¿")
    print(f"{'='*60}")
    
    # è·å–è‚¡ç¥¨åˆ—è¡¨ï¼ˆä»æœ¬åœ°ç¼“å­˜ï¼‰
    cache_dir = os.path.join(os.path.dirname(os.path.abspath(__file__)), 'cache')
    stock_list_path = os.path.join(cache_dir, 'stock_list_all.json')
    
    if not os.path.exists(stock_list_path):
        print("âŒ è‚¡ç¥¨åˆ—è¡¨ç¼“å­˜ä¸å­˜åœ¨")
        return []
    
    with open(stock_list_path, 'r', encoding='utf-8') as f:
        stock_list = json.load(f)
    
    # åŠ è½½æµé€šå¸‚å€¼æ•°æ®
    market_cap_path = os.path.join(cache_dir, 'market_cap.json')
    market_cap_data = {}
    if os.path.exists(market_cap_path):
        with open(market_cap_path, 'r', encoding='utf-8') as f:
            market_cap_data = json.load(f)
    
    print(f"ğŸ“Š å…± {len(stock_list)} åªè‚¡ç¥¨å¾…æ‰«æ")
    
    # æ‰«ææ‰€æœ‰è‚¡ç¥¨
    candidates = []
    processed = 0
    skipped = 0
    skipped_market_cap = 0
    
    for stock_info in stock_list:
        stock_code = stock_info.get('code', '')
        stock_name = stock_info.get('name', '')
        
        # æ’é™¤STè‚¡ç¥¨
        if 'ST' in stock_name.upper():
            skipped += 1
            continue
        
        # æ’é™¤åŒ—äº¤æ‰€è‚¡ç¥¨ï¼ˆ8å¼€å¤´ã€9å¼€å¤´ï¼‰
        if stock_code.startswith('8') or stock_code.startswith('9'):
            skipped += 1
            continue
        
        # æµé€šå¸‚å€¼ç­›é€‰
        cap_info = market_cap_data.get(stock_code, {})
        circulating_cap = cap_info.get('circulating_cap', 0)
        if circulating_cap > max_market_cap:
            skipped_market_cap += 1
            continue
        
        processed += 1
        
        # æ˜¾ç¤ºè¿›åº¦
        if processed % 500 == 0:
            print(f"   è¿›åº¦: {processed} | å·²æ‰¾åˆ°å€™é€‰: {len(candidates)}")
        
        # å¤„ç†å•åªè‚¡ç¥¨
        result = process_single_stock(
            analyzer, stock_code, stock_name, 
            common_features, scan_date, min_match_score
        )
        
        if result:
            result['æµé€šå¸‚å€¼'] = round(circulating_cap, 2)
            candidates.append(result)
    
    print(f"âœ… æ‰«æå®Œæˆ: å¤„ç† {processed} åª | å¸‚å€¼è¿‡å¤§è·³è¿‡ {skipped_market_cap} åª | æ‰¾åˆ° {len(candidates)} åªå€™é€‰")
    
    # æŒ‰åŒ¹é…åº¦æ’åºï¼Œå–å‰Nåª
    candidates_sorted = sorted(candidates, key=lambda x: x.get('åŒ¹é…åº¦', 0), reverse=True)
    top_stocks = candidates_sorted[:top_n]
    
    # æ˜¾ç¤ºç»“æœ
    if top_stocks:
        print(f"\nğŸ“ˆ {scan_date} åŒ¹é…åº¦å‰{top_n}çš„ä¸ªè‚¡ (åŒ¹é…åº¦â‰¥{min_match_score}):")
        print(f"{'æ’å':<4} {'ä»£ç ':<8} {'åç§°':<12} {'åŒ¹é…åº¦':<8} {'ä»·æ ¼':<8} {'æµé€šå¸‚å€¼':<10}")
        print("-" * 60)
        for i, stock in enumerate(top_stocks, 1):
            cap_str = f"{stock.get('æµé€šå¸‚å€¼', 0):.1f}äº¿"
            print(f"{i:<4} {stock['è‚¡ç¥¨ä»£ç ']:<8} {stock['è‚¡ç¥¨åç§°']:<12} "
                  f"{stock['åŒ¹é…åº¦']:.3f}    {stock['ä»·æ ¼']:.2f}    {cap_str}")
    else:
        print("âš ï¸ æœªæ‰¾åˆ°ç¬¦åˆæ¡ä»¶çš„è‚¡ç¥¨")
    
    return top_stocks


def process_single_stock(analyzer, stock_code, stock_name, common_features, scan_date, min_match_score):
    """
    å¤„ç†å•åªè‚¡ç¥¨ï¼Œè®¡ç®—åŒ¹é…åº¦
    """
    try:
        # ä»æœ¬åœ°ç¼“å­˜è·å–å‘¨Kçº¿æ•°æ®
        cache_dir = os.path.join(os.path.dirname(os.path.abspath(__file__)), 'cache', 'weekly_kline')
        
        # å°è¯•ä¸åŒçš„ç¼“å­˜æ–‡ä»¶æ ¼å¼
        csv_path = os.path.join(cache_dir, f'{stock_code}.csv')
        json_path = os.path.join(cache_dir, f'{stock_code}.json')
        
        weekly_df = None
        
        if os.path.exists(csv_path):
            weekly_df = pd.read_csv(csv_path)
        elif os.path.exists(json_path):
            with open(json_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
            weekly_df = pd.DataFrame(data)
        else:
            return None
        
        if weekly_df is None or len(weekly_df) < 40:
            return None
        
        # æ ‡å‡†åŒ–åˆ—å
        column_mapping = {
            'æ—¥æœŸ': 'æ—¥æœŸ',
            'date': 'æ—¥æœŸ',
            'æ”¶ç›˜': 'æ”¶ç›˜',
            'close': 'æ”¶ç›˜',
            'å¼€ç›˜': 'å¼€ç›˜',
            'open': 'å¼€ç›˜',
            'æœ€é«˜': 'æœ€é«˜',
            'high': 'æœ€é«˜',
            'æœ€ä½': 'æœ€ä½',
            'low': 'æœ€ä½',
            'å‘¨æˆäº¤é‡': 'å‘¨æˆäº¤é‡',
            'æˆäº¤é‡': 'å‘¨æˆäº¤é‡',
            'volume': 'å‘¨æˆäº¤é‡',
        }
        
        weekly_df = weekly_df.rename(columns=column_mapping)
        
        # ç¡®ä¿æ—¥æœŸåˆ—å­˜åœ¨
        if 'æ—¥æœŸ' not in weekly_df.columns:
            return None
        
        # æŒ‰æ—¥æœŸç­›é€‰ï¼ˆæˆªæ­¢åˆ°scan_dateï¼‰
        try:
            scan_ts = pd.to_datetime(scan_date, errors='coerce')
            weekly_df['__dt'] = pd.to_datetime(weekly_df['æ—¥æœŸ'], errors='coerce')
            weekly_df = weekly_df.dropna(subset=['__dt'])
            weekly_df = weekly_df[weekly_df['__dt'] <= scan_ts]
            weekly_df = weekly_df.drop(columns=['__dt'])
            
            if len(weekly_df) < 40:
                return None
        except Exception:
            return None
        
        # ä½¿ç”¨æœ€åä¸€æ¡ä½œä¸ºå½“å‰ç‚¹
        current_idx = len(weekly_df) - 1
        
        # æå–ç‰¹å¾
        features = analyzer.extract_features_at_start_point(
            stock_code, current_idx, lookback_weeks=40, weekly_df=weekly_df
        )
        
        if features is None:
            return None
        
        # è®¡ç®—åŒ¹é…åº¦
        match_result = analyzer._calculate_match_score(features, common_features, tolerance=0.3)
        total_match = match_result['æ€»åŒ¹é…åº¦']
        
        if total_match < min_match_score:
            return None
        
        # è·å–å½“å‰ä»·æ ¼
        current_price = float(weekly_df.iloc[current_idx]['æ”¶ç›˜'])
        current_date = weekly_df.iloc[current_idx]['æ—¥æœŸ']
        
        if isinstance(current_date, pd.Timestamp):
            current_date_str = current_date.strftime('%Y-%m-%d')
        else:
            current_date_str = str(current_date)
        
        return {
            'è‚¡ç¥¨ä»£ç ': stock_code,
            'è‚¡ç¥¨åç§°': stock_name,
            'åŒ¹é…åº¦': round(total_match, 3),
            'ä»·æ ¼': round(current_price, 2),
            'æ—¥æœŸ': current_date_str,
            'æ ¸å¿ƒç‰¹å¾åŒ¹é…': match_result.get('æ ¸å¿ƒç‰¹å¾åŒ¹é…', {})
        }
        
    except Exception as e:
        return None


def calculate_future_returns(top_stocks, scan_date, periods=[1, 2, 4, 8]):
    """
    è®¡ç®—é€‰å‡ºçš„è‚¡ç¥¨åœ¨æœªæ¥Nå‘¨çš„æ”¶ç›Š
    
    :param top_stocks: é€‰å‡ºçš„è‚¡ç¥¨åˆ—è¡¨
    :param scan_date: æ‰«ææ—¥æœŸ
    :param periods: æ”¶ç›Šè®¡ç®—å‘¨æœŸï¼ˆå‘¨æ•°ï¼‰
    :return: å¸¦æœ‰æ”¶ç›Šä¿¡æ¯çš„è‚¡ç¥¨åˆ—è¡¨
    """
    cache_dir = os.path.join(os.path.dirname(os.path.abspath(__file__)), 'cache', 'weekly_kline')
    
    for stock in top_stocks:
        stock_code = stock['è‚¡ç¥¨ä»£ç ']
        buy_price = stock['ä»·æ ¼']
        
        # è¯»å–å‘¨Kçº¿æ•°æ®
        csv_path = os.path.join(cache_dir, f'{stock_code}.csv')
        json_path = os.path.join(cache_dir, f'{stock_code}.json')
        
        weekly_df = None
        if os.path.exists(csv_path):
            weekly_df = pd.read_csv(csv_path)
        elif os.path.exists(json_path):
            with open(json_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
            weekly_df = pd.DataFrame(data)
        
        if weekly_df is None or len(weekly_df) == 0:
            continue
        
        # æ ‡å‡†åŒ–åˆ—å
        if 'close' in weekly_df.columns:
            weekly_df = weekly_df.rename(columns={'close': 'æ”¶ç›˜', 'date': 'æ—¥æœŸ'})
        
        # è½¬æ¢æ—¥æœŸ
        weekly_df['__dt'] = pd.to_datetime(weekly_df['æ—¥æœŸ'], errors='coerce')
        weekly_df = weekly_df.dropna(subset=['__dt'])
        weekly_df = weekly_df.sort_values('__dt').reset_index(drop=True)
        
        # æ‰¾åˆ°ä¹°å…¥ç‚¹
        scan_ts = pd.to_datetime(scan_date)
        buy_idx = None
        for i, row in weekly_df.iterrows():
            if row['__dt'] <= scan_ts:
                buy_idx = i
            else:
                break
        
        if buy_idx is None:
            continue
        
        # è®¡ç®—å„å‘¨æœŸæ”¶ç›Š
        for period in periods:
            future_idx = buy_idx + period
            if future_idx < len(weekly_df):
                future_price = float(weekly_df.iloc[future_idx]['æ”¶ç›˜'])
                ret = (future_price - buy_price) / buy_price * 100
                stock[f'{period}å‘¨åæ”¶ç›Š'] = round(ret, 2)
            else:
                stock[f'{period}å‘¨åæ”¶ç›Š'] = None
    
    return top_stocks


def main():
    """ä¸»å‡½æ•°"""
    print("=" * 80)
    print("ğŸš€ 2025å¹´12æœˆå›æµ‹ - æ¯å‘¨é€‰å‡ºåŒ¹é…åº¦å‰5çš„ä¸ªè‚¡")
    print("=" * 80)
    print()
    
    # åŠ è½½æ¨¡å‹
    model_path = 'trained_model.json'
    print(f"ğŸ“‚ åŠ è½½æ¨¡å‹: {model_path}")
    
    analyzer = BullStockAnalyzer(auto_load_default_stocks=False, auto_analyze_and_train=False)
    
    if not analyzer.load_model(model_path, skip_network=True):
        print("âŒ æ¨¡å‹åŠ è½½å¤±è´¥")
        return
    
    print("âœ… æ¨¡å‹åŠ è½½æˆåŠŸ")
    
    # è·å–ç‰¹å¾æ¨¡æ¿
    common_features = analyzer.trained_features.get('common_features', {})
    print(f"ğŸ“Š ç‰¹å¾æ¨¡æ¿åŒ…å« {len(common_features)} ä¸ªç‰¹å¾")
    print()
    
    # è·å–12æœˆæ¯å‘¨æ—¥æœŸ
    weeks = get_dec_2025_weeks()
    print(f"ğŸ“… å›æµ‹å‘¨æ•°: {len(weeks)} å‘¨")
    print(f"   æ—¥æœŸ: {', '.join(weeks)}")
    print()
    
    # å­˜å‚¨æ‰€æœ‰ç»“æœ
    all_results = []
    
    # é€å‘¨å›æµ‹
    for week_date in weeks:
        top_stocks = backtest_single_week(
            analyzer, week_date, common_features, 
            top_n=5, min_match_score=0.95, max_market_cap=100.0
        )
        
        # è®¡ç®—æœªæ¥æ”¶ç›Š
        if top_stocks:
            top_stocks = calculate_future_returns(top_stocks, week_date)
            
            for stock in top_stocks:
                stock['æ‰«æå‘¨'] = week_date
                all_results.append(stock)
    
    # æ±‡æ€»ç»“æœ
    print("\n" + "=" * 80)
    print("ğŸ“Š å›æµ‹æ±‡æ€»")
    print("=" * 80)
    
    if all_results:
        df = pd.DataFrame(all_results)
        
        # æ˜¾ç¤ºæ±‡æ€»è¡¨æ ¼
        print("\nå„å‘¨é€‰è‚¡ç»“æœ:")
        print("-" * 100)
        
        # æŒ‰å‘¨åˆ†ç»„æ˜¾ç¤º
        for week_date in weeks:
            week_stocks = [s for s in all_results if s.get('æ‰«æå‘¨') == week_date]
            if week_stocks:
                print(f"\nğŸ“… {week_date}:")
                print(f"{'æ’å':<4} {'ä»£ç ':<8} {'åç§°':<12} {'åŒ¹é…åº¦':<8} {'ä¹°å…¥ä»·':<8} "
                      f"{'1å‘¨æ”¶ç›Š':<10} {'2å‘¨æ”¶ç›Š':<10} {'4å‘¨æ”¶ç›Š':<10}")
                print("-" * 90)
                for i, s in enumerate(week_stocks, 1):
                    ret_1w = s.get('1å‘¨åæ”¶ç›Š', '--')
                    ret_2w = s.get('2å‘¨åæ”¶ç›Š', '--')
                    ret_4w = s.get('4å‘¨åæ”¶ç›Š', '--')
                    ret_1w_str = f"{ret_1w}%" if ret_1w is not None else '--'
                    ret_2w_str = f"{ret_2w}%" if ret_2w is not None else '--'
                    ret_4w_str = f"{ret_4w}%" if ret_4w is not None else '--'
                    print(f"{i:<4} {s['è‚¡ç¥¨ä»£ç ']:<8} {s['è‚¡ç¥¨åç§°']:<12} "
                          f"{s['åŒ¹é…åº¦']:.3f}    {s['ä»·æ ¼']:<8.2f} "
                          f"{ret_1w_str:<10} {ret_2w_str:<10} {ret_4w_str:<10}")
        
        # ç»Ÿè®¡å¹³å‡æ”¶ç›Š
        print("\n" + "=" * 80)
        print("ğŸ“ˆ æ”¶ç›Šç»Ÿè®¡:")
        print("-" * 50)
        
        for period in [1, 2, 4, 8]:
            col = f'{period}å‘¨åæ”¶ç›Š'
            if col in df.columns:
                valid_returns = df[col].dropna()
                if len(valid_returns) > 0:
                    avg_ret = valid_returns.mean()
                    win_rate = (valid_returns > 0).sum() / len(valid_returns) * 100
                    max_ret = valid_returns.max()
                    min_ret = valid_returns.min()
                    print(f"{period}å‘¨å: å¹³å‡æ”¶ç›Š {avg_ret:.2f}% | èƒœç‡ {win_rate:.1f}% | "
                          f"æœ€é«˜ {max_ret:.2f}% | æœ€ä½ {min_ret:.2f}%")
        
        # ä¿å­˜ç»“æœ
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        output_file = f'backtest_dec_2025_weekly_{timestamp}.csv'
        df.to_csv(output_file, index=False, encoding='utf-8-sig')
        print(f"\nâœ… ç»“æœå·²ä¿å­˜åˆ°: {output_file}")
        
    else:
        print("âš ï¸ æœªæ‰¾åˆ°ä»»ä½•ç¬¦åˆæ¡ä»¶çš„è‚¡ç¥¨")
    
    print("\n" + "=" * 80)
    print("å›æµ‹å®Œæˆï¼")
    print("=" * 80)


if __name__ == '__main__':
    main()
