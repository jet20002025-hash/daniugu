#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
è°ƒæ•´æ¨¡å‹å‚æ•°ï¼Œä½¿ç”¨ä¸æ‰«æå™¨ç›¸åŒçš„ç‰¹å¾æå–é€»è¾‘ï¼ˆä»æˆäº¤é‡çªå¢ç‚¹æå–ç‰¹å¾ï¼‰
"""
import sys
sys.path.insert(0, '.')
from bull_stock_analyzer import BullStockAnalyzer
import json
import pandas as pd
import numpy as np
import warnings
warnings.filterwarnings('ignore')

# ç›®æ ‡è‚¡ç¥¨åŠå…¶åŒ¹é…åº¦ï¼ˆä»å›¾ç‰‡ä¸­ï¼‰
TARGET_STOCKS = {
    '300141': ('å’Œé¡ºç”µæ°”', 0.970),
    '002928': ('åå¤èˆªç©º', 0.964),
    '002811': ('éƒ‘ä¸­è®¾è®¡', 0.962),
    '001217': ('åå°”æ³°', 0.961),
    '002197': ('è¯é€šç”µå­', 0.954),
    '002810': ('å±±ä¸œèµ«è¾¾', 0.951),
    '603008': ('å–œä¸´é—¨', 0.947),
    '603212': ('èµ›ä¼æŠ€æœ¯', 0.947),
    '002636': ('é‡‘å®‰å›½çºª', 0.942),
    '000532': ('åé‡‘èµ„æœ¬', 0.939),
    '002538': ('å¸å°”ç‰¹', 0.937),
    '002957': ('ç§‘ç‘æŠ€æœ¯', 0.937),
    '300414': ('ä¸­å…‰é˜²é›·', 0.936),
    '605155': ('è¥¿å¤§é—¨', 0.935),
    '002253': ('*STæ™ºèƒœ', 0.933),
    '301027': ('åè“é›†å›¢', 0.930),
}

# ä¸æ‰«æå™¨ç›¸åŒçš„æ ¸å¿ƒç‰¹å¾å®šä¹‰
SUPER_CORE_FEATURES = ['ç›ˆåˆ©ç­¹ç æ¯”ä¾‹', 'ä»·æ ¼ç›¸å¯¹ä½ç½®']
CORE_FEATURES = [
    'ç›ˆåˆ©ç­¹ç æ¯”ä¾‹', 'ä»·æ ¼ç›¸å¯¹ä½ç½®', '90%æˆæœ¬é›†ä¸­åº¦', 'èµ·ç‚¹å½“å‘¨é‡æ¯”', 'æˆäº¤é‡èç¼©ç¨‹åº¦', 'ä»·æ ¼ç›¸å¯¹MA20',
    'èµ·ç‚¹å‰20å‘¨æ³¢åŠ¨ç‡', 'æ˜¯å¦è·Œç ´æœ€å¤§é‡æœ€ä½ä»·', 'èµ·ç‚¹å‰40å‘¨æœ€å¤§é‡', 'ç›¸å¯¹é«˜ç‚¹è·Œå¹…'
]

def calculate_match_score_scanner(features, template):
    """ä½¿ç”¨ä¸BullStockAnalyzer._calculate_match_scoreå®Œå…¨ç›¸åŒçš„è®¡ç®—é€»è¾‘"""
    if not features or not template:
        return 0.0
    
    super_core_scores = []
    core_scores = []
    normal_scores = []
    
    for feature_name, feature_value in features.items():
        if feature_name in ['è‚¡ç¥¨ä»£ç ', 'è‚¡ç¥¨åç§°', 'èµ·ç‚¹æ—¥æœŸ', 'ç»ˆç‚¹æ—¥æœŸ']:
            continue
        
        if not isinstance(feature_value, (int, float)) or pd.isna(feature_value):
            continue
        
        if feature_name not in template:
            continue
        
        stats = template[feature_name]
        mean = stats.get('å‡å€¼', 0)
        std = stats.get('æ ‡å‡†å·®', 0)
        min_val = stats.get('æœ€å°å€¼', 0)
        max_val = stats.get('æœ€å¤§å€¼', 0)
        
        is_super_core = feature_name in SUPER_CORE_FEATURES
        
        if std > 0:
            z_score = abs((feature_value - mean) / std)
            decay_factor = 0.3 if is_super_core else 0.5
            match_score = 1.0 / (1.0 + z_score * decay_factor)
        else:
            match_score = 1.0 if abs(feature_value - mean) < 0.01 else 0.0
        
        if min_val <= feature_value <= max_val:
            match_score = min(1.0, match_score + 0.1)
        
        if is_super_core and std > 0:
            z_score = abs((feature_value - mean) / std)
            if z_score > 2.0:
                match_score = match_score * 0.5
            elif z_score > 1.5:
                match_score = match_score * 0.7
        
        if feature_name in SUPER_CORE_FEATURES:
            super_core_scores.append(match_score)
        elif feature_name in CORE_FEATURES:
            core_scores.append(match_score)
        else:
            normal_scores.append(match_score)
    
    super_core_weight = 6.0
    core_weight = 4.0
    normal_weight = 1.0
    
    super_core_avg = np.mean(super_core_scores) if super_core_scores else 0.0
    core_avg = np.mean(core_scores) if core_scores else 0.0
    normal_avg = np.mean(normal_scores) if normal_scores else 0.0
    
    total_weight = (len(super_core_scores) * super_core_weight + 
                    len(core_scores) * core_weight + 
                    len(normal_scores) * normal_weight)
    
    if total_weight > 0:
        total_match = (
            super_core_avg * len(super_core_scores) * super_core_weight +
            core_avg * len(core_scores) * core_weight +
            normal_avg * len(normal_scores) * normal_weight
        ) / total_weight
    else:
        total_match = 0.0
    
    return round(float(total_match), 3)

def extract_features_like_scanner(analyzer, code, scan_date='2025-12-31'):
    """
    ä½¿ç”¨ä¸æ‰«æå™¨ç›¸åŒçš„é€»è¾‘æå–ç‰¹å¾ï¼š
    1. è·å–å‘¨çº¿æ•°æ®
    2. æ‰¾æˆäº¤é‡çªå¢ç‚¹
    3. ä»çªå¢ç‚¹æå–ç‰¹å¾
    """
    kline = analyzer.fetcher.get_weekly_kline(code, period='2y')
    if kline is None or len(kline) == 0:
        return None, None
    
    kline['æ—¥æœŸ'] = pd.to_datetime(kline['æ—¥æœŸ'])
    scan_dt = pd.to_datetime(scan_date)
    mask = kline['æ—¥æœŸ'] <= scan_dt
    filtered = kline[mask].copy()
    
    if len(filtered) < 40:
        return None, None
    
    buy_point_idx = len(filtered) - 1
    
    # æ‰¾æˆäº¤é‡çªå¢ç‚¹ï¼ˆä¸æ‰«æå™¨é€»è¾‘ä¸€è‡´ï¼‰
    volume_surge_idx = analyzer.find_volume_surge_point(code, buy_point_idx, filtered, 
                                                         min_volume_ratio=3.0, lookback_weeks=52)
    
    # ç¡®å®šç‰¹å¾èµ·ç‚¹ï¼ˆä¸æ‰«æå™¨é€»è¾‘ä¸€è‡´ï¼‰
    if volume_surge_idx is not None and volume_surge_idx >= 40:
        feature_idx = volume_surge_idx
    else:
        feature_idx = buy_point_idx
    
    # æå–ç‰¹å¾
    features = analyzer.extract_features_at_start_point(code, feature_idx, lookback_weeks=40, weekly_df=filtered)
    
    return features, feature_idx

def main():
    print("=" * 80)
    print("è°ƒæ•´æ¨¡å‹å‚æ•°ï¼ˆä½¿ç”¨ä¸æ‰«æå™¨ç›¸åŒçš„ç‰¹å¾æå–é€»è¾‘ï¼‰")
    print("ç›®æ ‡: å’Œé¡ºç”µæ°” 0.970 æ’ç¬¬ä¸€")
    print("=" * 80)
    print()
    
    analyzer = BullStockAnalyzer(auto_load_default_stocks=False, auto_analyze_and_train=False)
    
    print("ğŸ“Š ä½¿ç”¨æ‰«æå™¨é€»è¾‘æå–ç›®æ ‡è‚¡ç¥¨ç‰¹å¾...")
    all_features = {}
    
    for code, (name, target_score) in TARGET_STOCKS.items():
        features, feature_idx = extract_features_like_scanner(analyzer, code)
        if features:
            all_features[code] = {
                'name': name,
                'target_score': target_score,
                'features': features,
                'feature_idx': feature_idx
            }
            print(f"  âœ… {code} {name}: ç‰¹å¾èµ·ç‚¹ç´¢å¼•={feature_idx}, ç‰¹å¾æ•°={len(features)}")
        else:
            print(f"  âš ï¸ {code} {name}: ç‰¹å¾æå–å¤±è´¥")
    
    print(f"\næˆåŠŸæå– {len(all_features)} åªè‚¡ç¥¨çš„ç‰¹å¾")
    
    if len(all_features) == 0:
        print("âŒ æ²¡æœ‰æå–åˆ°ç‰¹å¾")
        return
    
    # æ”¶é›†ç‰¹å¾ç»Ÿè®¡ä¿¡æ¯
    heshun = all_features.get('300141')
    if not heshun:
        print("âŒ æœªæ‰¾åˆ°å’Œé¡ºç”µæ°”")
        return
    
    heshun_features = heshun['features']
    
    print("\nğŸ”§ å¼€å§‹ä¼˜åŒ–æ¨¡æ¿å‚æ•°...")
    
    # æ”¶é›†æ‰€æœ‰ç‰¹å¾çš„ç»Ÿè®¡ä¿¡æ¯
    feature_stats = {}
    for feature_name in heshun_features.keys():
        if not isinstance(heshun_features[feature_name], (int, float)):
            continue
        
        values = [all_features[code]['features'].get(feature_name) 
                  for code in all_features 
                  if code in all_features and 
                  isinstance(all_features[code]['features'].get(feature_name), (int, float))]
        
        if len(values) > 0:
            feature_stats[feature_name] = {
                'values': values,
                'mean': np.mean(values),
                'std': np.std(values) if len(values) > 1 else 0.1,
                'min': min(values),
                'max': max(values),
                'heshun': heshun_features[feature_name]
            }
    
    best_template = None
    best_error = float('inf')
    
    for iteration in range(500):
        template = {}
        
        mean_shift = (iteration % 50) / 100.0
        std_scale = 1.0 + (iteration % 30) * 0.1
        range_expand = 0.2 + (iteration % 20) * 0.05
        
        for feature_name, stats in feature_stats.items():
            heshun_val = stats['heshun']
            mean_val = stats['mean']
            std_val = stats['std']
            
            adjusted_mean = heshun_val * (1 - mean_shift) + mean_val * mean_shift
            adjusted_std = max(std_val * std_scale, abs(heshun_val - mean_val) * 0.5, 0.01)
            
            range_size = max(abs(stats['max'] - stats['min']), 0.1)
            adjusted_min = stats['min'] - range_size * range_expand
            adjusted_max = stats['max'] + range_size * range_expand
            
            template[feature_name] = {
                "å‡å€¼": adjusted_mean,
                "ä¸­ä½æ•°": heshun_val,
                "æœ€å°å€¼": adjusted_min,
                "æœ€å¤§å€¼": adjusted_max,
                "æ ‡å‡†å·®": adjusted_std,
                "æ ·æœ¬æ•°": len(stats['values'])
            }
        
        calculated_scores = {}
        for code, info in all_features.items():
            score = calculate_match_score_scanner(info['features'], template)
            calculated_scores[code] = score
        
        total_error = 0
        sorted_scores = sorted(calculated_scores.items(), key=lambda x: x[1], reverse=True)
        
        if sorted_scores[0][0] != '300141':
            total_error += 5.0
        
        for code, info in all_features.items():
            target = info['target_score']
            actual = calculated_scores[code]
            total_error += (target - actual) ** 2
        
        if total_error < best_error:
            best_error = total_error
            best_template = {k: v.copy() for k, v in template.items()}
            
            if iteration % 100 == 0 or total_error < 0.1:
                print(f"  è¿­ä»£ {iteration}: è¯¯å·®={total_error:.4f}")
                for i, (code, score) in enumerate(sorted_scores[:3]):
                    name = all_features[code]['name']
                    target = all_features[code]['target_score']
                    print(f"    {i+1}. {code} {name}: {score:.3f} (ç›®æ ‡: {target:.3f})")
    
    # ç²¾ç»†è°ƒæ•´
    print("\nğŸ”¬ ç²¾ç»†è°ƒæ•´...")
    for iteration in range(1000):
        new_template = {k: v.copy() for k, v in best_template.items()}
        
        feature_name = np.random.choice(list(new_template.keys()))
        params = new_template[feature_name]
        
        adjustment = np.random.choice(['mean', 'std', 'range'])
        if adjustment == 'mean':
            params['å‡å€¼'] *= (0.98 + np.random.random() * 0.04)
        elif adjustment == 'std':
            params['æ ‡å‡†å·®'] *= (0.9 + np.random.random() * 0.2)
        else:
            params['æœ€å°å€¼'] *= (0.95 + np.random.random() * 0.1)
            params['æœ€å¤§å€¼'] *= (0.95 + np.random.random() * 0.1)
        
        calculated_scores = {}
        for code, info in all_features.items():
            score = calculate_match_score_scanner(info['features'], new_template)
            calculated_scores[code] = score
        
        total_error = 0
        sorted_scores = sorted(calculated_scores.items(), key=lambda x: x[1], reverse=True)
        
        if sorted_scores[0][0] != '300141':
            total_error += 10.0
        
        for code, info in all_features.items():
            target = info['target_score']
            actual = calculated_scores[code]
            total_error += (target - actual) ** 2
        
        if total_error < best_error:
            best_error = total_error
            best_template = {k: v.copy() for k, v in new_template.items()}
    
    # æœ€ç»ˆéªŒè¯
    print("\nğŸ“Š æœ€ç»ˆéªŒè¯:")
    calculated_scores = {}
    for code, info in all_features.items():
        score = calculate_match_score_scanner(info['features'], best_template)
        calculated_scores[code] = score
    
    sorted_scores = sorted(calculated_scores.items(), key=lambda x: x[1], reverse=True)
    
    print(f"\n{'æ’å':<4} {'è‚¡ç¥¨ä»£ç ':<10} {'è‚¡ç¥¨åç§°':<12} {'ç›®æ ‡åŒ¹é…åº¦':<12} {'å®é™…åŒ¹é…åº¦':<12} {'å·®å¼‚':<8}")
    print("-" * 70)
    
    for i, (code, score) in enumerate(sorted_scores, 1):
        if code in all_features:
            name = all_features[code]['name']
            target = all_features[code]['target_score']
            diff = score - target
            print(f"{i:<4} {code:<10} {name:<12} {target:.3f}        {score:.3f}        {diff:+.3f}")
    
    # ä¿å­˜æ¨¡å‹
    model = {
        "trained_at": pd.Timestamp.now().isoformat(),
        "buy_features": {
            "common_features": best_template,
            "sample_count": len(all_features),
            "trained_at": pd.Timestamp.now().isoformat(),
            "sample_stocks": list(all_features.keys()),
            "training_stocks": list(all_features.keys()),
            "match_scores": {
                code: {
                    "è‚¡ç¥¨åç§°": info['name'],
                    "åŒ¹é…åº¦": calculated_scores[code]
                }
                for code, info in all_features.items()
            }
        },
        "sell_features": None,
        "bull_stocks": [
            {
                "ä»£ç ": code,
                "åç§°": info['name'],
                "æ·»åŠ æ—¶é—´": pd.Timestamp.now().isoformat(),
                "æ•°æ®æ¡æ•°": 0
            }
            for code, info in all_features.items()
        ]
    }
    
    model_path = 'models/ç›®æ ‡æ¨¡å‹_å®Œå…¨åŒ¹é…æ‰«æå™¨.json'
    with open(model_path, 'w', encoding='utf-8') as f:
        json.dump(model, f, ensure_ascii=False, indent=2, default=str)
    
    print(f"\nâœ… æ¨¡å‹å·²ä¿å­˜åˆ°: {model_path}")
    return model_path

if __name__ == '__main__':
    main()
