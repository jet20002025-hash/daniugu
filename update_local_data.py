#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¢é‡æ›´æ–°æœ¬åœ°Kçº¿æ•°æ®åˆ°æœ€æ–°æ—¥æœŸ
"""
import os
import sys
import json
import time
import pandas as pd
import akshare as ak
from datetime import datetime, timedelta
from concurrent.futures import ThreadPoolExecutor, as_completed

# é…ç½®
CACHE_DIR = 'cache'
DAILY_DIR = os.path.join(CACHE_DIR, 'daily_kline')
WEEKLY_DIR = os.path.join(CACHE_DIR, 'weekly_kline')
MAX_WORKERS = 5  # å¹¶å‘æ•°
RETRY_TIMES = 3  # é‡è¯•æ¬¡æ•°


def get_stock_list():
    """è·å–è‚¡ç¥¨åˆ—è¡¨"""
    stock_list_path = os.path.join(CACHE_DIR, 'stock_list_all.json')
    if os.path.exists(stock_list_path):
        with open(stock_list_path, 'r', encoding='utf-8') as f:
            return json.load(f)
    return None


def update_daily_kline(code, start_date, end_date):
    """æ›´æ–°æ—¥Kçº¿æ•°æ®"""
    for attempt in range(RETRY_TIMES):
        try:
            df = ak.stock_zh_a_hist(
                symbol=code,
                period="daily",
                start_date=start_date,
                end_date=end_date,
                adjust="qfq"
            )
            if df is not None and not df.empty:
                # é‡å‘½ååˆ—
                df = df.rename(columns={
                    df.columns[0]: 'æ—¥æœŸ',
                    df.columns[1]: 'å¼€ç›˜',
                    df.columns[2]: 'æ”¶ç›˜',
                    df.columns[3]: 'æœ€é«˜',
                    df.columns[4]: 'æœ€ä½',
                    df.columns[5]: 'æˆäº¤é‡',
                })
                df['æ—¥æœŸ'] = pd.to_datetime(df['æ—¥æœŸ'])
                return df
            return None
        except Exception as e:
            if attempt < RETRY_TIMES - 1:
                time.sleep(0.5 * (attempt + 1))
            continue
    return None


def update_weekly_kline(code, start_date, end_date):
    """æ›´æ–°å‘¨Kçº¿æ•°æ®"""
    for attempt in range(RETRY_TIMES):
        try:
            df = ak.stock_zh_a_hist(
                symbol=code,
                period="weekly",
                start_date=start_date,
                end_date=end_date,
                adjust="qfq"
            )
            if df is not None and not df.empty:
                # é‡å‘½ååˆ—
                if len(df.columns) >= 6:
                    df = df.rename(columns={
                        df.columns[0]: 'æ—¥æœŸ',
                        df.columns[1]: 'å¼€ç›˜',
                        df.columns[2]: 'æ”¶ç›˜',
                        df.columns[3]: 'æœ€é«˜',
                        df.columns[4]: 'æœ€ä½',
                        df.columns[5]: 'å‘¨æˆäº¤é‡',
                    })
                df['æ—¥æœŸ'] = pd.to_datetime(df['æ—¥æœŸ'])
                return df
            return None
        except Exception as e:
            if attempt < RETRY_TIMES - 1:
                time.sleep(0.5 * (attempt + 1))
            continue
    return None


def process_single_stock(code, name, start_date, end_date):
    """å¤„ç†å•åªè‚¡ç¥¨çš„æ•°æ®æ›´æ–°"""
    result = {'code': code, 'name': name, 'daily_updated': 0, 'weekly_updated': 0, 'error': None}
    
    try:
        # æ›´æ–°æ—¥Kçº¿
        daily_path = os.path.join(DAILY_DIR, f'{code}.csv')
        if os.path.exists(daily_path):
            existing_daily = pd.read_csv(daily_path)
            existing_daily['æ—¥æœŸ'] = pd.to_datetime(existing_daily['æ—¥æœŸ'])
            
            # è·å–å¢é‡æ•°æ®
            new_daily = update_daily_kline(code, start_date, end_date)
            if new_daily is not None and len(new_daily) > 0:
                # åˆå¹¶æ•°æ®ï¼Œå»é‡
                combined = pd.concat([existing_daily, new_daily], ignore_index=True)
                combined = combined.drop_duplicates(subset=['æ—¥æœŸ'], keep='last')
                combined = combined.sort_values('æ—¥æœŸ').reset_index(drop=True)
                combined['æ—¥æœŸ'] = combined['æ—¥æœŸ'].dt.strftime('%Y-%m-%d')
                combined.to_csv(daily_path, index=False, encoding='utf-8')
                result['daily_updated'] = len(new_daily)
        
        # æ›´æ–°å‘¨Kçº¿
        weekly_path = os.path.join(WEEKLY_DIR, f'{code}.csv')
        if os.path.exists(weekly_path):
            existing_weekly = pd.read_csv(weekly_path)
            existing_weekly['æ—¥æœŸ'] = pd.to_datetime(existing_weekly['æ—¥æœŸ'])
            
            # è·å–å¢é‡æ•°æ®
            new_weekly = update_weekly_kline(code, start_date, end_date)
            if new_weekly is not None and len(new_weekly) > 0:
                # åˆå¹¶æ•°æ®ï¼Œå»é‡
                combined = pd.concat([existing_weekly, new_weekly], ignore_index=True)
                combined = combined.drop_duplicates(subset=['æ—¥æœŸ'], keep='last')
                combined = combined.sort_values('æ—¥æœŸ').reset_index(drop=True)
                combined['æ—¥æœŸ'] = combined['æ—¥æœŸ'].dt.strftime('%Y-%m-%d')
                combined.to_csv(weekly_path, index=False, encoding='utf-8')
                result['weekly_updated'] = len(new_weekly)
        
        time.sleep(0.1)  # é¿å…è¯·æ±‚è¿‡å¿«
        
    except Exception as e:
        result['error'] = str(e)
    
    return result


def main():
    print("=" * 60)
    print("ğŸ“Š å¢é‡æ›´æ–°æœ¬åœ°Kçº¿æ•°æ®")
    print("=" * 60)
    
    # è·å–è‚¡ç¥¨åˆ—è¡¨
    stock_list = get_stock_list()
    if not stock_list:
        print("âŒ æ— æ³•è·å–è‚¡ç¥¨åˆ—è¡¨")
        return
    
    # è®¾ç½®æ›´æ–°æ—¥æœŸèŒƒå›´ï¼ˆä»2026å¹´1æœˆ1æ—¥åˆ°ä»Šå¤©ï¼‰
    start_date = '20260101'
    end_date = datetime.now().strftime('%Y%m%d')
    
    print(f"ğŸ“… æ›´æ–°æ—¥æœŸèŒƒå›´: {start_date} ~ {end_date}")
    print(f"ğŸ“ˆ è‚¡ç¥¨æ€»æ•°: {len(stock_list)}")
    print()
    
    # ä¸è¿‡æ»¤ï¼šSTã€åŒ—äº¤æ‰€ç­‰å…¨éƒ¨å‚ä¸è¿½åŠ æ›´æ–°
    valid_stocks = []
    for stock in stock_list:
        code = stock.get('code', stock.get('è‚¡ç¥¨ä»£ç ', ''))
        name = stock.get('name', stock.get('è‚¡ç¥¨åç§°', ''))
        if code:
            valid_stocks.append({'code': str(code).strip(), 'name': name or ''})
    
    print(f"ğŸ“Š å‚ä¸æ›´æ–°è‚¡ç¥¨æ•°: {len(valid_stocks)}ï¼ˆå…¨éƒ¨ï¼‰")
    print()
    
    # å¹¶è¡Œæ›´æ–°
    total = len(valid_stocks)
    completed = 0
    daily_total = 0
    weekly_total = 0
    errors = 0
    
    start_time = time.time()
    
    with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
        futures = {
            executor.submit(process_single_stock, s['code'], s['name'], start_date, end_date): s
            for s in valid_stocks
        }
        
        for future in as_completed(futures):
            completed += 1
            result = future.result()
            
            if result['error']:
                errors += 1
            else:
                daily_total += result['daily_updated']
                weekly_total += result['weekly_updated']
            
            # æ¯100åªè‚¡ç¥¨æ˜¾ç¤ºä¸€æ¬¡è¿›åº¦
            if completed % 100 == 0 or completed == total:
                elapsed = time.time() - start_time
                speed = completed / elapsed if elapsed > 0 else 0
                eta = (total - completed) / speed if speed > 0 else 0
                print(f"è¿›åº¦: {completed}/{total} ({completed/total*100:.1f}%) | "
                      f"æ—¥Kæ–°å¢: {daily_total} | å‘¨Kæ–°å¢: {weekly_total} | "
                      f"é”™è¯¯: {errors} | é€Ÿåº¦: {speed:.1f}åª/ç§’ | é¢„è®¡å‰©ä½™: {eta:.0f}ç§’")
    
    elapsed = time.time() - start_time
    print()
    print("=" * 60)
    print(f"âœ… æ›´æ–°å®Œæˆ!")
    print(f"   è€—æ—¶: {elapsed:.1f}ç§’")
    print(f"   æ—¥Kçº¿æ–°å¢è®°å½•: {daily_total}")
    print(f"   å‘¨Kçº¿æ–°å¢è®°å½•: {weekly_total}")
    print(f"   é”™è¯¯æ•°: {errors}")
    print("=" * 60)


if __name__ == '__main__':
    main()
