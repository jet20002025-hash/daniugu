"""
æ•°æ®è·å–æ¨¡å—
ä½¿ç”¨akshareè·å–Aè‚¡å¸‚åœºæ•°æ®
"""
import os
# å°½é‡é¿å…ç³»ç»Ÿä»£ç†/ç¯å¢ƒä»£ç†å½±å“æ•°æ®æ‹‰å–ï¼ˆæœ¬åœ°é¢„ä¸‹è½½/æ‰«ææ›´ç¨³å®šï¼‰
os.environ.setdefault("NO_PROXY", "*")
os.environ.setdefault("no_proxy", "*")
import akshare as ak
import pandas as pd
import time
from datetime import datetime, timedelta
import warnings
warnings.filterwarnings('ignore')


class DataFetcher:
    """æ•°æ®è·å–ç±»"""
    
    def __init__(self):
        self.stock_list = None
        self._market_cap_cache = None  # ç¼“å­˜å¸‚å€¼æ•°æ®ï¼Œé¿å…é‡å¤è·å–
        # âœ… æ£€æµ‹æ˜¯å¦åœ¨ Vercel ç¯å¢ƒï¼ˆä¼˜å…ˆä½¿ç”¨ GitHub æ•°æ®åŒ…ï¼Œä¸è¿æ¥å®æ—¶ APIï¼‰
        # æ£€æµ‹ Vercel ç¯å¢ƒï¼ˆå¤šç§æ–¹å¼ï¼‰
        is_vercel_env = (
            os.environ.get('VERCEL') == '1' or 
            os.environ.get('VERCEL_ENV') is not None or
            os.environ.get('VERCEL_URL') is not None
        )
        # æ£€æµ‹æ˜¯å¦å¼ºåˆ¶ä½¿ç”¨ GitHub æ•°æ®åŒ…
        use_github_only = os.environ.get('USE_GITHUB_DATA_ONLY') == '1'
        # åœ¨ Vercel ç¯å¢ƒä¸­è‡ªåŠ¨å¯ç”¨ GitHub æ•°æ®åŒ…æ¨¡å¼
        self._is_vercel = is_vercel_env
        self._use_github_data_only = use_github_only or is_vercel_env

    # =========================
    # æœ¬åœ°æ–‡ä»¶ç¼“å­˜ï¼ˆç”¨äºæœ¬åœ°ç¯å¢ƒé¢„ä¸‹è½½/é¢„çƒ­ï¼‰
    # =========================
    def _local_cache_dir(self) -> str:
        import os
        base = os.environ.get("LOCAL_CACHE_DIR")
        if base:
            return base
        # é»˜è®¤æ”¾åœ¨é¡¹ç›®ç›®å½•ä¸‹çš„ cache/ï¼ˆä½¿ç”¨æ–‡ä»¶æ‰€åœ¨ç›®å½•è€Œéå½“å‰å·¥ä½œç›®å½•ï¼‰
        return os.path.join(os.path.dirname(os.path.abspath(__file__)), "cache")

    def _local_cache_paths(self):
        import os
        base = self._local_cache_dir()
        return {
            "base": base,
            "stock_list_json": os.path.join(base, "stock_list_all.json"),
            "stock_list_meta": os.path.join(base, "stock_list_all.meta.json"),
            "weekly_dir": os.path.join(base, "weekly_kline"),
            "daily_dir": os.path.join(base, "daily_kline"),
        }
        
    def _get_stock_list_from_cache(self, check_age=False):
        """
        ä»ç¼“å­˜ä¸­è·å–è‚¡ç¥¨åˆ—è¡¨
        :param check_age: æ˜¯å¦æ£€æŸ¥ç¼“å­˜å¹´é¾„ï¼ˆç”¨äºåˆ¤æ–­æ˜¯å¦è¿‡æœŸï¼‰
        :return: å¦‚æœ check_age=Trueï¼Œè¿”å› (stock_df, cache_timestamp, is_expired)ï¼Œå¦åˆ™è¿”å› stock_df
        """
        try:
            import os
            import json
            from datetime import datetime, timezone
            
            # å°è¯•ä½¿ç”¨ Upstash Redis
            redis_url = os.environ.get('UPSTASH_REDIS_REST_URL')
            redis_token = os.environ.get('UPSTASH_REDIS_REST_TOKEN')
            if redis_url and redis_token:
                import requests
                try:
                    # è·å–ç¼“å­˜æ•°æ®å’Œæ—¶é—´æˆ³
                    response = requests.get(
                        f"{redis_url}/get/stock_list_all",
                        headers={"Authorization": f"Bearer {redis_token}"},
                        timeout=2  # ç¼“å­˜è·å–åº”è¯¥å¾ˆå¿«
                    )
                    if response.status_code == 200:
                        result = response.json()
                        value_str = result.get('result')
                        if value_str:
                            # è§£æ JSON å­—ç¬¦ä¸²
                            # æ³¨æ„ï¼šå¦‚æœä¿å­˜æ—¶ä½¿ç”¨äº† json=stock_jsonï¼ˆåŒé‡ç¼–ç ï¼‰ï¼Œè¿™é‡Œéœ€è¦è§£æä¸¤æ¬¡
                            # å…ˆå°è¯•è§£æä¸€æ¬¡
                            stock_data = json.loads(value_str) if isinstance(value_str, str) else value_str
                            # å¦‚æœè§£æåä»ç„¶æ˜¯å­—ç¬¦ä¸²ï¼ˆè¯´æ˜æ˜¯åŒé‡ç¼–ç ï¼‰ï¼Œå†æ¬¡è§£æ
                            if isinstance(stock_data, str):
                                try:
                                    stock_data = json.loads(stock_data)
                                except (json.JSONDecodeError, TypeError):
                                    # å¦‚æœç¬¬äºŒæ¬¡è§£æå¤±è´¥ï¼Œä½¿ç”¨ç¬¬ä¸€æ¬¡è§£æçš„ç»“æœ
                                    pass
                            # è½¬æ¢ä¸º DataFrameï¼ˆç¡®ä¿æ•°æ®æ ¼å¼æ­£ç¡®ï¼‰
                            if isinstance(stock_data, list) and len(stock_data) > 0:
                                import pandas as pd
                                stock_df = pd.DataFrame(stock_data)
                                
                                # å°è¯•è·å–ç¼“å­˜æ—¶é—´æˆ³
                                cache_timestamp = None
                                is_expired = False
                                if check_age:
                                    try:
                                        # å°è¯•è·å–ç¼“å­˜çš„TTLï¼ˆå‰©ä½™æ—¶é—´ï¼‰
                                        ttl_response = requests.get(
                                            f"{redis_url}/ttl/stock_list_all",
                                            headers={"Authorization": f"Bearer {redis_token}"},
                                            timeout=2
                                        )
                                        if ttl_response.status_code == 200:
                                            ttl_result = ttl_response.json()
                                            ttl_seconds = ttl_result.get('result', -1)
                                            if ttl_seconds > 0:
                                                # TTL = 86400ç§’ï¼ˆ24å°æ—¶ï¼‰ï¼Œç¼“å­˜æ—¶é—´ = å½“å‰æ—¶é—´ - (86400 - TTL)
                                                cache_age_seconds = 86400 - ttl_seconds
                                                cache_timestamp = datetime.now(timezone.utc).timestamp() - cache_age_seconds
                                                # å¦‚æœåœ¨äº¤æ˜“æ—¶é—´æ®µå†…ä¸”ç¼“å­˜è¶…è¿‡5åˆ†é’Ÿï¼Œè®¤ä¸ºè¿‡æœŸ
                                                from datetime import timedelta
                                                beijing_now = datetime.now(timezone.utc) + timedelta(hours=8)
                                                is_in_trading_time = (
                                                    (beijing_now.hour == 9 and beijing_now.minute >= 30) or
                                                    beijing_now.hour == 10 or
                                                    (beijing_now.hour == 11 and beijing_now.minute <= 30) or
                                                    beijing_now.hour == 13 or
                                                    beijing_now.hour == 14 or
                                                    (beijing_now.hour == 15 and beijing_now.minute == 0)
                                                )
                                                if is_in_trading_time and cache_age_seconds > 300:  # 5åˆ†é’Ÿ = 300ç§’
                                                    is_expired = True
                                                    print(f"[get_all_stocks] âš ï¸ ç¼“å­˜å·²è¿‡æœŸï¼ˆäº¤æ˜“æ—¶é—´æ®µå†…ï¼Œç¼“å­˜å¹´é¾„: {cache_age_seconds//60}åˆ†é’Ÿï¼‰ï¼Œéœ€è¦åˆ·æ–°")
                                    except Exception as e:
                                        print(f"[get_all_stocks] âš ï¸ è·å–ç¼“å­˜TTLå¤±è´¥: {e}")
                                
                                if check_age:
                                    print(f"[get_all_stocks] âœ… ä» Redis ç¼“å­˜è·å–è‚¡ç¥¨åˆ—è¡¨: {len(stock_df)} åªï¼Œç¼“å­˜å¹´é¾„: {cache_age_seconds//60 if cache_timestamp else 'unknown'}åˆ†é’Ÿ")
                                    return stock_df, cache_timestamp, is_expired
                                else:
                                    print(f"[get_all_stocks] âœ… ä» Redis ç¼“å­˜è·å–è‚¡ç¥¨åˆ—è¡¨: {len(stock_df)} åª")
                                    return stock_df
                            else:
                                print(f"[get_all_stocks] âš ï¸ ç¼“å­˜æ•°æ®æ ¼å¼é”™è¯¯: {type(stock_data)}")
                except Exception as e:
                    print(f"[get_all_stocks] âš ï¸ ä» Redis ç¼“å­˜è·å–å¤±è´¥: {e}")
            
            # å°è¯•ä½¿ç”¨ Vercel KVï¼ˆVercel KV ä¸æ”¯æŒ TTL æŸ¥è¯¢ï¼Œæš‚æ—¶ä¸æ£€æŸ¥å¹´é¾„ï¼‰
            try:
                from vercel_kv import kv
                cached_data = kv.get('stock_list_all')
                if cached_data:
                    stock_data = json.loads(cached_data) if isinstance(cached_data, str) else cached_data
                    # è½¬æ¢ä¸º DataFrameï¼ˆç¡®ä¿æ•°æ®æ ¼å¼æ­£ç¡®ï¼‰
                    if isinstance(stock_data, list) and len(stock_data) > 0:
                        import pandas as pd
                        stock_df = pd.DataFrame(stock_data)
                        if check_age:
                            # Vercel KV ä¸æ”¯æŒTTLæŸ¥è¯¢ï¼Œè¿”å› None è¡¨ç¤ºæœªçŸ¥
                            print(f"[get_all_stocks] âœ… ä» Vercel KV ç¼“å­˜è·å–è‚¡ç¥¨åˆ—è¡¨: {len(stock_df)} åªï¼ˆæ— æ³•æ£€æŸ¥ç¼“å­˜å¹´é¾„ï¼‰")
                            return stock_df, None, False
                        else:
                            print(f"[get_all_stocks] âœ… ä» Vercel KV ç¼“å­˜è·å–è‚¡ç¥¨åˆ—è¡¨: {len(stock_df)} åª")
                            return stock_df
                    else:
                        print(f"[get_all_stocks] âš ï¸ Vercel KV ç¼“å­˜æ•°æ®æ ¼å¼é”™è¯¯: {type(stock_data)}")
            except Exception as e:
                print(f"[get_all_stocks] âš ï¸ ä» Vercel KV ç¼“å­˜è·å–å¤±è´¥: {e}")
            
            # âœ… æœ¬åœ°æ–‡ä»¶ç¼“å­˜ï¼ˆæ—  Redis/KV æ—¶çš„å…œåº•ï¼‰
            try:
                paths = self._local_cache_paths()
                stock_path = paths["stock_list_json"]
                meta_path = paths["stock_list_meta"]
                if os.path.exists(stock_path):
                    with open(stock_path, "r", encoding="utf-8") as f:
                        stock_data = json.load(f)
                    if isinstance(stock_data, list) and len(stock_data) > 0:
                        stock_df = pd.DataFrame(stock_data)
                        cache_timestamp = None
                        is_expired = False
                        if os.path.exists(meta_path):
                            try:
                                with open(meta_path, "r", encoding="utf-8") as f:
                                    meta = json.load(f)
                                cache_timestamp = meta.get("saved_at")
                                ttl = meta.get("ttl", 86400)
                                if check_age and cache_timestamp:
                                    age = datetime.now(timezone.utc).timestamp() - float(cache_timestamp)
                                    # äº¤æ˜“æ—¶æ®µå†…ç¼“å­˜è¶…è¿‡5åˆ†é’Ÿè§†ä¸ºè¿‡æœŸï¼›éäº¤æ˜“æ—¶æ®µæŒ‰ ttl åˆ¤æ–­
                                    beijing_now = datetime.now(timezone.utc) + timedelta(hours=8)
                                    is_in_trading_time = (
                                        (beijing_now.hour == 9 and beijing_now.minute >= 30) or
                                        beijing_now.hour == 10 or
                                        (beijing_now.hour == 11 and beijing_now.minute <= 30) or
                                        beijing_now.hour == 13 or
                                        beijing_now.hour == 14 or
                                        (beijing_now.hour == 15 and beijing_now.minute == 0)
                                    )
                                    if is_in_trading_time and age > 300:
                                        is_expired = True
                                    elif (not is_in_trading_time) and ttl and age > float(ttl):
                                        is_expired = True
                            except Exception:
                                pass
                        if check_age:
                            return stock_df, cache_timestamp, is_expired
                        return stock_df
            except Exception as e:
                # é™é»˜å¤±è´¥
                pass
                
        except Exception as e:
            print(f"[get_all_stocks] âš ï¸ ä»ç¼“å­˜è·å–è‚¡ç¥¨åˆ—è¡¨å¤±è´¥: {e}")
        
        if check_age:
            return None, None, True  # ç¼“å­˜ä¸å­˜åœ¨ï¼Œè®¤ä¸ºè¿‡æœŸ
        return None
    
    def _save_stock_list_to_cache(self, stock_df):
        """å°†è‚¡ç¥¨åˆ—è¡¨ä¿å­˜åˆ°ç¼“å­˜ï¼ˆTTL: 24å°æ—¶ = 86400ç§’ï¼‰"""
        try:
            import os
            import json
            
            # ç¡®ä¿ stock_df æ˜¯ DataFrame
            if stock_df is None or len(stock_df) == 0:
                print(f"[_save_stock_list_to_cache] âš ï¸ è‚¡ç¥¨åˆ—è¡¨ä¸ºç©ºï¼Œæ— æ³•ä¿å­˜åˆ°ç¼“å­˜")
                return False
            
            # å°† DataFrame è½¬æ¢ä¸º JSON æ ¼å¼ï¼ˆå­—å…¸åˆ—è¡¨ï¼‰
            try:
                stock_data = stock_df.to_dict('records')
                stock_json = json.dumps(stock_data, default=str, ensure_ascii=False)
                print(f"[_save_stock_list_to_cache] å‡†å¤‡ä¿å­˜ {len(stock_df)} åªè‚¡ç¥¨åˆ°ç¼“å­˜ï¼ŒJSON å¤§å°: {len(stock_json)} å­—ç¬¦")
            except Exception as e:
                print(f"[_save_stock_list_to_cache] âš ï¸ è½¬æ¢ DataFrame åˆ° JSON å¤±è´¥: {e}")
                import traceback
                print(traceback.format_exc())
                return False
            
            # å°è¯•ä½¿ç”¨ Upstash Redisï¼ˆæœ€å¤šé‡è¯•2æ¬¡ï¼‰
            redis_url = os.environ.get('UPSTASH_REDIS_REST_URL')
            redis_token = os.environ.get('UPSTASH_REDIS_REST_TOKEN')
            
            # è¯Šæ–­æ—¥å¿—ï¼šæ£€æŸ¥ç¯å¢ƒå˜é‡æ˜¯å¦è®¾ç½®
            if not redis_url:
                print(f"[_save_stock_list_to_cache] âš ï¸ UPSTASH_REDIS_REST_URL ç¯å¢ƒå˜é‡æœªè®¾ç½®ï¼Œè·³è¿‡ Redis ä¿å­˜")
            if not redis_token:
                print(f"[_save_stock_list_to_cache] âš ï¸ UPSTASH_REDIS_REST_TOKEN ç¯å¢ƒå˜é‡æœªè®¾ç½®ï¼Œè·³è¿‡ Redis ä¿å­˜")
            if redis_url and not redis_token:
                print(f"[_save_stock_list_to_cache] âš ï¸ UPSTASH_REDIS_REST_URL å·²è®¾ç½®ï¼Œä½† UPSTASH_REDIS_REST_TOKEN æœªè®¾ç½®ï¼Œè·³è¿‡ Redis ä¿å­˜")
            if not redis_url and redis_token:
                print(f"[_save_stock_list_to_cache] âš ï¸ UPSTASH_REDIS_REST_TOKEN å·²è®¾ç½®ï¼Œä½† UPSTASH_REDIS_REST_URL æœªè®¾ç½®ï¼Œè·³è¿‡ Redis ä¿å­˜")
            
            if redis_url and redis_token:
                print(f"[_save_stock_list_to_cache] âœ… Redis ç¯å¢ƒå˜é‡å·²è®¾ç½®ï¼Œå°è¯•ä¿å­˜åˆ° Redis...")
                import requests
                max_retries = 2
                for attempt in range(max_retries):
                    try:
                        # ç¼“å­˜ 24 å°æ—¶ï¼ˆ86400ç§’ï¼‰
                        # Upstash Redis REST API: POST /setex/{key}/{ttl}
                        # è¯·æ±‚ä½“æ ¼å¼ï¼šJSON å­—ç¬¦ä¸²ï¼ˆå€¼æœ¬èº«æ˜¯ JSON å­—ç¬¦ä¸²ï¼‰
                        # æ³¨æ„ï¼šä½¿ç”¨ data å‚æ•°å‘é€å­—ç¬¦ä¸²ï¼Œè€Œä¸æ˜¯ json å‚æ•°ï¼ˆé¿å…åŒé‡ç¼–ç ï¼‰
                        if attempt > 0:
                            print(f"[_save_stock_list_to_cache] é‡è¯•ä¿å­˜åˆ° Upstash Redisï¼ˆç¬¬ {attempt + 1}/{max_retries} æ¬¡ï¼‰...")
                        else:
                            print(f"[_save_stock_list_to_cache] å°è¯•ä¿å­˜åˆ° Upstash Redis...")
                        print(f"[_save_stock_list_to_cache] JSON å¤§å°: {len(stock_json)} å­—ç¬¦")
                        # Upstash Redis REST API setex éœ€è¦å°†å€¼ä½œä¸º JSON å­—ç¬¦ä¸²å‘é€
                        # æ³¨æ„ï¼šstock_json å·²ç»æ˜¯ JSON å­—ç¬¦ä¸²ï¼Œä½¿ç”¨ json å‚æ•°ä¼šå†æ¬¡ JSON ç¼–ç ï¼ˆåŒé‡ç¼–ç ï¼‰
                        # ä½†æ˜¯ scan_progress_store.py ä¸­çš„ _upstash_redis_set ä¹Ÿä½¿ç”¨äº† json=value_str
                        # è¿™è¯´æ˜ Upstash Redis REST API å¯èƒ½æ¥å—åŒé‡ç¼–ç çš„å€¼ï¼Œæˆ–è€…ä¼šè‡ªåŠ¨è§£æ
                        # ä¸ºäº†ä¿æŒä¸€è‡´ï¼Œæˆ‘ä»¬ä¹Ÿä½¿ç”¨ json å‚æ•°ï¼ˆä¸ scan_progress_store.py ä¿æŒä¸€è‡´ï¼‰
                        response = requests.post(
                            f"{redis_url}/setex/stock_list_all/86400",
                            headers={
                                "Authorization": f"Bearer {redis_token}",
                                "Content-Type": "application/json"
                            },
                            json=stock_json,  # ä½¿ç”¨ json å‚æ•°ï¼ˆä¸ scan_progress_store.py ä¿æŒä¸€è‡´ï¼‰
                            timeout=15  # å¢åŠ è¶…æ—¶æ—¶é—´åˆ°15ç§’ï¼ˆæ•°æ®è¾ƒå¤§ï¼Œå¯èƒ½éœ€è¦æ›´é•¿æ—¶é—´ï¼‰
                        )
                        if response.status_code == 200:
                            try:
                                result = response.json()
                                # Upstash è¿”å›æ ¼å¼: {"result": "OK"} æˆ– {"result": true}
                                if result.get('result') == 'OK' or result.get('result') is True:
                                    print(f"[_save_stock_list_to_cache] âœ… è‚¡ç¥¨åˆ—è¡¨å·²ä¿å­˜åˆ° Redis ç¼“å­˜ï¼ˆTTL: 24å°æ—¶ï¼Œè‚¡ç¥¨æ•°: {len(stock_df)}ï¼‰")
                                    return True
                                else:
                                    print(f"[_save_stock_list_to_cache] âš ï¸ Redis ä¿å­˜è¿”å›å¼‚å¸¸ç»“æœ: {result}")
                                    if attempt < max_retries - 1:
                                        import time
                                        time.sleep(1)  # ç­‰å¾…1ç§’åé‡è¯•
                                        continue
                            except Exception as parse_error:
                                print(f"[_save_stock_list_to_cache] âš ï¸ Redis ä¿å­˜å“åº”è§£æå¤±è´¥: {parse_error}ï¼Œä½†çŠ¶æ€ç ä¸º200ï¼Œè®¤ä¸ºä¿å­˜æˆåŠŸ")
                                return True
                        else:
                            try:
                                error_msg = response.text[:1000] if hasattr(response, 'text') else str(response.status_code)
                            except:
                                error_msg = f"çŠ¶æ€ç : {response.status_code}"
                            print(f"[_save_stock_list_to_cache] âš ï¸ Redis ä¿å­˜å¤±è´¥ï¼ŒçŠ¶æ€ç : {response.status_code}, å“åº”: {error_msg}")
                            print(f"[_save_stock_list_to_cache] å“åº”å¤´: {dict(response.headers) if hasattr(response, 'headers') else 'N/A'}")
                            if attempt < max_retries - 1:
                                import time
                                time.sleep(1)  # ç­‰å¾…1ç§’åé‡è¯•
                                continue
                    except requests.exceptions.Timeout:
                        print(f"[_save_stock_list_to_cache] âš ï¸ Redis ä¿å­˜è¶…æ—¶ï¼ˆ15ç§’ï¼‰")
                        if attempt < max_retries - 1:
                            import time
                            time.sleep(1)  # ç­‰å¾…1ç§’åé‡è¯•
                            continue
                    except Exception as e:
                        import traceback
                        error_detail = traceback.format_exc()
                        print(f"[_save_stock_list_to_cache] âš ï¸ ä¿å­˜åˆ° Redis ç¼“å­˜å¤±è´¥: {e}")
                        print(f"[_save_stock_list_to_cache] é”™è¯¯è¯¦æƒ…: {error_detail}")
                        if attempt < max_retries - 1:
                            import time
                            time.sleep(1)  # ç­‰å¾…1ç§’åé‡è¯•
                            continue
            
            # å°è¯•ä½¿ç”¨ Vercel KVï¼ˆå¦‚æœæ²¡æœ‰ä½¿ç”¨ Redis æˆ– Redis ä¿å­˜å¤±è´¥ï¼‰
            # å³ä½¿ Redis å¯ç”¨ï¼Œä¹Ÿå°è¯• Vercel KV ä½œä¸ºå¤‡ç”¨æ–¹æ¡ˆ
            try:
                from vercel_kv import kv
                print(f"[_save_stock_list_to_cache] å°è¯•ä¿å­˜åˆ° Vercel KVï¼ˆå¤‡ç”¨æ–¹æ¡ˆï¼‰...")
                print(f"[_save_stock_list_to_cache] JSON å¤§å°: {len(stock_json)} å­—ç¬¦")
                kv.set('stock_list_all', stock_json, ttl=86400)  # 24å°æ—¶
                print(f"[_save_stock_list_to_cache] âœ… è‚¡ç¥¨åˆ—è¡¨å·²ä¿å­˜åˆ° Vercel KV ç¼“å­˜ï¼ˆTTL: 24å°æ—¶ï¼Œè‚¡ç¥¨æ•°: {len(stock_df)}ï¼‰")
                return True
            except ImportError:
                print(f"[_save_stock_list_to_cache] âš ï¸ Vercel KV æœªå®‰è£…æˆ–ä¸å¯ç”¨ï¼ˆè¿™æ˜¯æ­£å¸¸çš„ï¼Œå¦‚æœä½¿ç”¨ Redisï¼‰")
                # ImportError ä¸æ˜¯çœŸæ­£çš„é”™è¯¯ï¼Œåªæ˜¯è¡¨ç¤º Vercel KV ä¸å¯ç”¨ï¼Œä¸é˜»æ­¢ç»§ç»­æ‰§è¡Œ
            except Exception as e:
                import traceback
                error_detail = traceback.format_exc()
                print(f"[_save_stock_list_to_cache] âš ï¸ ä¿å­˜åˆ° Vercel KV ç¼“å­˜å¤±è´¥: {e}")
                print(f"[_save_stock_list_to_cache] é”™è¯¯è¯¦æƒ…: {error_detail}")
                # Vercel KV å¤±è´¥ä¸æ˜¯è‡´å‘½é”™è¯¯ï¼Œç»§ç»­æ‰§è¡Œ
            
            # âœ… æœ¬åœ°æ–‡ä»¶ç¼“å­˜å…œåº•ï¼ˆå³ä½¿äº‘ç«¯ç¼“å­˜ä¸å¯ç”¨ï¼Œä¹Ÿèƒ½ç”¨äºæœ¬åœ°å®šæ—¶é¢„ä¸‹è½½ï¼‰
            try:
                import os
                from datetime import timezone
                paths = self._local_cache_paths()
                os.makedirs(paths["base"], exist_ok=True)
                with open(paths["stock_list_json"], "w", encoding="utf-8") as f:
                    json.dump(stock_data, f, ensure_ascii=False)
                with open(paths["stock_list_meta"], "w", encoding="utf-8") as f:
                    json.dump(
                        {"saved_at": datetime.now(timezone.utc).timestamp(), "ttl": 86400},
                        f,
                        ensure_ascii=False,
                    )
                print(f"[_save_stock_list_to_cache] âœ… è‚¡ç¥¨åˆ—è¡¨å·²ä¿å­˜åˆ°æœ¬åœ°ç¼“å­˜: {paths['stock_list_json']}")
                return True
            except Exception as e:
                # ç»§ç»­èµ°ä¸‹é¢çš„ç»Ÿä¸€å¤±è´¥è¿”å›
                pass
                
        except Exception as e:
            import traceback
            error_detail = traceback.format_exc()
            print(f"[_save_stock_list_to_cache] âš ï¸ ä¿å­˜è‚¡ç¥¨åˆ—è¡¨åˆ°ç¼“å­˜å¤±è´¥: {e}")
            print(f"[_save_stock_list_to_cache] é”™è¯¯è¯¦æƒ…: {error_detail}")
        
        print(f"[_save_stock_list_to_cache] âŒ æ‰€æœ‰ç¼“å­˜ä¿å­˜æ–¹å¼å‡å¤±è´¥ï¼Œè¿”å› False")
        return False
        
    def get_all_stocks(self, timeout=10, max_retries=3):
        """
        è·å–æ‰€æœ‰Aè‚¡è‚¡ç¥¨åˆ—è¡¨ï¼ˆä¼˜å…ˆä»ç¼“å­˜è·å–ï¼‰
        è¿”å›: DataFrameï¼ŒåŒ…å«è‚¡ç¥¨ä»£ç ã€åç§°ç­‰ä¿¡æ¯
        :param timeout: è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰ï¼Œé»˜è®¤10ç§’
        :param max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°ï¼Œé»˜è®¤3æ¬¡
        """
        import signal
        import threading
        import os
        import time
        
        # âœ… å¦‚æœè®¾ç½®äº† USE_GITHUB_DATA_ONLYï¼Œåªä½¿ç”¨ç¼“å­˜ï¼Œä¸è°ƒç”¨å®æ—¶ API
        if self._use_github_data_only:
            print("[get_all_stocks] âš ï¸  USE_GITHUB_DATA_ONLY æ¨¡å¼ï¼šåªä½¿ç”¨ç¼“å­˜ï¼Œä¸è¿æ¥å®æ—¶ API")
            cached = self._get_stock_list_from_cache()
            if cached is not None and len(cached) > 0:
                print(f"[get_all_stocks] âœ… ä»ç¼“å­˜è·å– {len(cached)} åªè‚¡ç¥¨")
                self.stock_list = cached
                return cached
            else:
                print("[get_all_stocks] âŒ ç¼“å­˜ä¸å­˜åœ¨ï¼Œä¸” USE_GITHUB_DATA_ONLY æ¨¡å¼ä¸‹ä¸è¿æ¥å®æ—¶ API")
                return None
        
        # é¦–å…ˆå°è¯•ä»ç¼“å­˜è·å–ï¼ˆä¼˜å…ˆä»ç¼“å­˜è¯»å–ï¼Œé¿å…æ¯æ¬¡è°ƒç”¨ akshare APIï¼‰
        # âœ… æœ¬åœ°ç­–ç•¥ï¼šä¸è¦æ¯æ¬¡ç™»å½•/è¿›å…¥é¡µé¢éƒ½åˆ·æ–°ã€‚æŒ‰â€œæ¯æ—¥ä¸¤æ¬¡â€èŠ‚æµåˆ·æ–°ï¼š
        # - 11:30ï¼ˆåˆç›˜åï¼‰
        # - 15:00ï¼ˆæ”¶ç›˜åï¼‰
        # åªæœ‰å½“ç¼“å­˜æ—¶é—´æ—©äºå½“æ—¥å¯¹åº”æ£€æŸ¥ç‚¹æ—¶ï¼Œæ‰è§¦å‘ä¸€æ¬¡åˆ·æ–°ï¼›å¦åˆ™ç›´æ¥ç”¨ç¼“å­˜ã€‚
        print("[get_all_stocks] å°è¯•ä»ç¼“å­˜è·å–è‚¡ç¥¨åˆ—è¡¨...")
        from datetime import datetime, timezone, timedelta
        beijing_now = datetime.now(timezone.utc) + timedelta(hours=8)

        def _checkpoint_dt(now_bj: datetime) -> tuple:
            """è¿”å› (required_checkpoint_dt, label)ã€‚å¦‚æœå½“å‰æ—¶é—´è¿˜æ²¡åˆ° 11:30ï¼Œåˆ™è¿”å› (None, None)ã€‚"""
            cp_1130 = now_bj.replace(hour=11, minute=30, second=0, microsecond=0)
            cp_1500 = now_bj.replace(hour=15, minute=0, second=0, microsecond=0)
            if now_bj >= cp_1500:
                return cp_1500, "15:00"
            if now_bj >= cp_1130:
                return cp_1130, "11:30"
            return None, None

        def _need_refresh_by_checkpoints(now_bj: datetime, cache_ts_utc: float) -> bool:
            """åˆ¤æ–­æ˜¯å¦éœ€è¦æŒ‰æ¯æ—¥æ£€æŸ¥ç‚¹åˆ·æ–°ç¼“å­˜ã€‚cache_ts_utc ä¸º UTC æ—¶é—´æˆ³ï¼ˆç§’ï¼‰ã€‚"""
            if not cache_ts_utc:
                return False
            required_cp, _ = _checkpoint_dt(now_bj)
            if required_cp is None:
                return False
            cache_bj = datetime.fromtimestamp(float(cache_ts_utc), tz=timezone.utc) + timedelta(hours=8)
            # åªè¦ç¼“å­˜æ—¶é—´æ—©äºå½“æ—¥å¯¹åº”æ£€æŸ¥ç‚¹ï¼Œå°±è®¤ä¸ºéœ€è¦åˆ·æ–°ä¸€æ¬¡
            return cache_bj < required_cp
        
        # ç»Ÿä¸€æŒ‰â€œæ¯æ—¥ä¸¤æ¬¡æ£€æŸ¥ç‚¹â€åˆ¤æ–­æ˜¯å¦éœ€è¦åˆ·æ–°
        expired_cache = None  # ä¿å­˜æ—§ç¼“å­˜ï¼Œä½œä¸ºå›é€€æ–¹æ¡ˆ
        cached_stocks, cache_timestamp, _legacy_is_expired = self._get_stock_list_from_cache(check_age=True)
        if cached_stocks is not None and len(cached_stocks) > 0:
            # æ²¡æœ‰æ‹¿åˆ°ç¼“å­˜æ—¶é—´æˆ³ï¼ˆæ¯”å¦‚ KV æ— æ³•æ¨æ–­ï¼‰å°±é»˜è®¤ä¸åˆ·æ–°ï¼Œé¿å…æ¯æ¬¡ç™»å½•è§¦å‘ç½‘ç»œè¯·æ±‚
            need_refresh = _need_refresh_by_checkpoints(beijing_now, cache_timestamp) if cache_timestamp else False
            if not need_refresh:
                self.stock_list = cached_stocks
                cp_dt, cp_label = _checkpoint_dt(beijing_now)
                if cp_label:
                    print(f"[get_all_stocks] âœ… ä½¿ç”¨ç¼“å­˜ï¼ˆå·²æ»¡è¶³å½“æ—¥ {cp_label} æ£€æŸ¥ç‚¹ï¼Œä¸åˆ·æ–°ï¼‰ï¼Œè‚¡ç¥¨æ•°: {len(cached_stocks)} åª")
                else:
                    print(f"[get_all_stocks] âœ… ä½¿ç”¨ç¼“å­˜ï¼ˆæœªåˆ° 11:30 æ£€æŸ¥ç‚¹ï¼Œä¸åˆ·æ–°ï¼‰ï¼Œè‚¡ç¥¨æ•°: {len(cached_stocks)} åª")
                return cached_stocks
            # éœ€è¦åˆ·æ–°ï¼šä¿ç•™æ—§ç¼“å­˜åšå›é€€
            expired_cache = cached_stocks
            cp_dt, cp_label = _checkpoint_dt(beijing_now)
            print(f"[get_all_stocks] âš ï¸ ç¼“å­˜æ—©äºå½“æ—¥ {cp_label} æ£€æŸ¥ç‚¹ï¼Œå°†å°è¯•ä» API åˆ·æ–°è‚¡ç¥¨åˆ—è¡¨...")
        else:
            print(f"[get_all_stocks] âš ï¸ ç¼“å­˜ä¸å­˜åœ¨æˆ–ä¸ºç©ºï¼Œå°†ä» API è·å–...")
        
        print("[get_all_stocks] âš ï¸ ç¼“å­˜ä¸­æ²¡æœ‰è‚¡ç¥¨åˆ—è¡¨ï¼Œå¼€å§‹ä» akshare API è·å–...")
        print("[get_all_stocks] ğŸ’¡ æç¤ºï¼šå»ºè®®åœ¨äº¤æ˜“æ—¶é—´æ®µé€šè¿‡ Cron Job è‡ªåŠ¨åˆ·æ–°ç¼“å­˜ï¼Œé¿å…æ‰«ææ—¶è¶…æ—¶")
        
        # æ£€æµ‹ Vercel ç¯å¢ƒï¼Œåœ¨ Vercel ä¸­ä½¿ç”¨æ›´çŸ­çš„è¶…æ—¶å’Œæ›´å°‘çš„é‡è¯•
        is_vercel = (
            os.environ.get('VERCEL') == '1' or 
            os.environ.get('VERCEL_ENV') is not None or
            os.environ.get('VERCEL_URL') is not None
        )
        
        # Vercel ç¯å¢ƒä¸­ï¼Œserverless å‡½æ•°æœ‰ 10 ç§’é™åˆ¶ï¼Œä½¿ç”¨æ›´çŸ­çš„è¶…æ—¶
        # è€ƒè™‘åˆ°éœ€è¦ç•™å‡ºæ—¶é—´ç»™å…¶ä»–ä»£ç æ‰§è¡Œï¼Œå®é™…è¶…æ—¶åº”è¯¥æ›´çŸ­
        if is_vercel:
            timeout = min(timeout, 5)  # Vercel ä¸­æœ€å¤š5ç§’ï¼Œç•™å‡º5ç§’ç»™å…¶ä»–å¤„ç†
            max_retries = 1  # Vercel ä¸­åªå°è¯•1æ¬¡ï¼Œé¿å…è¶…è¿‡æ‰§è¡Œæ—¶é—´é™åˆ¶
            print(f"[get_all_stocks] Vercel ç¯å¢ƒæ£€æµ‹åˆ°ï¼Œä½¿ç”¨è¶…çŸ­è¶…æ—¶æ—¶é—´: {timeout}ç§’ï¼Œåªå°è¯• {max_retries} æ¬¡ï¼ˆé¿å…è¶…è¿‡10ç§’é™åˆ¶ï¼‰")
            print(f"[get_all_stocks] âš ï¸ å¦‚æœç¼“å­˜ä¸å­˜åœ¨ï¼Œå¯èƒ½ä¼šå› ä¸º akshare API å“åº”æ…¢è€Œå¯¼è‡´è¶…æ—¶")
        else:
            max_retries = min(max_retries, 3)  # æœ¬åœ°ç¯å¢ƒä¸­æœ€å¤šé‡è¯•3æ¬¡
            print(f"[get_all_stocks] æœ¬åœ°ç¯å¢ƒï¼Œè¶…æ—¶æ—¶é—´: {timeout}ç§’ï¼Œæœ€å¤šé‡è¯• {max_retries} æ¬¡")
        
        for attempt in range(max_retries):
            try:
                print(f"[get_all_stocks] å°è¯•è·å–è‚¡ç¥¨åˆ—è¡¨ï¼ˆç¬¬ {attempt + 1}/{max_retries} æ¬¡ï¼Œè¶…æ—¶: {timeout}ç§’ï¼‰...")
                
                # ä½¿ç”¨çº¿ç¨‹å’Œè¶…æ—¶æœºåˆ¶
                result = [None]
                error = [None]
                start_time = time.time()
                
                def fetch_stocks():
                    try:
                        # åœ¨ Vercel ç¯å¢ƒä¸­ï¼Œå°è¯•ä½¿ç”¨æ›´å¿«çš„æ¥å£æˆ–æ·»åŠ é¢å¤–é”™è¯¯å¤„ç†
                        if is_vercel:
                            try:
                                print(f"[get_all_stocks] Vercel ç¯å¢ƒï¼šå¼€å§‹è°ƒç”¨ ak.stock_info_a_code_name()...")
                                result[0] = ak.stock_info_a_code_name()
                                elapsed = time.time() - start_time
                                print(f"[get_all_stocks] Vercel ç¯å¢ƒï¼šak.stock_info_a_code_name() è°ƒç”¨æˆåŠŸï¼Œè€—æ—¶ {elapsed:.2f}ç§’")
                            except Exception as e:
                                error[0] = e
                                elapsed = time.time() - start_time
                                print(f"[get_all_stocks] âŒ Vercel ç¯å¢ƒä¸­è·å–å¤±è´¥ï¼ˆè€—æ—¶ {elapsed:.2f}ç§’ï¼‰: {e}")
                                # åœ¨ Vercel ä¸­ï¼Œä¸æ‰“å°å®Œæ•´å †æ ˆï¼Œé¿å…æ—¥å¿—è¿‡é•¿
                        else:
                            result[0] = ak.stock_info_a_code_name()
                    except Exception as e:
                        error[0] = e
                        import traceback
                        elapsed = time.time() - start_time
                        print(f"[get_all_stocks] âŒ è·å–å¤±è´¥ï¼ˆè€—æ—¶ {elapsed:.2f}ç§’ï¼‰: {e}")
                        if not is_vercel:
                            print(f"[get_all_stocks] é”™è¯¯å †æ ˆ: {traceback.format_exc()}")
                
                fetch_thread = threading.Thread(target=fetch_stocks)
                fetch_thread.daemon = True
                fetch_thread.start()
                fetch_thread.join(timeout=timeout)
                
                elapsed_total = time.time() - start_time
                
                if fetch_thread.is_alive():
                    print(f"[get_all_stocks] â±ï¸ è·å–è¶…æ—¶ï¼ˆ>{timeout}ç§’ï¼Œå®é™…è€—æ—¶ {elapsed_total:.2f}ç§’ï¼‰")
                    if is_vercel:
                        # åœ¨ Vercel ä¸­ï¼Œè¶…æ—¶ç›´æ¥è¿”å› Noneï¼Œä¸é‡è¯•
                        print(f"[get_all_stocks] Vercel ç¯å¢ƒä¸­è¶…æ—¶ï¼Œç›´æ¥è¿”å› Noneï¼ˆé¿å…è¶…è¿‡10ç§’æ‰§è¡Œæ—¶é—´é™åˆ¶ï¼‰")
                        # å¦‚æœæœ‰è¿‡æœŸç¼“å­˜ï¼Œä½¿ç”¨è¿‡æœŸç¼“å­˜ä½œä¸ºå›é€€æ–¹æ¡ˆ
                        if expired_cache is not None and len(expired_cache) > 0:
                            print(f"[get_all_stocks] âš ï¸ API è·å–å¤±è´¥ï¼Œå›é€€åˆ°ä½¿ç”¨è¿‡æœŸç¼“å­˜ï¼ˆ{len(expired_cache)} åªè‚¡ç¥¨ï¼‰")
                            self.stock_list = expired_cache
                            return expired_cache
                        return None
                    if attempt < max_retries - 1:
                        # ä¸åœ¨ Vercel ä¸­æ—¶ï¼Œç­‰å¾…åé‡è¯•
                        print(f"[get_all_stocks] ç­‰å¾… 2 ç§’åé‡è¯•...")
                        time.sleep(2)
                        continue  # é‡è¯•
                    else:
                        print(f"[get_all_stocks] âŒ æ‰€æœ‰é‡è¯•éƒ½è¶…æ—¶")
                        # å¦‚æœæœ‰è¿‡æœŸç¼“å­˜ï¼Œä½¿ç”¨è¿‡æœŸç¼“å­˜ä½œä¸ºå›é€€æ–¹æ¡ˆ
                        if expired_cache is not None and len(expired_cache) > 0:
                            print(f"[get_all_stocks] âš ï¸ API è·å–å¤±è´¥ï¼Œå›é€€åˆ°ä½¿ç”¨è¿‡æœŸç¼“å­˜ï¼ˆ{len(expired_cache)} åªè‚¡ç¥¨ï¼‰")
                            self.stock_list = expired_cache
                            return expired_cache
                        return None
                
                if error[0]:
                    print(f"[get_all_stocks] âŒ è·å–å‡ºé”™ï¼ˆè€—æ—¶ {elapsed_total:.2f}ç§’ï¼‰: {error[0]}")
                    if is_vercel:
                        # åœ¨ Vercel ä¸­ï¼Œå¦‚æœå‡ºé”™ï¼Œç›´æ¥è¿”å› Noneï¼Œä¸é‡è¯•
                        print(f"[get_all_stocks] Vercel ç¯å¢ƒä¸­è·å–å‡ºé”™ï¼Œç›´æ¥è¿”å› Noneï¼ˆé¿å…è¶…è¿‡10ç§’æ‰§è¡Œæ—¶é—´é™åˆ¶ï¼‰")
                        # å¦‚æœæœ‰è¿‡æœŸç¼“å­˜ï¼Œä½¿ç”¨è¿‡æœŸç¼“å­˜ä½œä¸ºå›é€€æ–¹æ¡ˆ
                        if expired_cache is not None and len(expired_cache) > 0:
                            print(f"[get_all_stocks] âš ï¸ API è·å–å¤±è´¥ï¼Œå›é€€åˆ°ä½¿ç”¨è¿‡æœŸç¼“å­˜ï¼ˆ{len(expired_cache)} åªè‚¡ç¥¨ï¼‰")
                            self.stock_list = expired_cache
                            return expired_cache
                        return None
                    if attempt < max_retries - 1:
                        print(f"[get_all_stocks] ç­‰å¾… 2 ç§’åé‡è¯•...")
                        time.sleep(2)
                        continue  # é‡è¯•
                    else:
                        # å¦‚æœæœ‰è¿‡æœŸç¼“å­˜ï¼Œä½¿ç”¨è¿‡æœŸç¼“å­˜ä½œä¸ºå›é€€æ–¹æ¡ˆ
                        if expired_cache is not None and len(expired_cache) > 0:
                            print(f"[get_all_stocks] âš ï¸ API è·å–å¤±è´¥ï¼Œå›é€€åˆ°ä½¿ç”¨è¿‡æœŸç¼“å­˜ï¼ˆ{len(expired_cache)} åªè‚¡ç¥¨ï¼‰")
                            self.stock_list = expired_cache
                            return expired_cache
                        raise error[0]
                
                if result[0] is not None and len(result[0]) > 0:
                    stock_info = result[0]
                    self.stock_list = stock_info
                    elapsed_total = time.time() - start_time
                    print(f"[get_all_stocks] âœ… æˆåŠŸè·å– {len(stock_info)} åªAè‚¡è‚¡ç¥¨ï¼ˆè€—æ—¶ {elapsed_total:.2f}ç§’ï¼‰")
                    
                    # å°†è·å–çš„è‚¡ç¥¨åˆ—è¡¨ä¿å­˜åˆ°ç¼“å­˜ï¼ˆå¼‚æ­¥ä¿å­˜ï¼Œä¸é˜»å¡ï¼‰
                    try:
                        import threading
                        def save_cache():
                            try:
                                self._save_stock_list_to_cache(stock_info)
                            except Exception as e:
                                print(f"[get_all_stocks] âš ï¸ åå°ä¿å­˜ç¼“å­˜å¤±è´¥ï¼ˆä¸å½±å“ä½¿ç”¨ï¼‰: {e}")
                        
                        cache_thread = threading.Thread(target=save_cache)
                        cache_thread.daemon = True
                        cache_thread.start()
                        # ä¸ç­‰å¾…ç¼“å­˜ä¿å­˜å®Œæˆï¼Œç«‹å³è¿”å›ç»“æœ
                        print(f"[get_all_stocks] å·²å¯åŠ¨åå°çº¿ç¨‹ä¿å­˜è‚¡ç¥¨åˆ—è¡¨åˆ°ç¼“å­˜...")
                    except Exception as e:
                        print(f"[get_all_stocks] âš ï¸ å¯åŠ¨ç¼“å­˜ä¿å­˜çº¿ç¨‹å¤±è´¥ï¼ˆä¸å½±å“ä½¿ç”¨ï¼‰: {e}")
                    
                    return stock_info
                else:
                    print(f"[get_all_stocks] âš ï¸ è¿”å›ç»“æœä¸ºç©ºï¼ˆè€—æ—¶ {elapsed_total:.2f}ç§’ï¼‰")
                    if is_vercel:
                        # åœ¨ Vercel ä¸­ï¼Œå¦‚æœç»“æœä¸ºç©ºï¼Œç›´æ¥è¿”å› Noneï¼Œä¸é‡è¯•
                        print(f"[get_all_stocks] Vercel ç¯å¢ƒä¸­ç»“æœä¸ºç©ºï¼Œç›´æ¥è¿”å› None")
                        # å¦‚æœæœ‰è¿‡æœŸç¼“å­˜ï¼Œä½¿ç”¨è¿‡æœŸç¼“å­˜ä½œä¸ºå›é€€æ–¹æ¡ˆ
                        if expired_cache is not None and len(expired_cache) > 0:
                            print(f"[get_all_stocks] âš ï¸ API è¿”å›ä¸ºç©ºï¼Œå›é€€åˆ°ä½¿ç”¨è¿‡æœŸç¼“å­˜ï¼ˆ{len(expired_cache)} åªè‚¡ç¥¨ï¼‰")
                            self.stock_list = expired_cache
                            return expired_cache
                        return None
                    if attempt < max_retries - 1:
                        print(f"[get_all_stocks] ç­‰å¾… 2 ç§’åé‡è¯•...")
                        time.sleep(2)
                        continue  # é‡è¯•
                    else:
                        print(f"[get_all_stocks] âŒ æ‰€æœ‰é‡è¯•éƒ½å¤±è´¥ï¼Œè¿”å› None")
                        # å¦‚æœæœ‰è¿‡æœŸç¼“å­˜ï¼Œä½¿ç”¨è¿‡æœŸç¼“å­˜ä½œä¸ºå›é€€æ–¹æ¡ˆ
                        if expired_cache is not None and len(expired_cache) > 0:
                            print(f"[get_all_stocks] âš ï¸ æ‰€æœ‰é‡è¯•éƒ½å¤±è´¥ï¼Œå›é€€åˆ°ä½¿ç”¨è¿‡æœŸç¼“å­˜ï¼ˆ{len(expired_cache)} åªè‚¡ç¥¨ï¼‰")
                            self.stock_list = expired_cache
                            return expired_cache
                        return None
                        
            except Exception as e:
                import traceback
                error_detail = traceback.format_exc()
                elapsed_total = time.time() - start_time if 'start_time' in locals() else 0
                print(f"[get_all_stocks] âŒ è·å–è‚¡ç¥¨åˆ—è¡¨å¤±è´¥ï¼ˆç¬¬ {attempt + 1} æ¬¡å°è¯•ï¼Œè€—æ—¶ {elapsed_total:.2f}ç§’ï¼‰: {e}")
                if not is_vercel:
                    print(f"[get_all_stocks] é”™è¯¯è¯¦æƒ…: {error_detail}")
                
                if is_vercel:
                    # åœ¨ Vercel ä¸­ï¼Œå¦‚æœå‡ºé”™ï¼Œç›´æ¥è¿”å› Noneï¼Œä¸é‡è¯•
                    print(f"[get_all_stocks] Vercel ç¯å¢ƒä¸­å‡ºé”™ï¼Œç›´æ¥è¿”å› Noneï¼ˆé¿å…è¶…è¿‡10ç§’æ‰§è¡Œæ—¶é—´é™åˆ¶ï¼‰")
                    # å¦‚æœæœ‰è¿‡æœŸç¼“å­˜ï¼Œä½¿ç”¨è¿‡æœŸç¼“å­˜ä½œä¸ºå›é€€æ–¹æ¡ˆ
                    if expired_cache is not None and len(expired_cache) > 0:
                        print(f"[get_all_stocks] âš ï¸ å¼‚å¸¸å¤„ç†ï¼šå›é€€åˆ°ä½¿ç”¨è¿‡æœŸç¼“å­˜ï¼ˆ{len(expired_cache)} åªè‚¡ç¥¨ï¼‰")
                        self.stock_list = expired_cache
                        return expired_cache
                    return None
                
                if attempt < max_retries - 1:
                    print(f"[get_all_stocks] ç­‰å¾… 2 ç§’åé‡è¯•...")
                    time.sleep(2)
                    continue  # é‡è¯•
                else:
                    print(f"[get_all_stocks] âŒ æ‰€æœ‰é‡è¯•éƒ½å¤±è´¥")
                    # å¦‚æœæœ‰è¿‡æœŸç¼“å­˜ï¼Œä½¿ç”¨è¿‡æœŸç¼“å­˜ä½œä¸ºå›é€€æ–¹æ¡ˆ
                    if expired_cache is not None and len(expired_cache) > 0:
                        print(f"[get_all_stocks] âš ï¸ æ‰€æœ‰é‡è¯•éƒ½å¤±è´¥ï¼Œå›é€€åˆ°ä½¿ç”¨è¿‡æœŸç¼“å­˜ï¼ˆ{len(expired_cache)} åªè‚¡ç¥¨ï¼‰")
                        self.stock_list = expired_cache
                        return expired_cache
                    return None
        
        print(f"[get_all_stocks] âŒ æ‰€æœ‰é‡è¯•éƒ½å¤±è´¥ï¼Œè¿”å› None")
        # å¦‚æœæœ‰è¿‡æœŸç¼“å­˜ï¼Œä½¿ç”¨è¿‡æœŸç¼“å­˜ä½œä¸ºå›é€€æ–¹æ¡ˆ
        if expired_cache is not None and len(expired_cache) > 0:
            print(f"[get_all_stocks] âš ï¸ æ‰€æœ‰é‡è¯•éƒ½å¤±è´¥ï¼Œå›é€€åˆ°ä½¿ç”¨è¿‡æœŸç¼“å­˜ï¼ˆ{len(expired_cache)} åªè‚¡ç¥¨ï¼‰")
            self.stock_list = expired_cache
            return expired_cache
        return None
    
    def get_circulating_shares(self, stock_code, timeout=5):
        """
        è·å–è‚¡ç¥¨æµé€šè‚¡æœ¬ï¼ˆå•ä½ï¼šä¸‡è‚¡ï¼‰
        :param stock_code: è‚¡ç¥¨ä»£ç ï¼ˆå¦‚ '000001'ï¼‰
        :param timeout: è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰ï¼Œé»˜è®¤5ç§’
        :return: æµé€šè‚¡æœ¬ï¼ˆä¸‡è‚¡ï¼‰ï¼Œå¦‚æœè·å–å¤±è´¥è¿”å›None
        """
        # âœ… å¦‚æœè®¾ç½®äº† USE_GITHUB_DATA_ONLYï¼Œä¸è¿æ¥å®æ—¶ API
        if self._use_github_data_only:
            print(f"[get_circulating_shares] âš ï¸  USE_GITHUB_DATA_ONLY æ¨¡å¼ï¼šä¸è¿æ¥å®æ—¶ APIï¼Œè¿”å› None")
            return None
        
        try:
            import threading
            import time
            
            # ä½¿ç”¨ç¼“å­˜ï¼Œé¿å…é‡å¤è·å–å…¨éƒ¨è‚¡ç¥¨æ•°æ®
            if self._market_cap_cache is None:
                # ä½¿ç”¨å®æ—¶è¡Œæƒ…æ¥å£ï¼ˆæ‰¹é‡è·å–ï¼‰- è¿™ä¸ªæ“ä½œå¯èƒ½å¾ˆæ…¢ï¼Œä½¿ç”¨è¶…æ—¶ä¿æŠ¤
                result = [None]
                error = [None]
                
                def fetch_all_stocks():
                    try:
                        result[0] = ak.stock_zh_a_spot_em()
                    except Exception as e:
                        error[0] = e
                
                # å¦‚æœç¼“å­˜ä¸ºç©ºï¼Œéœ€è¦è·å–å…¨éƒ¨è‚¡ç¥¨æ•°æ®ï¼ˆå¯èƒ½å¾ˆæ…¢ï¼‰
                fetch_thread = threading.Thread(target=fetch_all_stocks)
                fetch_thread.daemon = True
                fetch_thread.start()
                fetch_thread.join(timeout=timeout)
                
                if fetch_thread.is_alive():
                    # è¶…æ—¶äº†ï¼Œè¿”å›Noneï¼Œä¸é˜»å¡
                    return None
                
                if error[0]:
                    return None
                
                df = result[0]
                if df is not None and not df.empty:
                    self._market_cap_cache = df
                else:
                    return None
            else:
                df = self._market_cap_cache
            
            if df is not None and not df.empty:
                # æŸ¥æ‰¾å¯¹åº”è‚¡ç¥¨ï¼ˆä»£ç åˆ—æ˜¯å­—ç¬¦ä¸²ç±»å‹ï¼‰
                stock_code_str = str(stock_code)
                stock_row = df[df['ä»£ç '] == stock_code_str]
                
                if not stock_row.empty:
                    # å°è¯•ä»å¤šä¸ªå¯èƒ½çš„åˆ—åè·å–æµé€šè‚¡æœ¬
                    circulating_shares = None
                    for col in ['æµé€šè‚¡', 'æµé€šè‚¡æœ¬', 'æµé€šå¸‚å€¼']:
                        if col in stock_row.columns:
                            shares_str = str(stock_row.iloc[0][col])
                            if pd.notna(shares_str) and shares_str not in ['nan', 'None', '']:
                                try:
                                    # ä¿å­˜åŸå§‹å€¼ï¼Œç”¨äºåˆ¤æ–­å•ä½
                                    original_value = str(stock_row.iloc[0][col])
                                    # å¤„ç†"ä¸‡"å•ä½å’Œé€—å·
                                    shares_str = shares_str.replace(',', '').replace('ä¸‡', '')
                                    circulating_shares = float(shares_str)
                                    
                                    # å¦‚æœåŸå§‹å€¼åŒ…å«"ä¸‡"ï¼Œè¯´æ˜å•ä½å·²ç»æ˜¯ä¸‡è‚¡ï¼Œç›´æ¥è¿”å›
                                    # å¦‚æœä¸åŒ…å«"ä¸‡"ï¼Œè¯´æ˜å•ä½æ˜¯è‚¡ï¼Œéœ€è¦è½¬æ¢ä¸ºä¸‡è‚¡
                                    if 'ä¸‡' not in original_value:
                                        circulating_shares = circulating_shares / 10000  # è‚¡è½¬æ¢ä¸ºä¸‡è‚¡
                                    
                                    return circulating_shares
                                except (ValueError, TypeError):
                                    continue
                    
                    # å¦‚æœæ²¡æ‰¾åˆ°æµé€šè‚¡æœ¬ï¼Œä½†æ‰¾åˆ°äº†æµé€šå¸‚å€¼ï¼Œå¯ä»¥å°è¯•ç”¨å½“å‰ä»·æ ¼åæ¨
                    # ä½†è¿™ä¸ªæ–¹æ³•éœ€è¦å½“å‰ä»·æ ¼ï¼Œæ‰€ä»¥è¿™é‡Œä¸å®ç°
                    return None
            
            return None
        except Exception as e:
            # é™é»˜å¤±è´¥ï¼Œè¿”å›None
            return None
    
    def calculate_circulating_market_cap(self, stock_code, current_price, timeout=5):
        """
        è®¡ç®—è‚¡ç¥¨æµé€šå¸‚å€¼ï¼ˆæµé€šè‚¡æœ¬ * å½“å‰è‚¡ä»·ï¼‰ï¼ˆå•ä½ï¼šäº¿å…ƒï¼‰
        :param stock_code: è‚¡ç¥¨ä»£ç ï¼ˆå¦‚ '000001'ï¼‰
        :param current_price: å½“å‰è‚¡ä»·
        :param timeout: è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰ï¼Œé»˜è®¤5ç§’
        :return: æµé€šå¸‚å€¼ï¼ˆäº¿å…ƒï¼‰ï¼Œå¦‚æœè·å–å¤±è´¥è¿”å›None
        """
        # âœ… å¦‚æœè®¾ç½®äº† USE_GITHUB_DATA_ONLYï¼Œä¸è¿æ¥å®æ—¶ API
        if self._use_github_data_only:
            print(f"[calculate_circulating_market_cap] âš ï¸  USE_GITHUB_DATA_ONLY æ¨¡å¼ï¼šä¸è¿æ¥å®æ—¶ APIï¼Œè¿”å› None")
            return None
        
        try:
            import threading
            import time
            
            # ä½¿ç”¨ç¼“å­˜ï¼Œé¿å…é‡å¤è·å–å…¨éƒ¨è‚¡ç¥¨æ•°æ®
            if self._market_cap_cache is None:
                # ä½¿ç”¨å®æ—¶è¡Œæƒ…æ¥å£ï¼ˆæ‰¹é‡è·å–ï¼‰- è¿™ä¸ªæ“ä½œå¯èƒ½å¾ˆæ…¢ï¼Œä½¿ç”¨è¶…æ—¶ä¿æŠ¤
                result = [None]
                error = [None]
                
                def fetch_all_stocks():
                    try:
                        result[0] = ak.stock_zh_a_spot_em()
                    except Exception as e:
                        error[0] = e
                
                # å¦‚æœç¼“å­˜ä¸ºç©ºï¼Œéœ€è¦è·å–å…¨éƒ¨è‚¡ç¥¨æ•°æ®ï¼ˆå¯èƒ½å¾ˆæ…¢ï¼‰
                fetch_thread = threading.Thread(target=fetch_all_stocks)
                fetch_thread.daemon = True
                fetch_thread.start()
                fetch_thread.join(timeout=timeout)
                
                if fetch_thread.is_alive():
                    # è¶…æ—¶äº†ï¼Œè¿”å›Noneï¼Œä¸é˜»å¡
                    return None
                
                if error[0]:
                    return None
                
                df = result[0]
                if df is not None and not df.empty:
                    self._market_cap_cache = df
                else:
                    return None
            else:
                df = self._market_cap_cache
            
            if df is not None and not df.empty:
                # æŸ¥æ‰¾å¯¹åº”è‚¡ç¥¨ï¼ˆä»£ç åˆ—æ˜¯å­—ç¬¦ä¸²ç±»å‹ï¼‰
                # ç¡®ä¿stock_codeæ˜¯å­—ç¬¦ä¸²æ ¼å¼
                stock_code_str = str(stock_code)
                stock_row = df[df['ä»£ç '] == stock_code_str]
                
                if not stock_row.empty:
                    # ä¼˜å…ˆä½¿ç”¨æµé€šå¸‚å€¼
                    if 'æµé€šå¸‚å€¼' in stock_row.columns:
                        market_cap = stock_row.iloc[0]['æµé€šå¸‚å€¼']
                        if pd.notna(market_cap):
                            try:
                                market_cap = float(market_cap)
                                # æµé€šå¸‚å€¼å•ä½æ˜¯å…ƒï¼Œè½¬æ¢ä¸ºäº¿å…ƒ
                                return market_cap / 100000000
                            except (ValueError, TypeError):
                                pass
                    
                    # å¦‚æœæµé€šå¸‚å€¼ä¸å­˜åœ¨ï¼Œå°è¯•ä½¿ç”¨æµé€šè‚¡æœ¬è®¡ç®—
                    # ä¼˜å…ˆä½¿ç”¨æ–°æ–¹æ³•è·å–æµé€šè‚¡æœ¬
                    circulating_shares = self.get_circulating_shares(stock_code, timeout=1)  # ä½¿ç”¨çŸ­è¶…æ—¶ï¼Œå› ä¸ºç¼“å­˜å·²å­˜åœ¨
                    
                    # å¦‚æœæ–°æ–¹æ³•å¤±è´¥ï¼Œå°è¯•ä»å½“å‰æ•°æ®ä¸­ç›´æ¥è·å–
                    if circulating_shares is None:
                        for col in ['æµé€šè‚¡', 'æµé€šè‚¡æœ¬']:
                            if col in stock_row.columns:
                                shares_str = str(stock_row.iloc[0][col])
                                if pd.notna(shares_str) and shares_str not in ['nan', 'None', '']:
                                    try:
                                        # ä¿å­˜åŸå§‹å€¼ï¼Œç”¨äºåˆ¤æ–­å•ä½
                                        original_value = str(stock_row.iloc[0][col])
                                        # å¤„ç†"ä¸‡"å•ä½å’Œé€—å·
                                        shares_str = shares_str.replace(',', '').replace('ä¸‡', '')
                                        circulating_shares = float(shares_str)
                                        
                                        # å¦‚æœåŸå§‹å€¼ä¸åŒ…å«"ä¸‡"ï¼Œè¯´æ˜å•ä½æ˜¯è‚¡ï¼Œéœ€è¦è½¬æ¢ä¸ºä¸‡è‚¡
                                        if 'ä¸‡' not in original_value:
                                            circulating_shares = circulating_shares / 10000  # è‚¡è½¬æ¢ä¸ºä¸‡è‚¡
                                        
                                        break
                                    except (ValueError, TypeError):
                                        continue
                    
                    # å¦‚æœæ‰¾åˆ°æµé€šè‚¡æœ¬ï¼ˆå•ä½ï¼šä¸‡è‚¡ï¼‰ï¼Œç”¨å½“å‰è‚¡ä»·è®¡ç®—æµé€šå¸‚å€¼ï¼ˆå•ä½ï¼šäº¿å…ƒï¼‰
                    # æµé€šå¸‚å€¼ = æµé€šè‚¡æœ¬ï¼ˆä¸‡è‚¡ï¼‰ * å½“å‰è‚¡ä»·ï¼ˆå…ƒ/è‚¡ï¼‰ / 10000ï¼ˆä¸‡å…ƒè½¬äº¿å…ƒï¼‰
                    if circulating_shares is not None and current_price:
                        market_cap = (circulating_shares * current_price) / 10000  # ä¸‡è‚¡ * å…ƒ/è‚¡ / 10000 = äº¿å…ƒ
                        return market_cap
            
            return None
        except Exception as e:
            # é™é»˜å¤±è´¥ï¼Œè¿”å›Noneï¼ˆå¯ä»¥å–æ¶ˆæ³¨é‡Šä¸‹é¢çš„è¡Œæ¥è°ƒè¯•ï¼‰
            # print(f"è®¡ç®—æµé€šå¸‚å€¼å¤±è´¥ {stock_code}: {e}")
            return None
    
    def get_market_cap(self, stock_code, timeout=5):
        """
        è·å–è‚¡ç¥¨æ€»å¸‚å€¼ï¼ˆå•ä½ï¼šäº¿å…ƒï¼‰
        :param stock_code: è‚¡ç¥¨ä»£ç ï¼ˆå¦‚ '000001'ï¼‰
        :param timeout: è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰ï¼Œé»˜è®¤5ç§’
        :return: æ€»å¸‚å€¼ï¼ˆäº¿å…ƒï¼‰ï¼Œå¦‚æœè·å–å¤±è´¥è¿”å›None
        """
        # âœ… å¦‚æœè®¾ç½®äº† USE_GITHUB_DATA_ONLYï¼Œä¸è¿æ¥å®æ—¶ API
        if self._use_github_data_only:
            print(f"[get_market_cap] âš ï¸  USE_GITHUB_DATA_ONLY æ¨¡å¼ï¼šä¸è¿æ¥å®æ—¶ APIï¼Œè¿”å› None")
            return None
        
        try:
            import threading
            import time
            
            # ä½¿ç”¨ç¼“å­˜ï¼Œé¿å…é‡å¤è·å–å…¨éƒ¨è‚¡ç¥¨æ•°æ®
            if self._market_cap_cache is None:
                # ä½¿ç”¨å®æ—¶è¡Œæƒ…æ¥å£ï¼ˆæ‰¹é‡è·å–ï¼‰- è¿™ä¸ªæ“ä½œå¯èƒ½å¾ˆæ…¢ï¼Œä½¿ç”¨è¶…æ—¶ä¿æŠ¤
                result = [None]
                error = [None]
                
                def fetch_all_stocks():
                    try:
                        result[0] = ak.stock_zh_a_spot_em()
                    except Exception as e:
                        error[0] = e
                
                # å¦‚æœç¼“å­˜ä¸ºç©ºï¼Œéœ€è¦è·å–å…¨éƒ¨è‚¡ç¥¨æ•°æ®ï¼ˆå¯èƒ½å¾ˆæ…¢ï¼‰
                fetch_thread = threading.Thread(target=fetch_all_stocks)
                fetch_thread.daemon = True
                fetch_thread.start()
                fetch_thread.join(timeout=timeout)
                
                if fetch_thread.is_alive():
                    # è¶…æ—¶äº†ï¼Œè¿”å›Noneï¼Œä¸é˜»å¡
                    return None
                
                if error[0]:
                    return None
                
                df = result[0]
                if df is not None and not df.empty:
                    self._market_cap_cache = df
                else:
                    return None
            else:
                df = self._market_cap_cache
            
            if df is not None and not df.empty:
                # æŸ¥æ‰¾å¯¹åº”è‚¡ç¥¨ï¼ˆä»£ç åˆ—æ˜¯å­—ç¬¦ä¸²ç±»å‹ï¼‰
                # ç¡®ä¿stock_codeæ˜¯å­—ç¬¦ä¸²æ ¼å¼
                stock_code_str = str(stock_code)
                stock_row = df[df['ä»£ç '] == stock_code_str]
                
                if not stock_row.empty:
                    if 'æ€»å¸‚å€¼' in stock_row.columns:
                        market_cap = stock_row.iloc[0]['æ€»å¸‚å€¼']
                        if pd.notna(market_cap):
                            try:
                                market_cap = float(market_cap)
                                # æ€»å¸‚å€¼å•ä½æ˜¯å…ƒï¼Œè½¬æ¢ä¸ºäº¿å…ƒ
                                return market_cap / 100000000
                            except (ValueError, TypeError):
                                pass
            
            return None
        except Exception as e:
            # é™é»˜å¤±è´¥ï¼Œè¿”å›Noneï¼ˆå¯ä»¥å–æ¶ˆæ³¨é‡Šä¸‹é¢çš„è¡Œæ¥è°ƒè¯•ï¼‰
            # print(f"è·å–å¸‚å€¼å¤±è´¥ {stock_code}: {e}")
            return None
    
    def get_daily_kline(self, stock_code, period="1y", use_cache=True, local_only=False):
        """
        è·å–æ—¥Kçº¿æ•°æ®
        :param stock_code: è‚¡ç¥¨ä»£ç ï¼ˆå¦‚ '000001'ï¼‰
        :param period: æ—¶é—´å‘¨æœŸï¼Œ'1y'è¡¨ç¤º1å¹´
        :param use_cache: æ˜¯å¦ä¼˜å…ˆä½¿ç”¨æœ¬åœ°ç¼“å­˜
        :param local_only: æ˜¯å¦ä»…ç”¨æœ¬åœ°ï¼ˆä¸ä»ç½‘ç»œè·å–ï¼‰ï¼›è‹¥ True ä¸”æœ¬åœ°æ— æ•°æ®åˆ™è¿”å› None
        :return: DataFrameï¼ŒåŒ…å«æ—¥æœŸã€å¼€ç›˜ã€æ”¶ç›˜ã€æœ€é«˜ã€æœ€ä½ã€æˆäº¤é‡ç­‰
        """
        # âœ… å¦‚æœè®¾ç½®äº† USE_GITHUB_DATA_ONLYï¼Œå¼ºåˆ¶ä½¿ç”¨æœ¬åœ°ç¼“å­˜
        if self._use_github_data_only:
            local_only = True
            use_cache = True
        if os.environ.get("TRAIN_LOCAL_ONLY") == "1":
            local_only = True
        if use_cache or local_only:
            cached = self._get_daily_kline_from_cache(stock_code)
            if cached is not None and len(cached) > 0:
                end_ts = datetime.now()
                start_ts = end_ts - timedelta(days=365 * 2)
                cached = cached.copy()
                cached["_dt"] = pd.to_datetime(cached["æ—¥æœŸ"], errors="coerce")
                cached = cached.dropna(subset=["_dt"])
                mask = (cached["_dt"] >= start_ts) & (cached["_dt"] <= end_ts)
                out = cached.loc[mask].drop(columns=["_dt"], errors="ignore").sort_values("æ—¥æœŸ").reset_index(drop=True)
                if len(out) > 0:
                    return out
            if local_only:
                return None
        try:
            end_date = datetime.now().strftime('%Y%m%d')
            start_date = (datetime.now() - timedelta(days=365 * 2)).strftime('%Y%m%d')
            
            df = None
            last_err = None
            for attempt in range(3):
                try:
                    df = ak.stock_zh_a_hist(
                        symbol=stock_code,
                        period="daily",
                        start_date=start_date,
                        end_date=end_date,
                        adjust="qfq",
                    )
                    last_err = None
                    break
                except Exception as e:
                    last_err = e
                    # è½»é‡é€€é¿
                    time.sleep(0.6 * (2 ** attempt))
            if df is None and last_err is not None:
                raise last_err
            
            if df is None or df.empty:
                return None
            
            # akshareè¿”å›çš„DataFrameåˆ—åé€šå¸¸æ˜¯ä¸­æ–‡ï¼Œç›´æ¥ä½¿ç”¨ä½ç½®ç´¢å¼•æ›´å¯é 
            # æ ‡å‡†åˆ—é¡ºåºï¼šæ—¥æœŸã€å¼€ç›˜ã€æ”¶ç›˜ã€æœ€é«˜ã€æœ€ä½ã€æˆäº¤é‡ã€æˆäº¤é¢ã€æŒ¯å¹…ã€æ¶¨è·Œå¹…ã€æ¶¨è·Œé¢ã€æ¢æ‰‹ç‡
            # ä½†å®é™…è¿”å›çš„åˆ—æ•°å¯èƒ½ä¸åŒï¼Œä½¿ç”¨ä½ç½®ç´¢å¼•è®¿é—®
            
            # å…ˆå°è¯•ä½¿ç”¨åˆ—åï¼ˆå¦‚æœakshareè¿”å›çš„æ˜¯æ ‡å‡†åˆ—åï¼‰
            # æ³¨æ„ï¼šå³ä½¿åˆ—æ•°ä¸è¶³6ï¼Œä¹Ÿè¦å°è¯•é‡å‘½å
            if len(df.columns) >= 5:  # è‡³å°‘éœ€è¦5åˆ—ï¼šæ—¥æœŸã€å¼€ç›˜ã€æ”¶ç›˜ã€æœ€é«˜ã€æœ€ä½
                # ä½¿ç”¨ä½ç½®ç´¢å¼•é‡å‘½åå…³é”®åˆ—
                # æ ¹æ®2025-12-31çš„æ­£ç¡®æ•°æ®æ ¸å¯¹ï¼š
                # æ­£ç¡®ï¼šå¼€ç›˜=4.66, æ”¶ç›˜=4.65, æœ€é«˜=4.68, æœ€ä½=4.62
                # akshareå®é™…è¿”å›çš„é¡ºåºå¯èƒ½æ˜¯ï¼šæ—¥æœŸã€å¼€ç›˜ã€æ”¶ç›˜ã€æœ€é«˜ã€æœ€ä½ã€æˆäº¤é‡
                # æˆ–è€…ï¼šæ—¥æœŸã€å…¶ä»–ã€å¼€ç›˜ã€æ”¶ç›˜ã€æœ€é«˜ã€æœ€ä½
                # éœ€è¦æ ¹æ®åˆ—åæˆ–æ•°æ®é€»è¾‘åˆ¤æ–­
                rename_dict = {}
                if len(df.columns) > 0:
                    rename_dict[df.columns[0]] = 'æ—¥æœŸ'
                
                # å°è¯•æ ¹æ®åˆ—ååˆ¤æ–­
                col_names = [str(col).lower() for col in df.columns]
                
                # æŸ¥æ‰¾åŒ…å«"å¼€ç›˜"ã€"æ”¶ç›˜"ã€"æœ€é«˜"ã€"æœ€ä½"çš„åˆ—
                open_idx = None
                close_idx = None
                high_idx = None
                low_idx = None
                
                for i, col_name in enumerate(col_names):
                    if 'å¼€ç›˜' in col_name or 'open' in col_name:
                        open_idx = i
                    elif 'æ”¶ç›˜' in col_name or 'close' in col_name:
                        close_idx = i
                    elif 'æœ€é«˜' in col_name or 'high' in col_name:
                        high_idx = i
                    elif 'æœ€ä½' in col_name or 'low' in col_name:
                        low_idx = i
                
                # å¦‚æœæ‰¾åˆ°äº†åˆ—åï¼Œä½¿ç”¨åˆ—åæ˜ å°„
                if open_idx and close_idx and high_idx and low_idx:
                    rename_dict[df.columns[open_idx]] = 'å¼€ç›˜'
                    rename_dict[df.columns[close_idx]] = 'æ”¶ç›˜'
                    rename_dict[df.columns[high_idx]] = 'æœ€é«˜'
                    rename_dict[df.columns[low_idx]] = 'æœ€ä½'
                else:
                    # å¦‚æœæ²¡æ‰¾åˆ°åˆ—åï¼Œæ ¹æ®2025-12-31çš„æ­£ç¡®æ•°æ®æ¨æ–­ï¼š
                    # æ­£ç¡®ï¼šå¼€ç›˜=4.66, æ”¶ç›˜=4.65, æœ€é«˜=4.68, æœ€ä½=4.62
                    # ä¹‹å‰é”™è¯¯æ˜¾ç¤ºï¼šåˆ—1=2.00, åˆ—2=4.66, åˆ—3=4.65, åˆ—4=4.68
                    # æ‰€ä»¥åˆ—é¡ºåºæ˜¯ï¼šæ—¥æœŸã€å…¶ä»–ã€å¼€ç›˜ã€æ”¶ç›˜ã€æœ€é«˜ã€æœ€ä½
                    # åˆ—1å¯èƒ½æ˜¯æ¶¨è·Œå¹…æˆ–å…¶ä»–æ•°æ®ï¼Œè·³è¿‡
                    if len(df.columns) > 2:
                        rename_dict[df.columns[2]] = 'å¼€ç›˜'  # åˆ—2æ˜¯å¼€ç›˜
                    if len(df.columns) > 3:
                        rename_dict[df.columns[3]] = 'æ”¶ç›˜'  # åˆ—3æ˜¯æ”¶ç›˜
                    if len(df.columns) > 4:
                        rename_dict[df.columns[4]] = 'æœ€é«˜'  # åˆ—4æ˜¯æœ€é«˜
                    if len(df.columns) > 5:
                        rename_dict[df.columns[5]] = 'æœ€ä½'  # åˆ—5æ˜¯æœ€ä½
                    
                    # å¦‚æœåˆ—1å­˜åœ¨ä½†ä¸æ˜¯å¼€ç›˜ï¼Œä¿ç•™åŸåˆ—åï¼ˆå¯èƒ½æ˜¯æ¶¨è·Œå¹…ç­‰ï¼‰
                    if len(df.columns) > 1 and df.columns[1] not in rename_dict:
                        # ä¸é‡å‘½ååˆ—1ï¼Œä¿æŒåŸæ ·
                        pass
                    
                    if len(df.columns) > 6:
                        rename_dict[df.columns[6]] = 'æˆäº¤é‡'  # åˆ—6æ˜¯æˆäº¤é‡
                
                # æ‰§è¡Œé‡å‘½å
                if rename_dict:
                    df = df.rename(columns=rename_dict)
                    print(f"[è°ƒè¯•] åˆ—é‡å‘½åå®Œæˆï¼Œæ–°åˆ—å: {list(df.columns)}")
                else:
                    print(f"[è­¦å‘Š] æœªæ‰§è¡Œåˆ—é‡å‘½åï¼ŒåŸå§‹åˆ—å: {list(df.columns)}")
            
            # ç¡®ä¿æ—¥æœŸåˆ—å­˜åœ¨ä¸”å¯è½¬æ¢
            if 'æ—¥æœŸ' in df.columns:
                df['æ—¥æœŸ'] = pd.to_datetime(df['æ—¥æœŸ'], errors='coerce')
            else:
                print(f"[é”™è¯¯] æœªæ‰¾åˆ°'æ—¥æœŸ'åˆ—ï¼Œå¯ç”¨åˆ—: {list(df.columns)}")
                return None
            
            # éªŒè¯å¿…è¦çš„åˆ—æ˜¯å¦å­˜åœ¨ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™ä½¿ç”¨ä½ç½®ç´¢å¼•åˆ›å»º
            required_cols = ['å¼€ç›˜', 'æ”¶ç›˜', 'æœ€é«˜', 'æœ€ä½']
            missing_cols = [col for col in required_cols if col not in df.columns]
            if missing_cols:
                # print(f"[è­¦å‘Š] ç¼ºå°‘å¿…è¦çš„åˆ—: {missing_cols}")
                # print(f"[è°ƒè¯•] å½“å‰åˆ—å: {list(df.columns)}")
                # print(f"[è°ƒè¯•] åˆ—æ•°: {len(df.columns)}")
                # å¦‚æœåˆ—æ•°è¶³å¤Ÿï¼Œå°è¯•ä½¿ç”¨ä½ç½®ç´¢å¼•åˆ›å»ºæ–°åˆ—
                # æ ¹æ®2025-12-31çš„æ­£ç¡®æ•°æ®ï¼šåˆ—é¡ºåºæ˜¯ æ—¥æœŸã€å…¶ä»–ã€å¼€ç›˜ã€æ”¶ç›˜ã€æœ€é«˜ã€æœ€ä½
                if len(df.columns) >= 6:
                    # print(f"[è°ƒè¯•] ä½¿ç”¨ä½ç½®ç´¢å¼•åˆ›å»ºç¼ºå¤±çš„åˆ—...")
                    if 'å¼€ç›˜' not in df.columns and len(df.columns) > 2:
                        df['å¼€ç›˜'] = df.iloc[:, 2]
                    if 'æ”¶ç›˜' not in df.columns and len(df.columns) > 3:
                        df['æ”¶ç›˜'] = df.iloc[:, 3]
                    if 'æœ€é«˜' not in df.columns and len(df.columns) > 4:
                        df['æœ€é«˜'] = df.iloc[:, 4]
                    if 'æœ€ä½' not in df.columns and len(df.columns) > 5:
                        df['æœ€ä½'] = df.iloc[:, 5]
                    # print(f"[è°ƒè¯•] åˆ›å»ºåçš„åˆ—å: {list(df.columns)}")
            
            # åˆ é™¤æ—¥æœŸä¸ºç©ºçš„è®°å½•
            df = df.dropna(subset=['æ—¥æœŸ'])
            
            if df.empty:
                return None
            
            df = df.sort_values('æ—¥æœŸ').reset_index(drop=True)
            
            return df
        except Exception as e:
            print(f"è·å– {stock_code} æ—¥Kçº¿æ•°æ®å¤±è´¥: {e}")
            return None
    
    def _get_weekly_kline_from_cache(self, stock_code, local_files_only=False):
        """
        ä»ç¼“å­˜ä¸­è·å–å‘¨Kçº¿æ•°æ®ï¼ˆæœ¬åœ°æ–‡ä»¶ä¼˜å…ˆï¼‰
        :param local_files_only: è‹¥ Trueï¼Œä»…è¯»æœ¬åœ° CSV/JSONï¼Œä¸è®¿é—® Redis/KVï¼ˆæ‰«æåŠ é€Ÿï¼‰
        """
        import os
        import json
        import pandas as pd
        
        paths = self._local_cache_paths()
        weekly_dir = paths["weekly_dir"]
        csv_path = os.path.join(weekly_dir, f"{stock_code}.csv")
        json_path = os.path.join(weekly_dir, f"{stock_code}.json")
        
        if os.path.exists(csv_path):
            df = pd.read_csv(csv_path)
            if df is not None and len(df) > 0:
                if 'æ—¥æœŸ' in df.columns:
                    df['æ—¥æœŸ'] = pd.to_datetime(df['æ—¥æœŸ'], errors='coerce')
                    df = df.dropna(subset=['æ—¥æœŸ'])
                    df = df.sort_values('æ—¥æœŸ').reset_index(drop=True)
                return df
        
        if os.path.exists(json_path):
            with open(json_path, "r", encoding="utf-8") as f:
                stock_data = json.load(f)
            if isinstance(stock_data, list) and len(stock_data) > 0:
                df = pd.DataFrame(stock_data)
                if 'æ—¥æœŸ' in df.columns:
                    df['æ—¥æœŸ'] = pd.to_datetime(df['æ—¥æœŸ'], errors='coerce')
                    df = df.dropna(subset=['æ—¥æœŸ'])
                    df = df.sort_values('æ—¥æœŸ').reset_index(drop=True)
                return df
        
        if local_files_only:
            return None
        
        try:
            redis_url = os.environ.get("UPSTASH_REDIS_REST_URL")
            redis_token = os.environ.get('UPSTASH_REDIS_REST_TOKEN')
            if redis_url and redis_token:
                import requests
                try:
                    key = f"stock_kline:{stock_code}"
                    response = requests.get(
                        f"{redis_url}/get/{key}",
                        headers={"Authorization": f"Bearer {redis_token}"},
                        timeout=2
                    )
                    if response.status_code == 200:
                        result = response.json()
                        value_str = result.get('result')
                        if value_str:
                            # è§£æ JSON å­—ç¬¦ä¸²ï¼ˆå¯èƒ½éœ€è¦è§£æä¸¤æ¬¡ï¼Œå¤„ç†åŒé‡ç¼–ç ï¼‰
                            stock_data = json.loads(value_str) if isinstance(value_str, str) else value_str
                            if isinstance(stock_data, str):
                                stock_data = json.loads(stock_data)
                            
                            if isinstance(stock_data, list) and len(stock_data) > 0:
                                import pandas as pd
                                df = pd.DataFrame(stock_data)
                                # ç¡®ä¿æ—¥æœŸåˆ—å­˜åœ¨ä¸”å¯è½¬æ¢
                                if 'æ—¥æœŸ' in df.columns:
                                    df['æ—¥æœŸ'] = pd.to_datetime(df['æ—¥æœŸ'], errors='coerce')
                                    df = df.dropna(subset=['æ—¥æœŸ'])
                                    df = df.sort_values('æ—¥æœŸ').reset_index(drop=True)
                                return df
                except Exception as e:
                    # é™é»˜å¤±è´¥ï¼Œç»§ç»­å°è¯•å…¶ä»–æ–¹å¼
                    pass
            
            # å°è¯•ä½¿ç”¨ Vercel KV
            try:
                from vercel_kv import kv
                key = f"stock_kline:{stock_code}"
                cached_data = kv.get(key)
                if cached_data:
                    stock_data = json.loads(cached_data) if isinstance(cached_data, str) else cached_data
                    if isinstance(stock_data, list) and len(stock_data) > 0:
                        import pandas as pd
                        df = pd.DataFrame(stock_data)
                        if 'æ—¥æœŸ' in df.columns:
                            df['æ—¥æœŸ'] = pd.to_datetime(df['æ—¥æœŸ'], errors='coerce')
                            df = df.dropna(subset=['æ—¥æœŸ'])
                            df = df.sort_values('æ—¥æœŸ').reset_index(drop=True)
                        return df
            except Exception:
                pass
            
            return None
        except Exception as e:
            return None
    
    def _get_daily_kline_from_cache(self, stock_code):
        """
        ä»æœ¬åœ°ç¼“å­˜è¯»å–æ—¥Kçº¿ï¼ˆcache/daily_kline/{code}.csvï¼‰
        :return: DataFrame æˆ– None
        """
        import os
        paths = self._local_cache_paths()
        csv_path = os.path.join(paths["daily_dir"], f"{stock_code}.csv")
        if not os.path.exists(csv_path):
            return None
        try:
            df = pd.read_csv(csv_path, encoding="utf-8-sig")
            if df is None or len(df) == 0:
                return None
            if "æ—¥æœŸ" not in df.columns:
                return None
            df["æ—¥æœŸ"] = pd.to_datetime(df["æ—¥æœŸ"], errors="coerce")
            df = df.dropna(subset=["æ—¥æœŸ"]).sort_values("æ—¥æœŸ").reset_index(drop=True)
            for col in ["å¼€ç›˜", "æ”¶ç›˜", "æœ€é«˜", "æœ€ä½"]:
                if col not in df.columns:
                    return None
            return df
        except Exception:
            return None
    
    def _save_weekly_kline_to_cache(self, stock_code, weekly_df, ttl=86400):
        """
        å°†å‘¨Kçº¿æ•°æ®ä¿å­˜åˆ°ç¼“å­˜ï¼ˆTTL: 24å°æ—¶ = 86400ç§’ï¼‰
        :param stock_code: è‚¡ç¥¨ä»£ç ï¼ˆå¦‚ '000001'ï¼‰
        :param weekly_df: å‘¨Kçº¿DataFrame
        :param ttl: ç¼“å­˜æ—¶é—´ï¼ˆç§’ï¼‰ï¼Œé»˜è®¤24å°æ—¶
        :return: boolï¼Œæ˜¯å¦ä¿å­˜æˆåŠŸ
        """
        try:
            import os
            import json
            
            if weekly_df is None or len(weekly_df) == 0:
                return False
            
            # å°† DataFrame è½¬æ¢ä¸º JSON æ ¼å¼ï¼ˆå­—å…¸åˆ—è¡¨ï¼‰
            try:
                stock_data = weekly_df.to_dict('records')
                stock_json = json.dumps(stock_data, default=str, ensure_ascii=False)
            except Exception as e:
                print(f"[_save_weekly_kline_to_cache] âš ï¸ è½¬æ¢ DataFrame åˆ° JSON å¤±è´¥: {e}")
                return False
            
            # å°è¯•ä½¿ç”¨ Upstash Redis
            redis_url = os.environ.get('UPSTASH_REDIS_REST_URL')
            redis_token = os.environ.get('UPSTASH_REDIS_REST_TOKEN')
            if redis_url and redis_token:
                import requests
                try:
                    key = f"stock_kline:{stock_code}"
                    response = requests.post(
                        f"{redis_url}/setex/{key}/{ttl}",
                        headers={
                            "Authorization": f"Bearer {redis_token}",
                            "Content-Type": "application/json"
                        },
                        json=stock_json,
                        timeout=5
                    )
                    if response.status_code == 200:
                        result = response.json()
                        if result.get('result') == 'OK' or result.get('result') is True:
                            return True
                except Exception as e:
                    # é™é»˜å¤±è´¥ï¼Œç»§ç»­å°è¯•å…¶ä»–æ–¹å¼
                    pass
            
            # å°è¯•ä½¿ç”¨ Vercel KV
            try:
                from vercel_kv import kv
                key = f"stock_kline:{stock_code}"
                kv.set(key, stock_json, ttl=ttl)
                return True
            except Exception:
                pass

            # âœ… æœ¬åœ°æ–‡ä»¶ç¼“å­˜å…œåº•
            try:
                import os
                from datetime import timezone
                paths = self._local_cache_paths()
                weekly_dir = paths["weekly_dir"]
                os.makedirs(weekly_dir, exist_ok=True)
                json_path = os.path.join(weekly_dir, f"{stock_code}.json")
                meta_path = os.path.join(weekly_dir, f"{stock_code}.meta.json")
                with open(json_path, "w", encoding="utf-8") as f:
                    f.write(stock_json)
                with open(meta_path, "w", encoding="utf-8") as f:
                    json.dump(
                        {"saved_at": datetime.now(timezone.utc).timestamp(), "ttl": ttl},
                        f,
                        ensure_ascii=False,
                    )
                return True
            except Exception:
                pass
            
            return False
        except Exception as e:
            return False

    # =========================
    # å¯æŒ‡å®šæ—¥æœŸåŒºé—´çš„æ•°æ®è·å–ï¼ˆç”¨äºæœ¬åœ°ç¦»çº¿ä¸‹è½½ 2024-2025ï¼‰
    # =========================
    def get_daily_kline_range(self, stock_code: str, start_date: str, end_date: str, adjust: str = "qfq", use_cache=True, local_only=False):
        """
        è·å–æ—¥Kçº¿æ•°æ®ï¼ˆæŒ‡å®šæ—¥æœŸåŒºé—´ï¼ŒYYYYMMDDï¼‰
        :param use_cache: æ˜¯å¦ä¼˜å…ˆä»æœ¬åœ°ç¼“å­˜è¯»å–
        :param local_only: æ˜¯å¦ä»…ç”¨æœ¬åœ°ï¼›è‹¥ True ä¸”æœ¬åœ°æ— æ•°æ®åˆ™è¿”å› None
        """
        # âœ… å¦‚æœè®¾ç½®äº† USE_GITHUB_DATA_ONLYï¼Œå¼ºåˆ¶ä½¿ç”¨æœ¬åœ°ç¼“å­˜
        if self._use_github_data_only:
            local_only = True
            use_cache = True
        if os.environ.get("TRAIN_LOCAL_ONLY") == "1":
            local_only = True
        if use_cache or local_only:
            cached = self._get_daily_kline_from_cache(stock_code)
            if cached is not None and len(cached) > 0:
                cached = cached.copy()
                cached["æ—¥æœŸ"] = pd.to_datetime(cached["æ—¥æœŸ"], errors="coerce")
                cached = cached.dropna(subset=["æ—¥æœŸ"])
                cached["_ymd"] = cached["æ—¥æœŸ"].dt.strftime("%Y%m%d")
                start_d = str(start_date).replace("-", "")[:8]
                end_d = str(end_date).replace("-", "")[:8]
                mask = (cached["_ymd"] >= start_d) & (cached["_ymd"] <= end_d)
                out = cached.loc[mask].drop(columns=["_ymd"], errors="ignore").sort_values("æ—¥æœŸ").reset_index(drop=True)
                if len(out) > 0:
                    return out
            if local_only:
                return None
        try:
            df = None
            last_err = None
            for attempt in range(5):
                try:
                    df = ak.stock_zh_a_hist(
                        symbol=str(stock_code),
                        period="daily",
                        start_date=str(start_date).replace("-", "")[:8],
                        end_date=str(end_date).replace("-", "")[:8],
                        adjust=adjust,
                    )
                    last_err = None
                    break
                except Exception as e:
                    last_err = e
                    time.sleep(0.8 * (2 ** attempt))
            if df is None and last_err is not None:
                raise last_err
            if df is None or df.empty:
                return None

            if len(df.columns) > 0:
                df = df.rename(columns={df.columns[0]: "æ—¥æœŸ"})
            if "æ—¥æœŸ" in df.columns:
                df["æ—¥æœŸ"] = pd.to_datetime(df["æ—¥æœŸ"], errors="coerce")
                df = df.dropna(subset=["æ—¥æœŸ"]).sort_values("æ—¥æœŸ").reset_index(drop=True)
            return df
        except Exception as e:
            return None

    def get_weekly_kline_range(self, stock_code: str, start_date: str, end_date: str, adjust: str = "qfq"):
        """
        è·å–å‘¨Kçº¿æ•°æ®ï¼ˆæŒ‡å®šæ—¥æœŸåŒºé—´ï¼ŒYYYYMMDDï¼‰
        """
        try:
            df = None
            last_err = None
            for attempt in range(5):
                try:
                    df = ak.stock_zh_a_hist(
                        symbol=str(stock_code),
                        period="weekly",
                        start_date=start_date,
                        end_date=end_date,
                        adjust=adjust,
                    )
                    last_err = None
                    break
                except Exception as e:
                    last_err = e
                    time.sleep(0.8 * (2 ** attempt))
            if df is None and last_err is not None:
                raise last_err
            if df is None or df.empty:
                return None

            # å°½é‡å¯¹é½ get_weekly_kline çš„æ¸…æ´—
            if len(df.columns) >= 1:
                df = df.rename(columns={df.columns[0]: "æ—¥æœŸ"})
            if "æ—¥æœŸ" in df.columns:
                df["æ—¥æœŸ"] = pd.to_datetime(df["æ—¥æœŸ"], errors="coerce")
                df = df.dropna(subset=["æ—¥æœŸ"]).sort_values("æ—¥æœŸ").reset_index(drop=True)
            # æˆäº¤é‡åˆ—åç»Ÿä¸€ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
            if "æˆäº¤é‡" in df.columns and "å‘¨æˆäº¤é‡" not in df.columns:
                df = df.rename(columns={"æˆäº¤é‡": "å‘¨æˆäº¤é‡"})
            return df
        except Exception:
            return None
    
    def get_weekly_kline(self, stock_code, period="1y", use_cache=True, local_only=False):
        """
        è·å–å‘¨Kçº¿æ•°æ®ï¼ˆåŒ…å«å‘¨æˆäº¤é‡ï¼‰
        :param stock_code: è‚¡ç¥¨ä»£ç ï¼ˆå¦‚ '000001'ï¼‰
        :param period: æ—¶é—´å‘¨æœŸï¼Œ'1y'è¡¨ç¤º1å¹´ï¼ˆå®é™…ä½¿ç”¨2å¹´ï¼‰
        :param use_cache: æ˜¯å¦ä½¿ç”¨ç¼“å­˜ï¼Œé»˜è®¤True
        :param local_only: æ˜¯å¦ä»…ä½¿ç”¨æœ¬åœ°æ•°æ®ï¼ˆä¸ä»ç½‘ç»œè·å–ï¼‰ï¼Œé»˜è®¤False
        :return: DataFrameï¼ŒåŒ…å«å‘¨æ—¥æœŸã€å¼€ç›˜ã€æ”¶ç›˜ã€æœ€é«˜ã€æœ€ä½ã€å‘¨æˆäº¤é‡ç­‰
        """
        # âœ… å¦‚æœè®¾ç½®äº† USE_GITHUB_DATA_ONLYï¼Œå¼ºåˆ¶ä½¿ç”¨æœ¬åœ°ç¼“å­˜
        if self._use_github_data_only:
            local_only = True
            use_cache = True
        if os.environ.get("TRAIN_LOCAL_ONLY") == "1":
            local_only = True
        if use_cache or local_only:
            cached_df = self._get_weekly_kline_from_cache(stock_code, local_files_only=local_only)
            if cached_df is not None and len(cached_df) > 0:
                # æ³¨é‡Šæ‰printè¾“å‡ºä»¥æé«˜æ€§èƒ½
                # print(f"[get_weekly_kline] âœ… ä»ç¼“å­˜è·å– {stock_code} çš„å‘¨Kçº¿æ•°æ®: {len(cached_df)} å‘¨")
                return cached_df
            
            # å¦‚æœæ˜¯ä»…æœ¬åœ°æ¨¡å¼ä¸”ç¼“å­˜ä¸å­˜åœ¨ï¼Œç›´æ¥è¿”å›Noneï¼ˆä¸å°è¯•ç½‘ç»œä¸‹è½½ï¼‰
            if local_only:
                # è®°å½•éœ€è¦ä¸‹è½½çš„è‚¡ç¥¨ï¼ˆç”¨äºåç»­æç¤ºï¼‰
                if not hasattr(self, '_missing_stocks'):
                    self._missing_stocks = set()
                self._missing_stocks.add(stock_code)
                return None
        
        # å¦‚æœæœ¬åœ°æ²¡æœ‰æ•°æ®ä¸”ä¸æ˜¯local_onlyæ¨¡å¼ï¼Œç»§ç»­å°è¯•ä»ç½‘ç»œè·å–
        # è®°å½•éœ€è¦ä¸‹è½½çš„è‚¡ç¥¨ï¼ˆç”¨äºåç»­æç¤ºï¼‰
        if not hasattr(self, '_missing_stocks'):
            self._missing_stocks = set()
        self._missing_stocks.add(stock_code)
        
        try:
            # æ³¨é‡Šæ‰printè¾“å‡ºä»¥æé«˜æ€§èƒ½
            # print(f"å¼€å§‹è·å– {stock_code} çš„å‘¨Kçº¿æ•°æ®...")
            # æ–¹æ³•1: å°è¯•ç›´æ¥ä½¿ç”¨akshareçš„å‘¨Kçº¿æ¥å£
            try:
                end_date = datetime.now().strftime('%Y%m%d')
                start_date = (datetime.now() - timedelta(days=365 * 2)).strftime('%Y%m%d')
                
                # æ³¨é‡Šæ‰printè¾“å‡ºä»¥æé«˜æ€§èƒ½
                # print(f"å°è¯•ç›´æ¥è·å–å‘¨Kçº¿: {stock_code}, {start_date} - {end_date}")
                # å¸¦é‡è¯•ï¼Œå‡å°‘ RemoteDisconnected ç­‰å¶å‘é”™è¯¯å¯¼è‡´çš„å¤±è´¥ç‡
                df = None
                last_err = None
                for attempt in range(3):
                    try:
                        df = ak.stock_zh_a_hist(
                            symbol=stock_code,
                            period="weekly",
                            start_date=start_date,
                            end_date=end_date,
                            adjust="qfq",
                        )
                        last_err = None
                        break
                    except Exception as e:
                        last_err = e
                        time.sleep(0.6 * (2 ** attempt))
                if df is None and last_err is not None:
                    raise last_err
                # æ³¨é‡Šæ‰printè¾“å‡ºä»¥æé«˜æ€§èƒ½
                # print(f"ç›´æ¥è·å–å‘¨Kçº¿ç»“æœ: {df is not None}, {len(df) if df is not None else 0} æ¡")
                
                if df is not None and not df.empty:
                    # é‡å‘½ååˆ—
                    if len(df.columns) >= 6:
                        rename_dict = {}
                        if len(df.columns) > 0:
                            rename_dict[df.columns[0]] = 'æ—¥æœŸ'
                        # æ ¹æ®2025-12-31çš„æ­£ç¡®æ•°æ®æ¨æ–­ï¼š
                        # æ­£ç¡®ï¼šå¼€ç›˜=4.66, æ”¶ç›˜=4.65, æœ€é«˜=4.68, æœ€ä½=4.62
                        # ä¹‹å‰é”™è¯¯æ˜¾ç¤ºï¼šåˆ—1=2.00, åˆ—2=4.66, åˆ—3=4.65, åˆ—4=4.68
                        # æ‰€ä»¥åˆ—é¡ºåºæ˜¯ï¼šæ—¥æœŸã€å…¶ä»–ï¼ˆåˆ—1ï¼Œå¯èƒ½æ˜¯æ¶¨è·Œå¹…ï¼‰ã€å¼€ç›˜ï¼ˆåˆ—2ï¼‰ã€æ”¶ç›˜ï¼ˆåˆ—3ï¼‰ã€æœ€é«˜ï¼ˆåˆ—4ï¼‰ã€æœ€ä½ï¼ˆåˆ—5ï¼‰
                        # åˆ—1è·³è¿‡ï¼ˆå¯èƒ½æ˜¯æ¶¨è·Œå¹…æˆ–å…¶ä»–æ•°æ®ï¼‰
                        if len(df.columns) > 2:
                            rename_dict[df.columns[2]] = 'å¼€ç›˜'  # åˆ—2æ˜¯å¼€ç›˜
                        if len(df.columns) > 3:
                            rename_dict[df.columns[3]] = 'æ”¶ç›˜'  # åˆ—3æ˜¯æ”¶ç›˜
                        if len(df.columns) > 4:
                            rename_dict[df.columns[4]] = 'æœ€é«˜'  # åˆ—4æ˜¯æœ€é«˜
                        if len(df.columns) > 5:
                            rename_dict[df.columns[5]] = 'æœ€ä½'  # åˆ—5æ˜¯æœ€ä½
                        if len(df.columns) > 6:
                            rename_dict[df.columns[6]] = 'æˆäº¤é‡'  # åˆ—6æ˜¯æˆäº¤é‡
                        
                        df = df.rename(columns=rename_dict)
                    
                    # ç¡®ä¿æ—¥æœŸåˆ—å­˜åœ¨ä¸”å¯è½¬æ¢
                    if 'æ—¥æœŸ' in df.columns:
                        df['æ—¥æœŸ'] = pd.to_datetime(df['æ—¥æœŸ'], errors='coerce')
                        df = df.dropna(subset=['æ—¥æœŸ'])
                        df = df.sort_values('æ—¥æœŸ').reset_index(drop=True)
                        # é‡å‘½åæˆäº¤é‡ä¸ºå‘¨æˆäº¤é‡
                        if 'æˆäº¤é‡' in df.columns:
                            df = df.rename(columns={'æˆäº¤é‡': 'å‘¨æˆäº¤é‡'})
                        # ä¿å­˜åˆ°ç¼“å­˜
                        if use_cache:
                            self._save_weekly_kline_to_cache(stock_code, df)
                        return df
            except Exception as e1:
                print(f"ç›´æ¥è·å–å‘¨Kçº¿å¤±è´¥: {e1}ï¼Œå°è¯•ä»æ—¥Kçº¿èšåˆ...")
                # å¦‚æœç›´æ¥è·å–å¤±è´¥ï¼Œä½¿ç”¨èšåˆæ–¹å¼
            
            # æ–¹æ³•2: ä»æ—¥Kçº¿èšåˆä¸ºå‘¨Kçº¿
            print(f"å¼€å§‹ä»æ—¥Kçº¿èšåˆå‘¨Kçº¿: {stock_code}")
            daily_df = self.get_daily_kline(stock_code, period)
            if daily_df is None or daily_df.empty:
                print(f"æ— æ³•è·å– {stock_code} çš„æ—¥Kçº¿æ•°æ®")
                return None
            print(f"è·å–åˆ° {len(daily_df)} æ¡æ—¥Kçº¿æ•°æ®ï¼Œå¼€å§‹èšåˆ...")
            
            # ç¡®ä¿å¿…è¦çš„åˆ—å­˜åœ¨
            required_cols = ['æ—¥æœŸ', 'å¼€ç›˜', 'æ”¶ç›˜', 'æœ€é«˜', 'æœ€ä½', 'æˆäº¤é‡']
            missing_cols = [col for col in required_cols if col not in daily_df.columns]
            if missing_cols:
                print(f"è­¦å‘Šï¼šç¼ºå°‘å¿…è¦çš„åˆ— {missing_cols}")
                return None
            
            # è½¬æ¢ä¸ºå‘¨Kçº¿
            weekly_df = daily_df.copy()
            
            # ä½¿ç”¨ISOå‘¨ï¼ˆå‘¨ä¸€å¼€å§‹ï¼‰
            weekly_df['å¹´å‘¨'] = weekly_df['æ—¥æœŸ'].dt.to_period('W-SUN')  # å‘¨æ—¥ç»“æŸçš„å‘¨
            
            # æŒ‰å‘¨èšåˆ
            def agg_week(group):
                return pd.Series({
                    'å¼€ç›˜': group['å¼€ç›˜'].iloc[0],
                    'æ”¶ç›˜': group['æ”¶ç›˜'].iloc[-1],
                    'æœ€é«˜': group['æœ€é«˜'].max(),
                    'æœ€ä½': group['æœ€ä½'].min(),
                    'å‘¨æˆäº¤é‡': group['æˆäº¤é‡'].sum()  # å‘¨æˆäº¤é‡ = è¯¥å‘¨æ‰€æœ‰äº¤æ˜“æ—¥çš„æˆäº¤é‡ä¹‹å’Œ
                })
            
            weekly_kline = weekly_df.groupby('å¹´å‘¨').apply(agg_week).reset_index()
            
            # å¦‚æœæˆäº¤é¢åˆ—å­˜åœ¨ï¼Œä¹Ÿèšåˆ
            if 'æˆäº¤é¢' in weekly_df.columns:
                weekly_kline['å‘¨æˆäº¤é¢'] = weekly_df.groupby('å¹´å‘¨')['æˆäº¤é¢'].sum().values
            
            # å°†å‘¨æœŸè½¬æ¢ä¸ºæ—¥æœŸï¼ˆä½¿ç”¨è¯¥å‘¨çš„æœ€åä¸€å¤©ï¼‰
            weekly_kline['æ—¥æœŸ'] = weekly_kline['å¹´å‘¨'].dt.to_timestamp() + pd.Timedelta(days=6)
            weekly_kline = weekly_kline.sort_values('æ—¥æœŸ').reset_index(drop=True)
            
            # è®¡ç®—å‘¨æ¶¨è·Œå¹…
            weekly_kline['æ¶¨è·Œå¹…'] = weekly_kline['æ”¶ç›˜'].pct_change() * 100
            weekly_kline['æ¶¨è·Œå¹…'] = weekly_kline['æ¶¨è·Œå¹…'].fillna(0)
            
            # ä¿å­˜åˆ°ç¼“å­˜
            if use_cache:
                self._save_weekly_kline_to_cache(stock_code, weekly_kline)
            
            return weekly_kline
        except Exception as e:
            import traceback
            print(f"è·å– {stock_code} å‘¨Kçº¿æ•°æ®å¤±è´¥: {e}")
            print(f"é”™è¯¯è¯¦æƒ…: {traceback.format_exc()}")
            return None
    
    def get_monthly_kline(self, stock_code, period="1y"):
        """
        è·å–æœˆKçº¿æ•°æ®
        :param stock_code: è‚¡ç¥¨ä»£ç 
        :param period: æ—¶é—´å‘¨æœŸ
        :return: DataFrameï¼ŒæœˆKçº¿æ•°æ®
        """
        try:
            # å…ˆè·å–æ—¥Kçº¿ï¼Œç„¶åè½¬æ¢ä¸ºæœˆKçº¿
            daily_df = self.get_daily_kline(stock_code, period)
            if daily_df is None or daily_df.empty:
                return None
            
            # è½¬æ¢ä¸ºæœˆKçº¿
            monthly_df = daily_df.copy()
            
            # ç¡®ä¿å¿…è¦çš„åˆ—å­˜åœ¨
            required_cols = ['æ—¥æœŸ', 'å¼€ç›˜', 'æ”¶ç›˜', 'æœ€é«˜', 'æœ€ä½', 'æˆäº¤é‡']
            missing_cols = [col for col in required_cols if col not in monthly_df.columns]
            if missing_cols:
                print(f"è­¦å‘Šï¼šç¼ºå°‘å¿…è¦çš„åˆ— {missing_cols}")
                return None
            
            monthly_df['å¹´æœˆ'] = monthly_df['æ—¥æœŸ'].dt.to_period('M')
            
            # æŒ‰æœˆèšåˆï¼ˆä½¿ç”¨applyæ–¹å¼ï¼Œæ›´å…¼å®¹ï¼‰
            def agg_month(group):
                return pd.Series({
                    'å¼€ç›˜': group['å¼€ç›˜'].iloc[0],
                    'æ”¶ç›˜': group['æ”¶ç›˜'].iloc[-1],
                    'æœ€é«˜': group['æœ€é«˜'].max(),
                    'æœ€ä½': group['æœ€ä½'].min(),
                    'æˆäº¤é‡': group['æˆäº¤é‡'].sum()
                })
            
            monthly_kline = monthly_df.groupby('å¹´æœˆ').apply(agg_month).reset_index()
            
            # å¦‚æœæˆäº¤é¢åˆ—å­˜åœ¨ï¼Œä¹Ÿèšåˆ
            if 'æˆäº¤é¢' in monthly_df.columns:
                monthly_kline['æˆäº¤é¢'] = monthly_df.groupby('å¹´æœˆ')['æˆäº¤é¢'].sum().values
            
            monthly_kline['æ—¥æœŸ'] = monthly_kline['å¹´æœˆ'].dt.to_timestamp()
            monthly_kline = monthly_kline.sort_values('æ—¥æœŸ').reset_index(drop=True)
            
            # è®¡ç®—æœˆæ¶¨è·Œå¹…
            monthly_kline['æ¶¨è·Œå¹…'] = monthly_kline['æ”¶ç›˜'].pct_change() * 100
            monthly_kline['æ¶¨è·Œå¹…'] = monthly_kline['æ¶¨è·Œå¹…'].fillna(0)
            
            return monthly_kline
        except Exception as e:
            import traceback
            print(f"è·å– {stock_code} æœˆKçº¿æ•°æ®å¤±è´¥: {e}")
            print(f"é”™è¯¯è¯¦æƒ…: {traceback.format_exc()}")
            return None
    
    def get_limit_up_info(self, stock_code, days=10):
        """
        è·å–æœ€è¿‘Nä¸ªäº¤æ˜“æ—¥çš„æ¶¨åœä¿¡æ¯
        :param stock_code: è‚¡ç¥¨ä»£ç 
        :param days: æŸ¥è¯¢å¤©æ•°
        :return: æ˜¯å¦æœ‰æ¶¨åœï¼ˆTrue/Falseï¼‰ï¼Œæ¶¨åœæ—¥æœŸåˆ—è¡¨
        """
        try:
            # è·å–æœ€è¿‘Nå¤©çš„æ—¥Kçº¿æ•°æ®
            end_date = datetime.now().strftime('%Y%m%d')
            start_date = (datetime.now() - timedelta(days=days*2)).strftime('%Y%m%d')  # å¤šå–ä¸€äº›ï¼Œæ’é™¤éäº¤æ˜“æ—¥
            
            df = ak.stock_zh_a_hist(symbol=stock_code, period="daily", 
                                    start_date=start_date, end_date=end_date, adjust="qfq")
            
            if df is None or df.empty:
                return False, []
            
            # ä½¿ç”¨ä½ç½®ç´¢å¼•è®¿é—®æ•°æ®ï¼Œé¿å…åˆ—åé—®é¢˜
            if len(df.columns) < 9:
                return False, []
            
            # ç›´æ¥ä½¿ç”¨ä½ç½®ç´¢å¼•
            df.iloc[:, 0] = pd.to_datetime(df.iloc[:, 0], errors='coerce')
            df = df.dropna(subset=[df.columns[0]])
            if df.empty:
                return False, []
            
            # æŒ‰æ—¥æœŸæ’åº
            df = df.sort_values(by=df.columns[0]).reset_index(drop=True)
            
            # å–æœ€è¿‘daysä¸ªäº¤æ˜“æ—¥
            recent_df = df.tail(days)
            
            # åˆ¤æ–­æ˜¯å¦æœ‰æ¶¨åœï¼ˆæ¶¨è·Œå¹… >= 9.5%ï¼Œè€ƒè™‘STè‚¡æ˜¯5%ï¼‰
            # æ¶¨è·Œå¹…é€šå¸¸åœ¨ç¬¬8åˆ—ï¼ˆç´¢å¼•8ï¼‰
            pct_chg_col = df.columns[8] if len(df.columns) > 8 else None
            if pct_chg_col is None:
                return False, []
            
            limit_up_mask = recent_df[pct_chg_col] >= 9.5
            date_col = df.columns[0]
            limit_up_days = recent_df[limit_up_mask][date_col].tolist()
            has_limit_up = len(limit_up_days) > 0
            
            return has_limit_up, limit_up_days
        except Exception as e:
            print(f"è·å– {stock_code} æ¶¨åœä¿¡æ¯å¤±è´¥: {e}")
            return False, []

