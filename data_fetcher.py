"""
æ•°æ®è·å–æ¨¡å—
ä½¿ç”¨akshareè·å–Aè‚¡å¸‚åœºæ•°æ®
"""
import akshare as ak
import pandas as pd
import time
from datetime import datetime, timedelta
import warnings
warnings.filterwarnings('ignore')


class DataFetcher:
    """æ•°æ®è·å–ç±»"""
    
    def __init__(self):
        self.stock_list = None
        self._market_cap_cache = None  # ç¼“å­˜å¸‚å€¼æ•°æ®ï¼Œé¿å…é‡å¤è·å–
        
    def _get_stock_list_from_cache(self, check_age=False):
        """
        ä»ç¼“å­˜ä¸­è·å–è‚¡ç¥¨åˆ—è¡¨
        :param check_age: æ˜¯å¦æ£€æŸ¥ç¼“å­˜å¹´é¾„ï¼ˆç”¨äºåˆ¤æ–­æ˜¯å¦è¿‡æœŸï¼‰
        :return: å¦‚æœ check_age=Trueï¼Œè¿”å› (stock_df, cache_timestamp, is_expired)ï¼Œå¦åˆ™è¿”å› stock_df
        """
        try:
            import os
            import json
            from datetime import datetime, timezone
            
            # å°è¯•ä½¿ç”¨ Upstash Redis
            redis_url = os.environ.get('UPSTASH_REDIS_REST_URL')
            redis_token = os.environ.get('UPSTASH_REDIS_REST_TOKEN')
            if redis_url and redis_token:
                import requests
                try:
                    # è·å–ç¼“å­˜æ•°æ®å’Œæ—¶é—´æˆ³
                    response = requests.get(
                        f"{redis_url}/get/stock_list_all",
                        headers={"Authorization": f"Bearer {redis_token}"},
                        timeout=2  # ç¼“å­˜è·å–åº”è¯¥å¾ˆå¿«
                    )
                    if response.status_code == 200:
                        result = response.json()
                        value_str = result.get('result')
                        if value_str:
                            # è§£æ JSON å­—ç¬¦ä¸²
                            stock_data = json.loads(value_str) if isinstance(value_str, str) else value_str
                            # è½¬æ¢ä¸º DataFrameï¼ˆç¡®ä¿æ•°æ®æ ¼å¼æ­£ç¡®ï¼‰
                            if isinstance(stock_data, list) and len(stock_data) > 0:
                                import pandas as pd
                                stock_df = pd.DataFrame(stock_data)
                                
                                # å°è¯•è·å–ç¼“å­˜æ—¶é—´æˆ³
                                cache_timestamp = None
                                is_expired = False
                                if check_age:
                                    try:
                                        # å°è¯•è·å–ç¼“å­˜çš„TTLï¼ˆå‰©ä½™æ—¶é—´ï¼‰
                                        ttl_response = requests.get(
                                            f"{redis_url}/ttl/stock_list_all",
                                            headers={"Authorization": f"Bearer {redis_token}"},
                                            timeout=2
                                        )
                                        if ttl_response.status_code == 200:
                                            ttl_result = ttl_response.json()
                                            ttl_seconds = ttl_result.get('result', -1)
                                            if ttl_seconds > 0:
                                                # TTL = 86400ç§’ï¼ˆ24å°æ—¶ï¼‰ï¼Œç¼“å­˜æ—¶é—´ = å½“å‰æ—¶é—´ - (86400 - TTL)
                                                cache_age_seconds = 86400 - ttl_seconds
                                                cache_timestamp = datetime.now(timezone.utc).timestamp() - cache_age_seconds
                                                # å¦‚æœåœ¨äº¤æ˜“æ—¶é—´æ®µå†…ä¸”ç¼“å­˜è¶…è¿‡5åˆ†é’Ÿï¼Œè®¤ä¸ºè¿‡æœŸ
                                                from datetime import timedelta
                                                beijing_now = datetime.now(timezone.utc) + timedelta(hours=8)
                                                is_in_trading_time = (
                                                    (beijing_now.hour == 9 and beijing_now.minute >= 30) or
                                                    beijing_now.hour == 10 or
                                                    (beijing_now.hour == 11 and beijing_now.minute <= 30) or
                                                    beijing_now.hour == 13 or
                                                    beijing_now.hour == 14 or
                                                    (beijing_now.hour == 15 and beijing_now.minute == 0)
                                                )
                                                if is_in_trading_time and cache_age_seconds > 300:  # 5åˆ†é’Ÿ = 300ç§’
                                                    is_expired = True
                                                    print(f"[get_all_stocks] âš ï¸ ç¼“å­˜å·²è¿‡æœŸï¼ˆäº¤æ˜“æ—¶é—´æ®µå†…ï¼Œç¼“å­˜å¹´é¾„: {cache_age_seconds//60}åˆ†é’Ÿï¼‰ï¼Œéœ€è¦åˆ·æ–°")
                                    except Exception as e:
                                        print(f"[get_all_stocks] âš ï¸ è·å–ç¼“å­˜TTLå¤±è´¥: {e}")
                                
                                if check_age:
                                    print(f"[get_all_stocks] âœ… ä» Redis ç¼“å­˜è·å–è‚¡ç¥¨åˆ—è¡¨: {len(stock_df)} åªï¼Œç¼“å­˜å¹´é¾„: {cache_age_seconds//60 if cache_timestamp else 'unknown'}åˆ†é’Ÿ")
                                    return stock_df, cache_timestamp, is_expired
                                else:
                                    print(f"[get_all_stocks] âœ… ä» Redis ç¼“å­˜è·å–è‚¡ç¥¨åˆ—è¡¨: {len(stock_df)} åª")
                                    return stock_df
                            else:
                                print(f"[get_all_stocks] âš ï¸ ç¼“å­˜æ•°æ®æ ¼å¼é”™è¯¯: {type(stock_data)}")
                except Exception as e:
                    print(f"[get_all_stocks] âš ï¸ ä» Redis ç¼“å­˜è·å–å¤±è´¥: {e}")
            
            # å°è¯•ä½¿ç”¨ Vercel KVï¼ˆVercel KV ä¸æ”¯æŒ TTL æŸ¥è¯¢ï¼Œæš‚æ—¶ä¸æ£€æŸ¥å¹´é¾„ï¼‰
            try:
                from vercel_kv import kv
                cached_data = kv.get('stock_list_all')
                if cached_data:
                    stock_data = json.loads(cached_data) if isinstance(cached_data, str) else cached_data
                    # è½¬æ¢ä¸º DataFrameï¼ˆç¡®ä¿æ•°æ®æ ¼å¼æ­£ç¡®ï¼‰
                    if isinstance(stock_data, list) and len(stock_data) > 0:
                        import pandas as pd
                        stock_df = pd.DataFrame(stock_data)
                        if check_age:
                            # Vercel KV ä¸æ”¯æŒTTLæŸ¥è¯¢ï¼Œè¿”å› None è¡¨ç¤ºæœªçŸ¥
                            print(f"[get_all_stocks] âœ… ä» Vercel KV ç¼“å­˜è·å–è‚¡ç¥¨åˆ—è¡¨: {len(stock_df)} åªï¼ˆæ— æ³•æ£€æŸ¥ç¼“å­˜å¹´é¾„ï¼‰")
                            return stock_df, None, False
                        else:
                            print(f"[get_all_stocks] âœ… ä» Vercel KV ç¼“å­˜è·å–è‚¡ç¥¨åˆ—è¡¨: {len(stock_df)} åª")
                            return stock_df
                    else:
                        print(f"[get_all_stocks] âš ï¸ Vercel KV ç¼“å­˜æ•°æ®æ ¼å¼é”™è¯¯: {type(stock_data)}")
            except Exception as e:
                print(f"[get_all_stocks] âš ï¸ ä» Vercel KV ç¼“å­˜è·å–å¤±è´¥: {e}")
                
        except Exception as e:
            print(f"[get_all_stocks] âš ï¸ ä»ç¼“å­˜è·å–è‚¡ç¥¨åˆ—è¡¨å¤±è´¥: {e}")
        
        if check_age:
            return None, None, True  # ç¼“å­˜ä¸å­˜åœ¨ï¼Œè®¤ä¸ºè¿‡æœŸ
        return None
    
    def _save_stock_list_to_cache(self, stock_df):
        """å°†è‚¡ç¥¨åˆ—è¡¨ä¿å­˜åˆ°ç¼“å­˜ï¼ˆTTL: 24å°æ—¶ = 86400ç§’ï¼‰"""
        try:
            import os
            import json
            
            # ç¡®ä¿ stock_df æ˜¯ DataFrame
            if stock_df is None or len(stock_df) == 0:
                print(f"[_save_stock_list_to_cache] âš ï¸ è‚¡ç¥¨åˆ—è¡¨ä¸ºç©ºï¼Œæ— æ³•ä¿å­˜åˆ°ç¼“å­˜")
                return False
            
            # å°† DataFrame è½¬æ¢ä¸º JSON æ ¼å¼ï¼ˆå­—å…¸åˆ—è¡¨ï¼‰
            try:
                stock_data = stock_df.to_dict('records')
                stock_json = json.dumps(stock_data, default=str, ensure_ascii=False)
                print(f"[_save_stock_list_to_cache] å‡†å¤‡ä¿å­˜ {len(stock_df)} åªè‚¡ç¥¨åˆ°ç¼“å­˜ï¼ŒJSON å¤§å°: {len(stock_json)} å­—ç¬¦")
            except Exception as e:
                print(f"[_save_stock_list_to_cache] âš ï¸ è½¬æ¢ DataFrame åˆ° JSON å¤±è´¥: {e}")
                import traceback
                print(traceback.format_exc())
                return False
            
            # å°è¯•ä½¿ç”¨ Upstash Redisï¼ˆæœ€å¤šé‡è¯•2æ¬¡ï¼‰
            redis_url = os.environ.get('UPSTASH_REDIS_REST_URL')
            redis_token = os.environ.get('UPSTASH_REDIS_REST_TOKEN')
            
            # è¯Šæ–­æ—¥å¿—ï¼šæ£€æŸ¥ç¯å¢ƒå˜é‡æ˜¯å¦è®¾ç½®
            if not redis_url:
                print(f"[_save_stock_list_to_cache] âš ï¸ UPSTASH_REDIS_REST_URL ç¯å¢ƒå˜é‡æœªè®¾ç½®ï¼Œè·³è¿‡ Redis ä¿å­˜")
            if not redis_token:
                print(f"[_save_stock_list_to_cache] âš ï¸ UPSTASH_REDIS_REST_TOKEN ç¯å¢ƒå˜é‡æœªè®¾ç½®ï¼Œè·³è¿‡ Redis ä¿å­˜")
            if redis_url and not redis_token:
                print(f"[_save_stock_list_to_cache] âš ï¸ UPSTASH_REDIS_REST_URL å·²è®¾ç½®ï¼Œä½† UPSTASH_REDIS_REST_TOKEN æœªè®¾ç½®ï¼Œè·³è¿‡ Redis ä¿å­˜")
            if not redis_url and redis_token:
                print(f"[_save_stock_list_to_cache] âš ï¸ UPSTASH_REDIS_REST_TOKEN å·²è®¾ç½®ï¼Œä½† UPSTASH_REDIS_REST_URL æœªè®¾ç½®ï¼Œè·³è¿‡ Redis ä¿å­˜")
            
            if redis_url and redis_token:
                print(f"[_save_stock_list_to_cache] âœ… Redis ç¯å¢ƒå˜é‡å·²è®¾ç½®ï¼Œå°è¯•ä¿å­˜åˆ° Redis...")
                import requests
                max_retries = 2
                for attempt in range(max_retries):
                    try:
                        # ç¼“å­˜ 24 å°æ—¶ï¼ˆ86400ç§’ï¼‰
                        # Upstash Redis REST API: POST /setex/{key}/{ttl}
                        # è¯·æ±‚ä½“æ ¼å¼ï¼šJSON å­—ç¬¦ä¸²ï¼ˆå€¼æœ¬èº«æ˜¯ JSON å­—ç¬¦ä¸²ï¼‰
                        # æ³¨æ„ï¼šä½¿ç”¨ data å‚æ•°å‘é€å­—ç¬¦ä¸²ï¼Œè€Œä¸æ˜¯ json å‚æ•°ï¼ˆé¿å…åŒé‡ç¼–ç ï¼‰
                        if attempt > 0:
                            print(f"[_save_stock_list_to_cache] é‡è¯•ä¿å­˜åˆ° Upstash Redisï¼ˆç¬¬ {attempt + 1}/{max_retries} æ¬¡ï¼‰...")
                        else:
                            print(f"[_save_stock_list_to_cache] å°è¯•ä¿å­˜åˆ° Upstash Redis...")
                        print(f"[_save_stock_list_to_cache] JSON å¤§å°: {len(stock_json)} å­—ç¬¦")
                        # Upstash Redis REST API setex éœ€è¦å°†å€¼ä½œä¸ºå­—ç¬¦ä¸²å‘é€
                        # ä½¿ç”¨ data å‚æ•°ç›´æ¥å‘é€å­—ç¬¦ä¸²ï¼Œé¿å…åŒé‡ JSON ç¼–ç 
                        response = requests.post(
                            f"{redis_url}/setex/stock_list_all/86400",
                            headers={
                                "Authorization": f"Bearer {redis_token}",
                                "Content-Type": "application/json"
                            },
                            data=stock_json.encode('utf-8'),  # ä½¿ç”¨ data å‚æ•°å‘é€ UTF-8 ç¼–ç çš„å­—ç¬¦ä¸²
                            timeout=15  # å¢åŠ è¶…æ—¶æ—¶é—´åˆ°15ç§’ï¼ˆæ•°æ®è¾ƒå¤§ï¼Œå¯èƒ½éœ€è¦æ›´é•¿æ—¶é—´ï¼‰
                        )
                        if response.status_code == 200:
                            try:
                                result = response.json()
                                # Upstash è¿”å›æ ¼å¼: {"result": "OK"} æˆ– {"result": true}
                                if result.get('result') == 'OK' or result.get('result') is True:
                                    print(f"[_save_stock_list_to_cache] âœ… è‚¡ç¥¨åˆ—è¡¨å·²ä¿å­˜åˆ° Redis ç¼“å­˜ï¼ˆTTL: 24å°æ—¶ï¼Œè‚¡ç¥¨æ•°: {len(stock_df)}ï¼‰")
                                    return True
                                else:
                                    print(f"[_save_stock_list_to_cache] âš ï¸ Redis ä¿å­˜è¿”å›å¼‚å¸¸ç»“æœ: {result}")
                                    if attempt < max_retries - 1:
                                        import time
                                        time.sleep(1)  # ç­‰å¾…1ç§’åé‡è¯•
                                        continue
                            except Exception as parse_error:
                                print(f"[_save_stock_list_to_cache] âš ï¸ Redis ä¿å­˜å“åº”è§£æå¤±è´¥: {parse_error}ï¼Œä½†çŠ¶æ€ç ä¸º200ï¼Œè®¤ä¸ºä¿å­˜æˆåŠŸ")
                                return True
                        else:
                            try:
                                error_msg = response.text[:1000] if hasattr(response, 'text') else str(response.status_code)
                            except:
                                error_msg = f"çŠ¶æ€ç : {response.status_code}"
                            print(f"[_save_stock_list_to_cache] âš ï¸ Redis ä¿å­˜å¤±è´¥ï¼ŒçŠ¶æ€ç : {response.status_code}, å“åº”: {error_msg}")
                            print(f"[_save_stock_list_to_cache] å“åº”å¤´: {dict(response.headers) if hasattr(response, 'headers') else 'N/A'}")
                            if attempt < max_retries - 1:
                                import time
                                time.sleep(1)  # ç­‰å¾…1ç§’åé‡è¯•
                                continue
                    except requests.exceptions.Timeout:
                        print(f"[_save_stock_list_to_cache] âš ï¸ Redis ä¿å­˜è¶…æ—¶ï¼ˆ15ç§’ï¼‰")
                        if attempt < max_retries - 1:
                            import time
                            time.sleep(1)  # ç­‰å¾…1ç§’åé‡è¯•
                            continue
                    except Exception as e:
                        import traceback
                        error_detail = traceback.format_exc()
                        print(f"[_save_stock_list_to_cache] âš ï¸ ä¿å­˜åˆ° Redis ç¼“å­˜å¤±è´¥: {e}")
                        print(f"[_save_stock_list_to_cache] é”™è¯¯è¯¦æƒ…: {error_detail}")
                        if attempt < max_retries - 1:
                            import time
                            time.sleep(1)  # ç­‰å¾…1ç§’åé‡è¯•
                            continue
            
            # å°è¯•ä½¿ç”¨ Vercel KVï¼ˆå¦‚æœæ²¡æœ‰ä½¿ç”¨ Redis æˆ– Redis ä¿å­˜å¤±è´¥ï¼‰
            # å³ä½¿ Redis å¯ç”¨ï¼Œä¹Ÿå°è¯• Vercel KV ä½œä¸ºå¤‡ç”¨æ–¹æ¡ˆ
            try:
                from vercel_kv import kv
                print(f"[_save_stock_list_to_cache] å°è¯•ä¿å­˜åˆ° Vercel KVï¼ˆå¤‡ç”¨æ–¹æ¡ˆï¼‰...")
                print(f"[_save_stock_list_to_cache] JSON å¤§å°: {len(stock_json)} å­—ç¬¦")
                kv.set('stock_list_all', stock_json, ttl=86400)  # 24å°æ—¶
                print(f"[_save_stock_list_to_cache] âœ… è‚¡ç¥¨åˆ—è¡¨å·²ä¿å­˜åˆ° Vercel KV ç¼“å­˜ï¼ˆTTL: 24å°æ—¶ï¼Œè‚¡ç¥¨æ•°: {len(stock_df)}ï¼‰")
                return True
            except ImportError:
                print(f"[_save_stock_list_to_cache] âš ï¸ Vercel KV æœªå®‰è£…æˆ–ä¸å¯ç”¨ï¼ˆè¿™æ˜¯æ­£å¸¸çš„ï¼Œå¦‚æœä½¿ç”¨ Redisï¼‰")
                # ImportError ä¸æ˜¯çœŸæ­£çš„é”™è¯¯ï¼Œåªæ˜¯è¡¨ç¤º Vercel KV ä¸å¯ç”¨ï¼Œä¸é˜»æ­¢ç»§ç»­æ‰§è¡Œ
            except Exception as e:
                import traceback
                error_detail = traceback.format_exc()
                print(f"[_save_stock_list_to_cache] âš ï¸ ä¿å­˜åˆ° Vercel KV ç¼“å­˜å¤±è´¥: {e}")
                print(f"[_save_stock_list_to_cache] é”™è¯¯è¯¦æƒ…: {error_detail}")
                # Vercel KV å¤±è´¥ä¸æ˜¯è‡´å‘½é”™è¯¯ï¼Œç»§ç»­æ‰§è¡Œ
                
        except Exception as e:
            import traceback
            error_detail = traceback.format_exc()
            print(f"[_save_stock_list_to_cache] âš ï¸ ä¿å­˜è‚¡ç¥¨åˆ—è¡¨åˆ°ç¼“å­˜å¤±è´¥: {e}")
            print(f"[_save_stock_list_to_cache] é”™è¯¯è¯¦æƒ…: {error_detail}")
        
        print(f"[_save_stock_list_to_cache] âŒ æ‰€æœ‰ç¼“å­˜ä¿å­˜æ–¹å¼å‡å¤±è´¥ï¼Œè¿”å› False")
        return False
        
    def get_all_stocks(self, timeout=10, max_retries=3):
        """
        è·å–æ‰€æœ‰Aè‚¡è‚¡ç¥¨åˆ—è¡¨ï¼ˆä¼˜å…ˆä»ç¼“å­˜è·å–ï¼‰
        è¿”å›: DataFrameï¼ŒåŒ…å«è‚¡ç¥¨ä»£ç ã€åç§°ç­‰ä¿¡æ¯
        :param timeout: è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰ï¼Œé»˜è®¤10ç§’
        :param max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°ï¼Œé»˜è®¤3æ¬¡
        """
        import signal
        import threading
        import os
        
        # é¦–å…ˆå°è¯•ä»ç¼“å­˜è·å–ï¼ˆä¼˜å…ˆä»ç¼“å­˜è¯»å–ï¼Œé¿å…æ¯æ¬¡è°ƒç”¨ akshare APIï¼‰
        # åœ¨äº¤æ˜“æ—¶é—´æ®µå†…ï¼Œæ£€æŸ¥ç¼“å­˜å¹´é¾„ï¼Œå¦‚æœè¿‡æœŸåˆ™åˆ·æ–°
        print("[get_all_stocks] å°è¯•ä»ç¼“å­˜è·å–è‚¡ç¥¨åˆ—è¡¨...")
        from datetime import datetime, timezone, timedelta
        beijing_now = datetime.now(timezone.utc) + timedelta(hours=8)
        is_in_trading_time = (
            (beijing_now.hour == 9 and beijing_now.minute >= 30) or
            beijing_now.hour == 10 or
            (beijing_now.hour == 11 and beijing_now.minute <= 30) or
            beijing_now.hour == 13 or
            beijing_now.hour == 14 or
            (beijing_now.hour == 15 and beijing_now.minute == 0)
        )
        
        # å¦‚æœåœ¨äº¤æ˜“æ—¶é—´æ®µå†…ï¼Œæ£€æŸ¥ç¼“å­˜å¹´é¾„
        if is_in_trading_time:
            cached_stocks, cache_timestamp, is_expired = self._get_stock_list_from_cache(check_age=True)
            if cached_stocks is not None and len(cached_stocks) > 0 and not is_expired:
                self.stock_list = cached_stocks
                print(f"[get_all_stocks] âœ… ä»ç¼“å­˜è·å–æˆåŠŸï¼ˆäº¤æ˜“æ—¶é—´æ®µå†…ï¼Œç¼“å­˜æœªè¿‡æœŸï¼‰ï¼Œè‚¡ç¥¨æ•°: {len(cached_stocks)} åª")
                return cached_stocks
            elif cached_stocks is not None and len(cached_stocks) > 0 and is_expired:
                print(f"[get_all_stocks] âš ï¸ ç¼“å­˜å·²è¿‡æœŸï¼ˆäº¤æ˜“æ—¶é—´æ®µå†…ï¼Œç¼“å­˜è¶…è¿‡5åˆ†é’Ÿï¼‰ï¼Œå°†ä» API è·å–æœ€æ–°æ•°æ®...")
                # ç»§ç»­æ‰§è¡Œï¼Œä» API è·å–æœ€æ–°æ•°æ®
            elif cached_stocks is None:
                print(f"[get_all_stocks] âš ï¸ ç¼“å­˜ä¸å­˜åœ¨ï¼Œå°†ä» API è·å–...")
            else:
                print(f"[get_all_stocks] âš ï¸ ç¼“å­˜æ•°æ®ä¸ºç©ºï¼Œå°†ä» API è·å–...")
        else:
            # éäº¤æ˜“æ—¶é—´æ®µï¼Œç›´æ¥ä½¿ç”¨ç¼“å­˜ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
            cached_stocks = self._get_stock_list_from_cache(check_age=False)
            if cached_stocks is not None and len(cached_stocks) > 0:
                self.stock_list = cached_stocks
                print(f"[get_all_stocks] âœ… ä»ç¼“å­˜è·å–æˆåŠŸï¼Œè‚¡ç¥¨æ•°: {len(cached_stocks)} åªï¼ˆéäº¤æ˜“æ—¶é—´æ®µï¼Œæ— éœ€è°ƒç”¨ akshare APIï¼‰")
                return cached_stocks
        
        print("[get_all_stocks] âš ï¸ ç¼“å­˜ä¸­æ²¡æœ‰è‚¡ç¥¨åˆ—è¡¨ï¼Œå¼€å§‹ä» akshare API è·å–...")
        print("[get_all_stocks] ğŸ’¡ æç¤ºï¼šå»ºè®®åœ¨äº¤æ˜“æ—¶é—´æ®µé€šè¿‡ Cron Job è‡ªåŠ¨åˆ·æ–°ç¼“å­˜ï¼Œé¿å…æ‰«ææ—¶è¶…æ—¶")
        
        # æ£€æµ‹ Vercel ç¯å¢ƒï¼Œåœ¨ Vercel ä¸­ä½¿ç”¨æ›´çŸ­çš„è¶…æ—¶å’Œæ›´å°‘çš„é‡è¯•
        is_vercel = (
            os.environ.get('VERCEL') == '1' or 
            os.environ.get('VERCEL_ENV') is not None or
            os.environ.get('VERCEL_URL') is not None
        )
        
        # Vercel ç¯å¢ƒä¸­ï¼Œserverless å‡½æ•°æœ‰ 10 ç§’é™åˆ¶ï¼Œä½¿ç”¨æ›´çŸ­çš„è¶…æ—¶
        # è€ƒè™‘åˆ°éœ€è¦ç•™å‡ºæ—¶é—´ç»™å…¶ä»–ä»£ç æ‰§è¡Œï¼Œå®é™…è¶…æ—¶åº”è¯¥æ›´çŸ­
        if is_vercel:
            timeout = min(timeout, 5)  # Vercel ä¸­æœ€å¤š5ç§’ï¼Œç•™å‡º5ç§’ç»™å…¶ä»–å¤„ç†
            max_retries = 1  # Vercel ä¸­åªå°è¯•1æ¬¡ï¼Œé¿å…è¶…è¿‡æ‰§è¡Œæ—¶é—´é™åˆ¶
            print(f"[get_all_stocks] Vercel ç¯å¢ƒæ£€æµ‹åˆ°ï¼Œä½¿ç”¨è¶…çŸ­è¶…æ—¶æ—¶é—´: {timeout}ç§’ï¼Œåªå°è¯• {max_retries} æ¬¡ï¼ˆé¿å…è¶…è¿‡10ç§’é™åˆ¶ï¼‰")
            print(f"[get_all_stocks] âš ï¸ å¦‚æœç¼“å­˜ä¸å­˜åœ¨ï¼Œå¯èƒ½ä¼šå› ä¸º akshare API å“åº”æ…¢è€Œå¯¼è‡´è¶…æ—¶")
        else:
            max_retries = min(max_retries, 3)  # æœ¬åœ°ç¯å¢ƒä¸­æœ€å¤šé‡è¯•3æ¬¡
            print(f"[get_all_stocks] æœ¬åœ°ç¯å¢ƒï¼Œè¶…æ—¶æ—¶é—´: {timeout}ç§’ï¼Œæœ€å¤šé‡è¯• {max_retries} æ¬¡")
        
        for attempt in range(max_retries):
            try:
                print(f"[get_all_stocks] å°è¯•è·å–è‚¡ç¥¨åˆ—è¡¨ï¼ˆç¬¬ {attempt + 1}/{max_retries} æ¬¡ï¼Œè¶…æ—¶: {timeout}ç§’ï¼‰...")
                
                # ä½¿ç”¨çº¿ç¨‹å’Œè¶…æ—¶æœºåˆ¶
                result = [None]
                error = [None]
                start_time = time.time()
                
                def fetch_stocks():
                    try:
                        # åœ¨ Vercel ç¯å¢ƒä¸­ï¼Œå°è¯•ä½¿ç”¨æ›´å¿«çš„æ¥å£æˆ–æ·»åŠ é¢å¤–é”™è¯¯å¤„ç†
                        if is_vercel:
                            try:
                                print(f"[get_all_stocks] Vercel ç¯å¢ƒï¼šå¼€å§‹è°ƒç”¨ ak.stock_info_a_code_name()...")
                                result[0] = ak.stock_info_a_code_name()
                                elapsed = time.time() - start_time
                                print(f"[get_all_stocks] Vercel ç¯å¢ƒï¼šak.stock_info_a_code_name() è°ƒç”¨æˆåŠŸï¼Œè€—æ—¶ {elapsed:.2f}ç§’")
                            except Exception as e:
                                error[0] = e
                                elapsed = time.time() - start_time
                                print(f"[get_all_stocks] âŒ Vercel ç¯å¢ƒä¸­è·å–å¤±è´¥ï¼ˆè€—æ—¶ {elapsed:.2f}ç§’ï¼‰: {e}")
                                # åœ¨ Vercel ä¸­ï¼Œä¸æ‰“å°å®Œæ•´å †æ ˆï¼Œé¿å…æ—¥å¿—è¿‡é•¿
                        else:
                            result[0] = ak.stock_info_a_code_name()
                    except Exception as e:
                        error[0] = e
                        import traceback
                        elapsed = time.time() - start_time
                        print(f"[get_all_stocks] âŒ è·å–å¤±è´¥ï¼ˆè€—æ—¶ {elapsed:.2f}ç§’ï¼‰: {e}")
                        if not is_vercel:
                            print(f"[get_all_stocks] é”™è¯¯å †æ ˆ: {traceback.format_exc()}")
                
                fetch_thread = threading.Thread(target=fetch_stocks)
                fetch_thread.daemon = True
                fetch_thread.start()
                fetch_thread.join(timeout=timeout)
                
                elapsed_total = time.time() - start_time
                
                if fetch_thread.is_alive():
                    print(f"[get_all_stocks] â±ï¸ è·å–è¶…æ—¶ï¼ˆ>{timeout}ç§’ï¼Œå®é™…è€—æ—¶ {elapsed_total:.2f}ç§’ï¼‰")
                    if is_vercel:
                        # åœ¨ Vercel ä¸­ï¼Œè¶…æ—¶ç›´æ¥è¿”å› Noneï¼Œä¸é‡è¯•
                        print(f"[get_all_stocks] Vercel ç¯å¢ƒä¸­è¶…æ—¶ï¼Œç›´æ¥è¿”å› Noneï¼ˆé¿å…è¶…è¿‡10ç§’æ‰§è¡Œæ—¶é—´é™åˆ¶ï¼‰")
                        return None
                    if attempt < max_retries - 1:
                        # ä¸åœ¨ Vercel ä¸­æ—¶ï¼Œç­‰å¾…åé‡è¯•
                        print(f"[get_all_stocks] ç­‰å¾… 2 ç§’åé‡è¯•...")
                        import time
                        time.sleep(2)
                        continue  # é‡è¯•
                    else:
                        print(f"[get_all_stocks] âŒ æ‰€æœ‰é‡è¯•éƒ½è¶…æ—¶")
                        return None
                
                if error[0]:
                    print(f"[get_all_stocks] âŒ è·å–å‡ºé”™ï¼ˆè€—æ—¶ {elapsed_total:.2f}ç§’ï¼‰: {error[0]}")
                    if is_vercel:
                        # åœ¨ Vercel ä¸­ï¼Œå¦‚æœå‡ºé”™ï¼Œç›´æ¥è¿”å› Noneï¼Œä¸é‡è¯•
                        print(f"[get_all_stocks] Vercel ç¯å¢ƒä¸­è·å–å‡ºé”™ï¼Œç›´æ¥è¿”å› Noneï¼ˆé¿å…è¶…è¿‡10ç§’æ‰§è¡Œæ—¶é—´é™åˆ¶ï¼‰")
                        return None
                    if attempt < max_retries - 1:
                        print(f"[get_all_stocks] ç­‰å¾… 2 ç§’åé‡è¯•...")
                        import time
                        time.sleep(2)
                        continue  # é‡è¯•
                    else:
                        raise error[0]
                
                if result[0] is not None and len(result[0]) > 0:
                    stock_info = result[0]
                    self.stock_list = stock_info
                    elapsed_total = time.time() - start_time
                    print(f"[get_all_stocks] âœ… æˆåŠŸè·å– {len(stock_info)} åªAè‚¡è‚¡ç¥¨ï¼ˆè€—æ—¶ {elapsed_total:.2f}ç§’ï¼‰")
                    
                    # å°†è·å–çš„è‚¡ç¥¨åˆ—è¡¨ä¿å­˜åˆ°ç¼“å­˜ï¼ˆå¼‚æ­¥ä¿å­˜ï¼Œä¸é˜»å¡ï¼‰
                    try:
                        import threading
                        def save_cache():
                            try:
                                self._save_stock_list_to_cache(stock_info)
                            except Exception as e:
                                print(f"[get_all_stocks] âš ï¸ åå°ä¿å­˜ç¼“å­˜å¤±è´¥ï¼ˆä¸å½±å“ä½¿ç”¨ï¼‰: {e}")
                        
                        cache_thread = threading.Thread(target=save_cache)
                        cache_thread.daemon = True
                        cache_thread.start()
                        # ä¸ç­‰å¾…ç¼“å­˜ä¿å­˜å®Œæˆï¼Œç«‹å³è¿”å›ç»“æœ
                        print(f"[get_all_stocks] å·²å¯åŠ¨åå°çº¿ç¨‹ä¿å­˜è‚¡ç¥¨åˆ—è¡¨åˆ°ç¼“å­˜...")
                    except Exception as e:
                        print(f"[get_all_stocks] âš ï¸ å¯åŠ¨ç¼“å­˜ä¿å­˜çº¿ç¨‹å¤±è´¥ï¼ˆä¸å½±å“ä½¿ç”¨ï¼‰: {e}")
                    
                    return stock_info
                else:
                    print(f"[get_all_stocks] âš ï¸ è¿”å›ç»“æœä¸ºç©ºï¼ˆè€—æ—¶ {elapsed_total:.2f}ç§’ï¼‰")
                    if is_vercel:
                        # åœ¨ Vercel ä¸­ï¼Œå¦‚æœç»“æœä¸ºç©ºï¼Œç›´æ¥è¿”å› Noneï¼Œä¸é‡è¯•
                        print(f"[get_all_stocks] Vercel ç¯å¢ƒä¸­ç»“æœä¸ºç©ºï¼Œç›´æ¥è¿”å› None")
                        return None
                    if attempt < max_retries - 1:
                        print(f"[get_all_stocks] ç­‰å¾… 2 ç§’åé‡è¯•...")
                        import time
                        time.sleep(2)
                        continue  # é‡è¯•
                    else:
                        print(f"[get_all_stocks] âŒ æ‰€æœ‰é‡è¯•éƒ½å¤±è´¥ï¼Œè¿”å› None")
                        return None
                        
            except Exception as e:
                import traceback
                error_detail = traceback.format_exc()
                elapsed_total = time.time() - start_time if 'start_time' in locals() else 0
                print(f"[get_all_stocks] âŒ è·å–è‚¡ç¥¨åˆ—è¡¨å¤±è´¥ï¼ˆç¬¬ {attempt + 1} æ¬¡å°è¯•ï¼Œè€—æ—¶ {elapsed_total:.2f}ç§’ï¼‰: {e}")
                if not is_vercel:
                    print(f"[get_all_stocks] é”™è¯¯è¯¦æƒ…: {error_detail}")
                
                if is_vercel:
                    # åœ¨ Vercel ä¸­ï¼Œå¦‚æœå‡ºé”™ï¼Œç›´æ¥è¿”å› Noneï¼Œä¸é‡è¯•
                    print(f"[get_all_stocks] Vercel ç¯å¢ƒä¸­å‡ºé”™ï¼Œç›´æ¥è¿”å› Noneï¼ˆé¿å…è¶…è¿‡10ç§’æ‰§è¡Œæ—¶é—´é™åˆ¶ï¼‰")
                    return None
                
                if attempt < max_retries - 1:
                    print(f"[get_all_stocks] ç­‰å¾… 2 ç§’åé‡è¯•...")
                    import time
                    time.sleep(2)
                    continue  # é‡è¯•
                else:
                    print(f"[get_all_stocks] âŒ æ‰€æœ‰é‡è¯•éƒ½å¤±è´¥")
                    return None
        
        print(f"[get_all_stocks] âŒ æ‰€æœ‰é‡è¯•éƒ½å¤±è´¥ï¼Œè¿”å› None")
        return None
    
    def get_market_cap(self, stock_code, timeout=5):
        """
        è·å–è‚¡ç¥¨æ€»å¸‚å€¼ï¼ˆå•ä½ï¼šäº¿å…ƒï¼‰
        :param stock_code: è‚¡ç¥¨ä»£ç ï¼ˆå¦‚ '000001'ï¼‰
        :param timeout: è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰ï¼Œé»˜è®¤5ç§’
        :return: æ€»å¸‚å€¼ï¼ˆäº¿å…ƒï¼‰ï¼Œå¦‚æœè·å–å¤±è´¥è¿”å›None
        """
        try:
            import threading
            import time
            
            # ä½¿ç”¨ç¼“å­˜ï¼Œé¿å…é‡å¤è·å–å…¨éƒ¨è‚¡ç¥¨æ•°æ®
            if self._market_cap_cache is None:
                # ä½¿ç”¨å®æ—¶è¡Œæƒ…æ¥å£ï¼ˆæ‰¹é‡è·å–ï¼‰- è¿™ä¸ªæ“ä½œå¯èƒ½å¾ˆæ…¢ï¼Œä½¿ç”¨è¶…æ—¶ä¿æŠ¤
                result = [None]
                error = [None]
                
                def fetch_all_stocks():
                    try:
                        result[0] = ak.stock_zh_a_spot_em()
                    except Exception as e:
                        error[0] = e
                
                # å¦‚æœç¼“å­˜ä¸ºç©ºï¼Œéœ€è¦è·å–å…¨éƒ¨è‚¡ç¥¨æ•°æ®ï¼ˆå¯èƒ½å¾ˆæ…¢ï¼‰
                fetch_thread = threading.Thread(target=fetch_all_stocks)
                fetch_thread.daemon = True
                fetch_thread.start()
                fetch_thread.join(timeout=timeout)
                
                if fetch_thread.is_alive():
                    # è¶…æ—¶äº†ï¼Œè¿”å›Noneï¼Œä¸é˜»å¡
                    return None
                
                if error[0]:
                    return None
                
                df = result[0]
                if df is not None and not df.empty:
                    self._market_cap_cache = df
                else:
                    return None
            else:
                df = self._market_cap_cache
            
            if df is not None and not df.empty:
                # æŸ¥æ‰¾å¯¹åº”è‚¡ç¥¨ï¼ˆä»£ç åˆ—æ˜¯å­—ç¬¦ä¸²ç±»å‹ï¼‰
                # ç¡®ä¿stock_codeæ˜¯å­—ç¬¦ä¸²æ ¼å¼
                stock_code_str = str(stock_code)
                stock_row = df[df['ä»£ç '] == stock_code_str]
                
                if not stock_row.empty:
                    if 'æ€»å¸‚å€¼' in stock_row.columns:
                        market_cap = stock_row.iloc[0]['æ€»å¸‚å€¼']
                        if pd.notna(market_cap):
                            try:
                                market_cap = float(market_cap)
                                # æ€»å¸‚å€¼å•ä½æ˜¯å…ƒï¼Œè½¬æ¢ä¸ºäº¿å…ƒ
                                return market_cap / 100000000
                            except (ValueError, TypeError):
                                pass
            
            return None
        except Exception as e:
            # é™é»˜å¤±è´¥ï¼Œè¿”å›Noneï¼ˆå¯ä»¥å–æ¶ˆæ³¨é‡Šä¸‹é¢çš„è¡Œæ¥è°ƒè¯•ï¼‰
            # print(f"è·å–å¸‚å€¼å¤±è´¥ {stock_code}: {e}")
            return None
    
    def get_daily_kline(self, stock_code, period="1y"):
        """
        è·å–æ—¥Kçº¿æ•°æ®
        :param stock_code: è‚¡ç¥¨ä»£ç ï¼ˆå¦‚ '000001'ï¼‰
        :param period: æ—¶é—´å‘¨æœŸï¼Œ'1y'è¡¨ç¤º1å¹´
        :return: DataFrameï¼ŒåŒ…å«æ—¥æœŸã€å¼€ç›˜ã€æ”¶ç›˜ã€æœ€é«˜ã€æœ€ä½ã€æˆäº¤é‡ç­‰
        """
        try:
            # è®¡ç®—å¼€å§‹æ—¥æœŸï¼ˆ2å¹´å‰ï¼Œç¡®ä¿æœ‰è¶³å¤Ÿå†å²æ•°æ®ï¼‰
            # ç»“æŸæ—¥æœŸä½¿ç”¨ä»Šå¤©ï¼Œç¡®ä¿è·å–æœ€æ–°æ•°æ®
            end_date = datetime.now().strftime('%Y%m%d')
            start_date = (datetime.now() - timedelta(days=365 * 2)).strftime('%Y%m%d')
            
            # è·å–æ—¥Kçº¿æ•°æ®
            df = ak.stock_zh_a_hist(symbol=stock_code, period="daily", 
                                    start_date=start_date, end_date=end_date, adjust="qfq")
            
            if df is None or df.empty:
                return None
            
            # akshareè¿”å›çš„DataFrameåˆ—åé€šå¸¸æ˜¯ä¸­æ–‡ï¼Œç›´æ¥ä½¿ç”¨ä½ç½®ç´¢å¼•æ›´å¯é 
            # æ ‡å‡†åˆ—é¡ºåºï¼šæ—¥æœŸã€å¼€ç›˜ã€æ”¶ç›˜ã€æœ€é«˜ã€æœ€ä½ã€æˆäº¤é‡ã€æˆäº¤é¢ã€æŒ¯å¹…ã€æ¶¨è·Œå¹…ã€æ¶¨è·Œé¢ã€æ¢æ‰‹ç‡
            # ä½†å®é™…è¿”å›çš„åˆ—æ•°å¯èƒ½ä¸åŒï¼Œä½¿ç”¨ä½ç½®ç´¢å¼•è®¿é—®
            
            # å…ˆå°è¯•ä½¿ç”¨åˆ—åï¼ˆå¦‚æœakshareè¿”å›çš„æ˜¯æ ‡å‡†åˆ—åï¼‰
            # æ³¨æ„ï¼šå³ä½¿åˆ—æ•°ä¸è¶³6ï¼Œä¹Ÿè¦å°è¯•é‡å‘½å
            if len(df.columns) >= 5:  # è‡³å°‘éœ€è¦5åˆ—ï¼šæ—¥æœŸã€å¼€ç›˜ã€æ”¶ç›˜ã€æœ€é«˜ã€æœ€ä½
                # ä½¿ç”¨ä½ç½®ç´¢å¼•é‡å‘½åå…³é”®åˆ—
                # æ ¹æ®2025-12-31çš„æ­£ç¡®æ•°æ®æ ¸å¯¹ï¼š
                # æ­£ç¡®ï¼šå¼€ç›˜=4.66, æ”¶ç›˜=4.65, æœ€é«˜=4.68, æœ€ä½=4.62
                # akshareå®é™…è¿”å›çš„é¡ºåºå¯èƒ½æ˜¯ï¼šæ—¥æœŸã€å¼€ç›˜ã€æ”¶ç›˜ã€æœ€é«˜ã€æœ€ä½ã€æˆäº¤é‡
                # æˆ–è€…ï¼šæ—¥æœŸã€å…¶ä»–ã€å¼€ç›˜ã€æ”¶ç›˜ã€æœ€é«˜ã€æœ€ä½
                # éœ€è¦æ ¹æ®åˆ—åæˆ–æ•°æ®é€»è¾‘åˆ¤æ–­
                rename_dict = {}
                if len(df.columns) > 0:
                    rename_dict[df.columns[0]] = 'æ—¥æœŸ'
                
                # å°è¯•æ ¹æ®åˆ—ååˆ¤æ–­
                col_names = [str(col).lower() for col in df.columns]
                
                # æŸ¥æ‰¾åŒ…å«"å¼€ç›˜"ã€"æ”¶ç›˜"ã€"æœ€é«˜"ã€"æœ€ä½"çš„åˆ—
                open_idx = None
                close_idx = None
                high_idx = None
                low_idx = None
                
                for i, col_name in enumerate(col_names):
                    if 'å¼€ç›˜' in col_name or 'open' in col_name:
                        open_idx = i
                    elif 'æ”¶ç›˜' in col_name or 'close' in col_name:
                        close_idx = i
                    elif 'æœ€é«˜' in col_name or 'high' in col_name:
                        high_idx = i
                    elif 'æœ€ä½' in col_name or 'low' in col_name:
                        low_idx = i
                
                # å¦‚æœæ‰¾åˆ°äº†åˆ—åï¼Œä½¿ç”¨åˆ—åæ˜ å°„
                if open_idx and close_idx and high_idx and low_idx:
                    rename_dict[df.columns[open_idx]] = 'å¼€ç›˜'
                    rename_dict[df.columns[close_idx]] = 'æ”¶ç›˜'
                    rename_dict[df.columns[high_idx]] = 'æœ€é«˜'
                    rename_dict[df.columns[low_idx]] = 'æœ€ä½'
                else:
                    # å¦‚æœæ²¡æ‰¾åˆ°åˆ—åï¼Œæ ¹æ®2025-12-31çš„æ­£ç¡®æ•°æ®æ¨æ–­ï¼š
                    # æ­£ç¡®ï¼šå¼€ç›˜=4.66, æ”¶ç›˜=4.65, æœ€é«˜=4.68, æœ€ä½=4.62
                    # ä¹‹å‰é”™è¯¯æ˜¾ç¤ºï¼šåˆ—1=2.00, åˆ—2=4.66, åˆ—3=4.65, åˆ—4=4.68
                    # æ‰€ä»¥åˆ—é¡ºåºæ˜¯ï¼šæ—¥æœŸã€å…¶ä»–ã€å¼€ç›˜ã€æ”¶ç›˜ã€æœ€é«˜ã€æœ€ä½
                    # åˆ—1å¯èƒ½æ˜¯æ¶¨è·Œå¹…æˆ–å…¶ä»–æ•°æ®ï¼Œè·³è¿‡
                    if len(df.columns) > 2:
                        rename_dict[df.columns[2]] = 'å¼€ç›˜'  # åˆ—2æ˜¯å¼€ç›˜
                    if len(df.columns) > 3:
                        rename_dict[df.columns[3]] = 'æ”¶ç›˜'  # åˆ—3æ˜¯æ”¶ç›˜
                    if len(df.columns) > 4:
                        rename_dict[df.columns[4]] = 'æœ€é«˜'  # åˆ—4æ˜¯æœ€é«˜
                    if len(df.columns) > 5:
                        rename_dict[df.columns[5]] = 'æœ€ä½'  # åˆ—5æ˜¯æœ€ä½
                    
                    # å¦‚æœåˆ—1å­˜åœ¨ä½†ä¸æ˜¯å¼€ç›˜ï¼Œä¿ç•™åŸåˆ—åï¼ˆå¯èƒ½æ˜¯æ¶¨è·Œå¹…ç­‰ï¼‰
                    if len(df.columns) > 1 and df.columns[1] not in rename_dict:
                        # ä¸é‡å‘½ååˆ—1ï¼Œä¿æŒåŸæ ·
                        pass
                    
                    if len(df.columns) > 6:
                        rename_dict[df.columns[6]] = 'æˆäº¤é‡'  # åˆ—6æ˜¯æˆäº¤é‡
                
                # æ‰§è¡Œé‡å‘½å
                if rename_dict:
                    df = df.rename(columns=rename_dict)
                    print(f"[è°ƒè¯•] åˆ—é‡å‘½åå®Œæˆï¼Œæ–°åˆ—å: {list(df.columns)}")
                else:
                    print(f"[è­¦å‘Š] æœªæ‰§è¡Œåˆ—é‡å‘½åï¼ŒåŸå§‹åˆ—å: {list(df.columns)}")
            
            # ç¡®ä¿æ—¥æœŸåˆ—å­˜åœ¨ä¸”å¯è½¬æ¢
            if 'æ—¥æœŸ' in df.columns:
                df['æ—¥æœŸ'] = pd.to_datetime(df['æ—¥æœŸ'], errors='coerce')
            else:
                print(f"[é”™è¯¯] æœªæ‰¾åˆ°'æ—¥æœŸ'åˆ—ï¼Œå¯ç”¨åˆ—: {list(df.columns)}")
                return None
            
            # éªŒè¯å¿…è¦çš„åˆ—æ˜¯å¦å­˜åœ¨ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™ä½¿ç”¨ä½ç½®ç´¢å¼•åˆ›å»º
            required_cols = ['å¼€ç›˜', 'æ”¶ç›˜', 'æœ€é«˜', 'æœ€ä½']
            missing_cols = [col for col in required_cols if col not in df.columns]
            if missing_cols:
                # print(f"[è­¦å‘Š] ç¼ºå°‘å¿…è¦çš„åˆ—: {missing_cols}")
                # print(f"[è°ƒè¯•] å½“å‰åˆ—å: {list(df.columns)}")
                # print(f"[è°ƒè¯•] åˆ—æ•°: {len(df.columns)}")
                # å¦‚æœåˆ—æ•°è¶³å¤Ÿï¼Œå°è¯•ä½¿ç”¨ä½ç½®ç´¢å¼•åˆ›å»ºæ–°åˆ—
                # æ ¹æ®2025-12-31çš„æ­£ç¡®æ•°æ®ï¼šåˆ—é¡ºåºæ˜¯ æ—¥æœŸã€å…¶ä»–ã€å¼€ç›˜ã€æ”¶ç›˜ã€æœ€é«˜ã€æœ€ä½
                if len(df.columns) >= 6:
                    # print(f"[è°ƒè¯•] ä½¿ç”¨ä½ç½®ç´¢å¼•åˆ›å»ºç¼ºå¤±çš„åˆ—...")
                    if 'å¼€ç›˜' not in df.columns and len(df.columns) > 2:
                        df['å¼€ç›˜'] = df.iloc[:, 2]
                    if 'æ”¶ç›˜' not in df.columns and len(df.columns) > 3:
                        df['æ”¶ç›˜'] = df.iloc[:, 3]
                    if 'æœ€é«˜' not in df.columns and len(df.columns) > 4:
                        df['æœ€é«˜'] = df.iloc[:, 4]
                    if 'æœ€ä½' not in df.columns and len(df.columns) > 5:
                        df['æœ€ä½'] = df.iloc[:, 5]
                    # print(f"[è°ƒè¯•] åˆ›å»ºåçš„åˆ—å: {list(df.columns)}")
            
            # åˆ é™¤æ—¥æœŸä¸ºç©ºçš„è®°å½•
            df = df.dropna(subset=['æ—¥æœŸ'])
            
            if df.empty:
                return None
            
            df = df.sort_values('æ—¥æœŸ').reset_index(drop=True)
            
            return df
        except Exception as e:
            print(f"è·å– {stock_code} æ—¥Kçº¿æ•°æ®å¤±è´¥: {e}")
            return None
    
    def get_weekly_kline(self, stock_code, period="1y"):
        """
        è·å–å‘¨Kçº¿æ•°æ®ï¼ˆåŒ…å«å‘¨æˆäº¤é‡ï¼‰
        :param stock_code: è‚¡ç¥¨ä»£ç ï¼ˆå¦‚ '000001'ï¼‰
        :param period: æ—¶é—´å‘¨æœŸï¼Œ'1y'è¡¨ç¤º1å¹´
        :return: DataFrameï¼ŒåŒ…å«å‘¨æ—¥æœŸã€å¼€ç›˜ã€æ”¶ç›˜ã€æœ€é«˜ã€æœ€ä½ã€å‘¨æˆäº¤é‡ç­‰
        """
        try:
            print(f"å¼€å§‹è·å– {stock_code} çš„å‘¨Kçº¿æ•°æ®...")
            # æ–¹æ³•1: å°è¯•ç›´æ¥ä½¿ç”¨akshareçš„å‘¨Kçº¿æ¥å£
            try:
                end_date = datetime.now().strftime('%Y%m%d')
                start_date = (datetime.now() - timedelta(days=365 * 2)).strftime('%Y%m%d')
                
                print(f"å°è¯•ç›´æ¥è·å–å‘¨Kçº¿: {stock_code}, {start_date} - {end_date}")
                df = ak.stock_zh_a_hist(symbol=stock_code, period="weekly", 
                                        start_date=start_date, end_date=end_date, adjust="qfq")
                print(f"ç›´æ¥è·å–å‘¨Kçº¿ç»“æœ: {df is not None}, {len(df) if df is not None else 0} æ¡")
                
                if df is not None and not df.empty:
                    # é‡å‘½ååˆ—
                    if len(df.columns) >= 6:
                        rename_dict = {}
                        if len(df.columns) > 0:
                            rename_dict[df.columns[0]] = 'æ—¥æœŸ'
                        # æ ¹æ®2025-12-31çš„æ­£ç¡®æ•°æ®æ¨æ–­ï¼š
                        # æ­£ç¡®ï¼šå¼€ç›˜=4.66, æ”¶ç›˜=4.65, æœ€é«˜=4.68, æœ€ä½=4.62
                        # ä¹‹å‰é”™è¯¯æ˜¾ç¤ºï¼šåˆ—1=2.00, åˆ—2=4.66, åˆ—3=4.65, åˆ—4=4.68
                        # æ‰€ä»¥åˆ—é¡ºåºæ˜¯ï¼šæ—¥æœŸã€å…¶ä»–ï¼ˆåˆ—1ï¼Œå¯èƒ½æ˜¯æ¶¨è·Œå¹…ï¼‰ã€å¼€ç›˜ï¼ˆåˆ—2ï¼‰ã€æ”¶ç›˜ï¼ˆåˆ—3ï¼‰ã€æœ€é«˜ï¼ˆåˆ—4ï¼‰ã€æœ€ä½ï¼ˆåˆ—5ï¼‰
                        # åˆ—1è·³è¿‡ï¼ˆå¯èƒ½æ˜¯æ¶¨è·Œå¹…æˆ–å…¶ä»–æ•°æ®ï¼‰
                        if len(df.columns) > 2:
                            rename_dict[df.columns[2]] = 'å¼€ç›˜'  # åˆ—2æ˜¯å¼€ç›˜
                        if len(df.columns) > 3:
                            rename_dict[df.columns[3]] = 'æ”¶ç›˜'  # åˆ—3æ˜¯æ”¶ç›˜
                        if len(df.columns) > 4:
                            rename_dict[df.columns[4]] = 'æœ€é«˜'  # åˆ—4æ˜¯æœ€é«˜
                        if len(df.columns) > 5:
                            rename_dict[df.columns[5]] = 'æœ€ä½'  # åˆ—5æ˜¯æœ€ä½
                        if len(df.columns) > 6:
                            rename_dict[df.columns[6]] = 'æˆäº¤é‡'  # åˆ—6æ˜¯æˆäº¤é‡
                        
                        df = df.rename(columns=rename_dict)
                    
                    # ç¡®ä¿æ—¥æœŸåˆ—å­˜åœ¨ä¸”å¯è½¬æ¢
                    if 'æ—¥æœŸ' in df.columns:
                        df['æ—¥æœŸ'] = pd.to_datetime(df['æ—¥æœŸ'], errors='coerce')
                        df = df.dropna(subset=['æ—¥æœŸ'])
                        df = df.sort_values('æ—¥æœŸ').reset_index(drop=True)
                        # é‡å‘½åæˆäº¤é‡ä¸ºå‘¨æˆäº¤é‡
                        if 'æˆäº¤é‡' in df.columns:
                            df = df.rename(columns={'æˆäº¤é‡': 'å‘¨æˆäº¤é‡'})
                        return df
            except Exception as e1:
                print(f"ç›´æ¥è·å–å‘¨Kçº¿å¤±è´¥: {e1}ï¼Œå°è¯•ä»æ—¥Kçº¿èšåˆ...")
                # å¦‚æœç›´æ¥è·å–å¤±è´¥ï¼Œä½¿ç”¨èšåˆæ–¹å¼
            
            # æ–¹æ³•2: ä»æ—¥Kçº¿èšåˆä¸ºå‘¨Kçº¿
            print(f"å¼€å§‹ä»æ—¥Kçº¿èšåˆå‘¨Kçº¿: {stock_code}")
            daily_df = self.get_daily_kline(stock_code, period)
            if daily_df is None or daily_df.empty:
                print(f"æ— æ³•è·å– {stock_code} çš„æ—¥Kçº¿æ•°æ®")
                return None
            print(f"è·å–åˆ° {len(daily_df)} æ¡æ—¥Kçº¿æ•°æ®ï¼Œå¼€å§‹èšåˆ...")
            
            # ç¡®ä¿å¿…è¦çš„åˆ—å­˜åœ¨
            required_cols = ['æ—¥æœŸ', 'å¼€ç›˜', 'æ”¶ç›˜', 'æœ€é«˜', 'æœ€ä½', 'æˆäº¤é‡']
            missing_cols = [col for col in required_cols if col not in daily_df.columns]
            if missing_cols:
                print(f"è­¦å‘Šï¼šç¼ºå°‘å¿…è¦çš„åˆ— {missing_cols}")
                return None
            
            # è½¬æ¢ä¸ºå‘¨Kçº¿
            weekly_df = daily_df.copy()
            
            # ä½¿ç”¨ISOå‘¨ï¼ˆå‘¨ä¸€å¼€å§‹ï¼‰
            weekly_df['å¹´å‘¨'] = weekly_df['æ—¥æœŸ'].dt.to_period('W-SUN')  # å‘¨æ—¥ç»“æŸçš„å‘¨
            
            # æŒ‰å‘¨èšåˆ
            def agg_week(group):
                return pd.Series({
                    'å¼€ç›˜': group['å¼€ç›˜'].iloc[0],
                    'æ”¶ç›˜': group['æ”¶ç›˜'].iloc[-1],
                    'æœ€é«˜': group['æœ€é«˜'].max(),
                    'æœ€ä½': group['æœ€ä½'].min(),
                    'å‘¨æˆäº¤é‡': group['æˆäº¤é‡'].sum()  # å‘¨æˆäº¤é‡ = è¯¥å‘¨æ‰€æœ‰äº¤æ˜“æ—¥çš„æˆäº¤é‡ä¹‹å’Œ
                })
            
            weekly_kline = weekly_df.groupby('å¹´å‘¨').apply(agg_week).reset_index()
            
            # å¦‚æœæˆäº¤é¢åˆ—å­˜åœ¨ï¼Œä¹Ÿèšåˆ
            if 'æˆäº¤é¢' in weekly_df.columns:
                weekly_kline['å‘¨æˆäº¤é¢'] = weekly_df.groupby('å¹´å‘¨')['æˆäº¤é¢'].sum().values
            
            # å°†å‘¨æœŸè½¬æ¢ä¸ºæ—¥æœŸï¼ˆä½¿ç”¨è¯¥å‘¨çš„æœ€åä¸€å¤©ï¼‰
            weekly_kline['æ—¥æœŸ'] = weekly_kline['å¹´å‘¨'].dt.to_timestamp() + pd.Timedelta(days=6)
            weekly_kline = weekly_kline.sort_values('æ—¥æœŸ').reset_index(drop=True)
            
            # è®¡ç®—å‘¨æ¶¨è·Œå¹…
            weekly_kline['æ¶¨è·Œå¹…'] = weekly_kline['æ”¶ç›˜'].pct_change() * 100
            weekly_kline['æ¶¨è·Œå¹…'] = weekly_kline['æ¶¨è·Œå¹…'].fillna(0)
            
            return weekly_kline
        except Exception as e:
            import traceback
            print(f"è·å– {stock_code} å‘¨Kçº¿æ•°æ®å¤±è´¥: {e}")
            print(f"é”™è¯¯è¯¦æƒ…: {traceback.format_exc()}")
            return None
    
    def get_monthly_kline(self, stock_code, period="1y"):
        """
        è·å–æœˆKçº¿æ•°æ®
        :param stock_code: è‚¡ç¥¨ä»£ç 
        :param period: æ—¶é—´å‘¨æœŸ
        :return: DataFrameï¼ŒæœˆKçº¿æ•°æ®
        """
        try:
            # å…ˆè·å–æ—¥Kçº¿ï¼Œç„¶åè½¬æ¢ä¸ºæœˆKçº¿
            daily_df = self.get_daily_kline(stock_code, period)
            if daily_df is None or daily_df.empty:
                return None
            
            # è½¬æ¢ä¸ºæœˆKçº¿
            monthly_df = daily_df.copy()
            
            # ç¡®ä¿å¿…è¦çš„åˆ—å­˜åœ¨
            required_cols = ['æ—¥æœŸ', 'å¼€ç›˜', 'æ”¶ç›˜', 'æœ€é«˜', 'æœ€ä½', 'æˆäº¤é‡']
            missing_cols = [col for col in required_cols if col not in monthly_df.columns]
            if missing_cols:
                print(f"è­¦å‘Šï¼šç¼ºå°‘å¿…è¦çš„åˆ— {missing_cols}")
                return None
            
            monthly_df['å¹´æœˆ'] = monthly_df['æ—¥æœŸ'].dt.to_period('M')
            
            # æŒ‰æœˆèšåˆï¼ˆä½¿ç”¨applyæ–¹å¼ï¼Œæ›´å…¼å®¹ï¼‰
            def agg_month(group):
                return pd.Series({
                    'å¼€ç›˜': group['å¼€ç›˜'].iloc[0],
                    'æ”¶ç›˜': group['æ”¶ç›˜'].iloc[-1],
                    'æœ€é«˜': group['æœ€é«˜'].max(),
                    'æœ€ä½': group['æœ€ä½'].min(),
                    'æˆäº¤é‡': group['æˆäº¤é‡'].sum()
                })
            
            monthly_kline = monthly_df.groupby('å¹´æœˆ').apply(agg_month).reset_index()
            
            # å¦‚æœæˆäº¤é¢åˆ—å­˜åœ¨ï¼Œä¹Ÿèšåˆ
            if 'æˆäº¤é¢' in monthly_df.columns:
                monthly_kline['æˆäº¤é¢'] = monthly_df.groupby('å¹´æœˆ')['æˆäº¤é¢'].sum().values
            
            monthly_kline['æ—¥æœŸ'] = monthly_kline['å¹´æœˆ'].dt.to_timestamp()
            monthly_kline = monthly_kline.sort_values('æ—¥æœŸ').reset_index(drop=True)
            
            # è®¡ç®—æœˆæ¶¨è·Œå¹…
            monthly_kline['æ¶¨è·Œå¹…'] = monthly_kline['æ”¶ç›˜'].pct_change() * 100
            monthly_kline['æ¶¨è·Œå¹…'] = monthly_kline['æ¶¨è·Œå¹…'].fillna(0)
            
            return monthly_kline
        except Exception as e:
            import traceback
            print(f"è·å– {stock_code} æœˆKçº¿æ•°æ®å¤±è´¥: {e}")
            print(f"é”™è¯¯è¯¦æƒ…: {traceback.format_exc()}")
            return None
    
    def get_limit_up_info(self, stock_code, days=10):
        """
        è·å–æœ€è¿‘Nä¸ªäº¤æ˜“æ—¥çš„æ¶¨åœä¿¡æ¯
        :param stock_code: è‚¡ç¥¨ä»£ç 
        :param days: æŸ¥è¯¢å¤©æ•°
        :return: æ˜¯å¦æœ‰æ¶¨åœï¼ˆTrue/Falseï¼‰ï¼Œæ¶¨åœæ—¥æœŸåˆ—è¡¨
        """
        try:
            # è·å–æœ€è¿‘Nå¤©çš„æ—¥Kçº¿æ•°æ®
            end_date = datetime.now().strftime('%Y%m%d')
            start_date = (datetime.now() - timedelta(days=days*2)).strftime('%Y%m%d')  # å¤šå–ä¸€äº›ï¼Œæ’é™¤éäº¤æ˜“æ—¥
            
            df = ak.stock_zh_a_hist(symbol=stock_code, period="daily", 
                                    start_date=start_date, end_date=end_date, adjust="qfq")
            
            if df is None or df.empty:
                return False, []
            
            # ä½¿ç”¨ä½ç½®ç´¢å¼•è®¿é—®æ•°æ®ï¼Œé¿å…åˆ—åé—®é¢˜
            if len(df.columns) < 9:
                return False, []
            
            # ç›´æ¥ä½¿ç”¨ä½ç½®ç´¢å¼•
            df.iloc[:, 0] = pd.to_datetime(df.iloc[:, 0], errors='coerce')
            df = df.dropna(subset=[df.columns[0]])
            if df.empty:
                return False, []
            
            # æŒ‰æ—¥æœŸæ’åº
            df = df.sort_values(by=df.columns[0]).reset_index(drop=True)
            
            # å–æœ€è¿‘daysä¸ªäº¤æ˜“æ—¥
            recent_df = df.tail(days)
            
            # åˆ¤æ–­æ˜¯å¦æœ‰æ¶¨åœï¼ˆæ¶¨è·Œå¹… >= 9.5%ï¼Œè€ƒè™‘STè‚¡æ˜¯5%ï¼‰
            # æ¶¨è·Œå¹…é€šå¸¸åœ¨ç¬¬8åˆ—ï¼ˆç´¢å¼•8ï¼‰
            pct_chg_col = df.columns[8] if len(df.columns) > 8 else None
            if pct_chg_col is None:
                return False, []
            
            limit_up_mask = recent_df[pct_chg_col] >= 9.5
            date_col = df.columns[0]
            limit_up_days = recent_df[limit_up_mask][date_col].tolist()
            has_limit_up = len(limit_up_days) > 0
            
            return has_limit_up, limit_up_days
        except Exception as e:
            print(f"è·å– {stock_code} æ¶¨åœä¿¡æ¯å¤±è´¥: {e}")
            return False, []

